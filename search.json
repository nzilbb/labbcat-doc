[
  {
    "objectID": "worksheets/demo/4-alignment.html",
    "href": "worksheets/demo/4-alignment.html",
    "title": "4 - Alignment",
    "section": "",
    "text": "The Hidden Markov Model Toolkit (HTK) is a speech recognition toolkit developed at Cambridge University. It is a set of programs that can be used to build speech recognition systems. Part of the process of building such systems involves force-aligning training data - i.e. automatically lining up phonemic-transcriptions of known words with the audio signal in the training recordings. LaBB-CAT takes advantage of this capability to facilitate forced-alignment for your transcripts.\nIn order to do this, HTK needs the following ingredients:\n\na set of recordings broken up into short utterances\northographic transcriptions of each utterance\nphonemic transcriptions of each of the words in each utterance\n\nIn the demo database you have all of these three ingredients, and the data has be force-aligned using HTK.\nThis means that, in addition to the manually alignment of utterance start/end times, HTK has automatically provided start and end times for words, and also for the speech sounds (‘phones’) within each word.\n\nSelect the transcripts option on the menu, and open a transcript in the list.\nTick the segment layer; this is the layer that contains the phone that HTK has aligned.\n\nThe segment layer looks similar to the phonemes layer on the transcript page, but there are several important differences:\n\nEach of the phonemes layer annotations has the transcription for the whole word, e.g.\n\n/dɪfrənt/\n…but the segment layer has, for each word, several annotations, one for each phone.\n/d/\n/ɪ/\n/f/\n/r/\n/ə/\n/n/\n/t/\n\nThe phonemes layer annotation are word tags that are not aligned, but the segment layer annotations have a start and end time specified.\nThe phonemes layer can include more than one phonemic transcription for a word - all possible pronunciations found in CELEX are tagged on each token, e.g.\n\n\n/dɪfrənt/\n\n/dɪfrn̩t/\n\n/dɪfərənt/\n\n/dɪfərn̩t/\n…but the segment layer annotations represent only one pronunciation; the pronunciation that HTK determined to be the one that best matched the audio.\n\n\nThe interactive transcript page doesn’t show you the alignments of the words or phones, but you can see those using the \"EMU webApp\" that is integrated into LaBB-CAT.\n(For more information about EMU, see: http://ips-lmu.github.io/EMU.html)\n\nClick on a line that has been aligned (i.e. that has segments under the words).\nSelect the ‘View in EMU webApp’ option on the menu.\nA new window will appear, and after a short delay, you will see the wave form of the utterance audio, with a spectrogram, and below this, segment annotations which are aligned with the audio above and represent individual sounds within each word.\nYou can check the alignments by clicking on a segment to selected it, and then clicking the Play Selected button below.\n\n\n\n\nLaBB-CAT also integrates directly with Praat, if you have it installed on your computer. With Praat integration installed, you can similarly inspect alignments, but you can also correct them by moving the alignments in Praat and then saving them back to LaBB-CAT.\n\n\n\n\n\n\nNote\n\n\n\nIf you don’t use Praat, or don’t have it installed on your computer, you can skip this section.\n\n\nAlthough you can’t actually correct the Demo LaBB-CAT alignments, because you have read-only access to the data, you may like to install the Praat integration to get an idea of how it works:\nFirst, the LaBB-CAT/Praat integration has to be set up; this only has to be done once:\n\nOn the top-right of the transcript page, above the playback controls, there’s a Praat icon - click it.\nFollow the instructions that appear (these vary depending on what web browser you use).\n\nYou may need to grant a browser extension permission to install, and it’s possible you will need a connection to the internet in order to download this extension.\n\nNow Praat integration has been set up, and you should be able to access Praat options in the transcript page from now on…\nClick on a line that has been aligned, and select the ‘Open Text Grid in Praat’ option on the menu.\n\nYou may also be prompted to download and run a program called “install-jsendpraat.jar”. If so, click the link, save the resulting file, run the program, and then do this step again.\nYou also may be asked where Praat is installed; Navigate to the location where Praat is installed, and double-click the “Praat.exe” file (on some systems the file may simply be called “Praat”). The Praat program may open, and then immediately close, as LaBB-CAT tests it can communicate with Praat.\n\nIf in doubt, check the  online help on the transcript page; it has a section explaining how to set up Praat integration on various browsers and operating systems.\n\nAfter a short delay, Praat should open, and show you a spectrogram of the line’s audio, with a TextGrid below that includes the words and the segments.\nIf you click on a word, and hit the &lt;tab&gt; key, the word’s interval is played. Try out various words, and see what you think about how accurate HTK has been with its alignment.\nTry this out with different lines in the transcript.\nYou will see that in some cases the alignment is pretty good, and in other cases, it’s not so good. In the not-so-good cases, see if you can figure out why HTK got it wrong.\n\nIf you had ‘edit’ rather than ‘read-only’ permissions in LaBB-CAT, then each time you opened an utterance in Praat, a button would appear in the transcript to the left of the line, labelled Import Changes. This button would allow you to save any adjustments you might want to make to the alignments back into the LaBB-CAT database.\n\nThese changes are flagged as manual edits, so if forced-alignment is run again, they will not be over-written with new bad alignments.\nThis mechanism can also be used to add other annotations from Praat into LaBB-CAT annotation layers.\n\n\nOnce the words and phone have been aligned with HTK there are a number of annotation possibilities that arise.\nFor example, word syllabification information can be retrieved from CELEX and combined with the aligned phones to construct aligned syllable annotations.\n\nTick the alignment project at the top of the transcript.\nThis reveals several layers.\nTick the syllables layer\nOnce the transcript has re-loaded, open an utterance with the EMU webApp or with Praat.\nYou will see that, in addition to aligned words and phones, the syllables are also aligned, and labelled with their phonemic transcription, and stressed syllables are prepended with an apostrophe.\n\nAlso, with exact word durations (i.e. excluding pauses in speech) and syllable counts, the speaker’s articulation rate, in syllables per minute, can be computed. The Statistics Layer Manager is a module that can be configured to compute sums, counts, and rates of various kinds over different scopes, including syllables per minute.\n\nTick the syllables per minute layer.\nYou will see that each utterance has a spanning annotation across the top of it, labelled with a number; that number is the articulation rate for that particular utterance.\nBoth local and global articulation rate can be calculated …\nClick on the name of the speaker at the top of the transcript.\nThis will open the participant attributes page for that speaker.\nYou will see that one of the attributes is Syllables per Minute; this is the speakers overall articulation rate, across all their utterances.\n\nArticulation rate is calculated by excluding the durations of inter-word pauses. These pauses themselves can be annotated, for search or analysis purposes.\n\nGo back to the transcript page.\nTick the previous pause layer.\nYou will see that many of the word tokens in the transcript are tagged with a number. These words are preceded by a pause in speech, and the number is the length of that pause in seconds.\nOpen an utterance in the EMU webApp or in Praat to confirm these pauses are correct.\n\nYou may notice that pauses in the middle of utterances are always right, but the pause before the first word in the utterance seems wrong. See if you can figure out why.\n\n\n\n\nGiven that HTK has created individually aligned phones in the database, those speech sounds can be searched and exported.\nLet’s say you’re particularly interested in the vowel in the word ‘KIT’. You can now identify and extract instances of that phoneme.\n\nClick the search link on the menu.\nTick the segment layer.\n\nThe segments layer contains annotations at the sub-word level - i.e. there are potentially multiple annotations per word, each annotation representing a single phone of the word. You will see that, as with other layers, there is a box on the segments layer for a regular expression.\nAs with other patterns in the search matrix, the pattern that you enter in the box is matched against individual annotations. So if you enter I (i.e. capital I) in the in the box, it will match each ‘KIT’ vowel segment in each word in the database.\n\n\n\n\n\n\nImportant\n\n\n\nIf you enter a pattern that would match more than a single character on this layer (i.e. more than a single phoneme) then no search results will be returned, because each annotation on this layer is only a single character long (remember the DISC encoding uses one character per phoneme).\nFor example, if you enter .*IN for your search, intending to match all words ending in “…ing”, then no results will be returned, because no single segment will ever match that pattern.\n\n\n\nWe want to search for all instances of the ‘KIT’ vowel, so enter I in the segments pattern box.\nClick Search\nAfter a short delay, you should see a list of results.\n\nYou will see that the results list words that have the ‘KIT’, but in many cases it’s not the main stressed vowel. What if we’re only interested in stressed ‘KIT’ vowels?\nThat’s ok, because we also have stress-marked syllable annotations, so we can add that layer to the search matrix, and identify only stressed vowels …\n\nNote down the number of results returned by your last search.\nBack on the search form, add the syllables layer to the search matrix.\nAs we have seen, stressed syllables are labelled with an apostrophe at the start. Enter a regular expression that will identify all syllables that start with apostrophe.\nIn this way, the results will give use all KIT vowels that are within a stressed syllable.\nClick Search again.\nThis time you will see fewer results returned, because we’ve filtered out the un-stressed version of the vowel.\n\n\nYou can export all these vowel tokens to a CSV file for analysis or further processing. The CSV file can include all kinds of other information, including participant and transcript attributes and other annotations.\n\nOn the results page, next to the CSV Export button there’s a ▼ button. Press it.\nYou will see several columns of checkboxes appear.\nTick the following checkboxes:\n\n\nUnder Participant tick gender, age_category, and syllables per minute\nUnder Span tick topic\nUnder Phrase tick syllables per minute\n\nNow press the CSV Export button above.\nSave and open the resulting file.\nYou will see that the file includes extra columns for the attributes and layers that you ticked (e.g. the topic marked in the original ELAN transcript, the speaker’s articulation rate, the local articulation rate, etc.).\n\nThe CSV file includes whatever annotation you might be interested, so you can go on to do qualitative or statistical analysis with other tools like Microsoft Excel or R. You can even add your own annotations to the CSV file and import them back into LaBB-CAT.\n\n\n\nThe CSV file also includes the columns “Target segments start” and “Target segments end”; these columns have the start and end time of the matching ‘KIT’ vowel token. Given this information, LaBB-CAT can extract acoustic measurements on the speech sounds using Praat.\n\n\n\n\n\n\nNote\n\n\n\nThe following steps work even if you don’t have Praat installed on your own computer, because Praat is used on the LaBB-CAT server …\n\n\n\nIn LaBB-CAT, click the upload menu option.\nClick the process with praat option.\nClick Browse and select the CSV results that you saved above.\nYou will see a form to fill in, and the first couple of settings (Transcript Name column and Participant column should be already filled in).\nFor the Start Time column, ensure that the Target segment start option is selected.\nFor the End Time column, ensure the Target segment end option is selected.\n\nThese two settings define the start/end times of the phone. For some measurements you might extract from Praat, processing signal that includes surrounding context is usually a good idea. You’ll see there’s a setting for that (which you can leave at the default of 0.025s), and you will see options for various measurements.\nThe default options are for F1 and F2 only, but if you feel like getting other measurements, feel free to tick those options too. You can expand each section with the ⯈ button to reveal more settings, which allow you to specify more detail about how Praat should do its computations. Again, feel free to look at those and try different settings.\n\nClick Process.\nYou will see a progress bar while LaBB-CAT generates Praat scripts and runs them.\nOnce Praat has finished processing the intervals, you will get a CSV file (you might have to click the CSV file with measurements link) - save and open it.\n\nYou will see that it’s a copy of the CSV file you uploaded, with some extra columns added on the right.\nDepending on your settings, this will include at least one column per measurement you selected (the formant columns also include on that contains the time at which the measurements were taken), and a final column called Error which is hopefully blank, but which might contain errors reported back by Praat (e.g. if it couldn’t find the audio file or ran into any other problem during processing).\n\nIn this worksheet you have seen that:\n\nHTK can be used to compute word and phone alignments automatically from your data.\nThe resulting alignments can be inspected and corrected directly from the transcript page.\nArticulation rate can be computed, excluding inter-word pauses.\nInter-word pauses can also be tagged.\nIndividual phone tokens can be searched for and extracted.\nAcoustic measurements for matching phones can also be made."
  },
  {
    "objectID": "worksheets/demo/4-alignment.html#htk",
    "href": "worksheets/demo/4-alignment.html#htk",
    "title": "4 - Alignment",
    "section": "",
    "text": "The Hidden Markov Model Toolkit (HTK) is a speech recognition toolkit developed at Cambridge University. It is a set of programs that can be used to build speech recognition systems. Part of the process of building such systems involves force-aligning training data - i.e. automatically lining up phonemic-transcriptions of known words with the audio signal in the training recordings. LaBB-CAT takes advantage of this capability to facilitate forced-alignment for your transcripts.\nIn order to do this, HTK needs the following ingredients:\n\na set of recordings broken up into short utterances\northographic transcriptions of each utterance\nphonemic transcriptions of each of the words in each utterance\n\nIn the demo database you have all of these three ingredients, and the data has be force-aligned using HTK.\nThis means that, in addition to the manually alignment of utterance start/end times, HTK has automatically provided start and end times for words, and also for the speech sounds (‘phones’) within each word.\n\nSelect the transcripts option on the menu, and open a transcript in the list.\nTick the segment layer; this is the layer that contains the phone that HTK has aligned.\n\nThe segment layer looks similar to the phonemes layer on the transcript page, but there are several important differences:\n\nEach of the phonemes layer annotations has the transcription for the whole word, e.g.\n\n/dɪfrənt/\n…but the segment layer has, for each word, several annotations, one for each phone.\n/d/\n/ɪ/\n/f/\n/r/\n/ə/\n/n/\n/t/\n\nThe phonemes layer annotation are word tags that are not aligned, but the segment layer annotations have a start and end time specified.\nThe phonemes layer can include more than one phonemic transcription for a word - all possible pronunciations found in CELEX are tagged on each token, e.g.\n\n\n/dɪfrənt/\n\n/dɪfrn̩t/\n\n/dɪfərənt/\n\n/dɪfərn̩t/\n…but the segment layer annotations represent only one pronunciation; the pronunciation that HTK determined to be the one that best matched the audio.\n\n\nThe interactive transcript page doesn’t show you the alignments of the words or phones, but you can see those using the \"EMU webApp\" that is integrated into LaBB-CAT.\n(For more information about EMU, see: http://ips-lmu.github.io/EMU.html)\n\nClick on a line that has been aligned (i.e. that has segments under the words).\nSelect the ‘View in EMU webApp’ option on the menu.\nA new window will appear, and after a short delay, you will see the wave form of the utterance audio, with a spectrogram, and below this, segment annotations which are aligned with the audio above and represent individual sounds within each word.\nYou can check the alignments by clicking on a segment to selected it, and then clicking the Play Selected button below."
  },
  {
    "objectID": "worksheets/demo/4-alignment.html#praat-browser-integration",
    "href": "worksheets/demo/4-alignment.html#praat-browser-integration",
    "title": "4 - Alignment",
    "section": "",
    "text": "LaBB-CAT also integrates directly with Praat, if you have it installed on your computer. With Praat integration installed, you can similarly inspect alignments, but you can also correct them by moving the alignments in Praat and then saving them back to LaBB-CAT.\n\n\n\n\n\n\nNote\n\n\n\nIf you don’t use Praat, or don’t have it installed on your computer, you can skip this section.\n\n\nAlthough you can’t actually correct the Demo LaBB-CAT alignments, because you have read-only access to the data, you may like to install the Praat integration to get an idea of how it works:\nFirst, the LaBB-CAT/Praat integration has to be set up; this only has to be done once:\n\nOn the top-right of the transcript page, above the playback controls, there’s a Praat icon - click it.\nFollow the instructions that appear (these vary depending on what web browser you use).\n\nYou may need to grant a browser extension permission to install, and it’s possible you will need a connection to the internet in order to download this extension.\n\nNow Praat integration has been set up, and you should be able to access Praat options in the transcript page from now on…\nClick on a line that has been aligned, and select the ‘Open Text Grid in Praat’ option on the menu.\n\nYou may also be prompted to download and run a program called “install-jsendpraat.jar”. If so, click the link, save the resulting file, run the program, and then do this step again.\nYou also may be asked where Praat is installed; Navigate to the location where Praat is installed, and double-click the “Praat.exe” file (on some systems the file may simply be called “Praat”). The Praat program may open, and then immediately close, as LaBB-CAT tests it can communicate with Praat.\n\nIf in doubt, check the  online help on the transcript page; it has a section explaining how to set up Praat integration on various browsers and operating systems.\n\nAfter a short delay, Praat should open, and show you a spectrogram of the line’s audio, with a TextGrid below that includes the words and the segments.\nIf you click on a word, and hit the &lt;tab&gt; key, the word’s interval is played. Try out various words, and see what you think about how accurate HTK has been with its alignment.\nTry this out with different lines in the transcript.\nYou will see that in some cases the alignment is pretty good, and in other cases, it’s not so good. In the not-so-good cases, see if you can figure out why HTK got it wrong.\n\nIf you had ‘edit’ rather than ‘read-only’ permissions in LaBB-CAT, then each time you opened an utterance in Praat, a button would appear in the transcript to the left of the line, labelled Import Changes. This button would allow you to save any adjustments you might want to make to the alignments back into the LaBB-CAT database.\n\nThese changes are flagged as manual edits, so if forced-alignment is run again, they will not be over-written with new bad alignments.\nThis mechanism can also be used to add other annotations from Praat into LaBB-CAT annotation layers.\n\n\nOnce the words and phone have been aligned with HTK there are a number of annotation possibilities that arise.\nFor example, word syllabification information can be retrieved from CELEX and combined with the aligned phones to construct aligned syllable annotations.\n\nTick the alignment project at the top of the transcript.\nThis reveals several layers.\nTick the syllables layer\nOnce the transcript has re-loaded, open an utterance with the EMU webApp or with Praat.\nYou will see that, in addition to aligned words and phones, the syllables are also aligned, and labelled with their phonemic transcription, and stressed syllables are prepended with an apostrophe.\n\nAlso, with exact word durations (i.e. excluding pauses in speech) and syllable counts, the speaker’s articulation rate, in syllables per minute, can be computed. The Statistics Layer Manager is a module that can be configured to compute sums, counts, and rates of various kinds over different scopes, including syllables per minute.\n\nTick the syllables per minute layer.\nYou will see that each utterance has a spanning annotation across the top of it, labelled with a number; that number is the articulation rate for that particular utterance.\nBoth local and global articulation rate can be calculated …\nClick on the name of the speaker at the top of the transcript.\nThis will open the participant attributes page for that speaker.\nYou will see that one of the attributes is Syllables per Minute; this is the speakers overall articulation rate, across all their utterances.\n\nArticulation rate is calculated by excluding the durations of inter-word pauses. These pauses themselves can be annotated, for search or analysis purposes.\n\nGo back to the transcript page.\nTick the previous pause layer.\nYou will see that many of the word tokens in the transcript are tagged with a number. These words are preceded by a pause in speech, and the number is the length of that pause in seconds.\nOpen an utterance in the EMU webApp or in Praat to confirm these pauses are correct.\n\nYou may notice that pauses in the middle of utterances are always right, but the pause before the first word in the utterance seems wrong. See if you can figure out why."
  },
  {
    "objectID": "worksheets/demo/4-alignment.html#searching",
    "href": "worksheets/demo/4-alignment.html#searching",
    "title": "4 - Alignment",
    "section": "",
    "text": "Given that HTK has created individually aligned phones in the database, those speech sounds can be searched and exported.\nLet’s say you’re particularly interested in the vowel in the word ‘KIT’. You can now identify and extract instances of that phoneme.\n\nClick the search link on the menu.\nTick the segment layer.\n\nThe segments layer contains annotations at the sub-word level - i.e. there are potentially multiple annotations per word, each annotation representing a single phone of the word. You will see that, as with other layers, there is a box on the segments layer for a regular expression.\nAs with other patterns in the search matrix, the pattern that you enter in the box is matched against individual annotations. So if you enter I (i.e. capital I) in the in the box, it will match each ‘KIT’ vowel segment in each word in the database.\n\n\n\n\n\n\nImportant\n\n\n\nIf you enter a pattern that would match more than a single character on this layer (i.e. more than a single phoneme) then no search results will be returned, because each annotation on this layer is only a single character long (remember the DISC encoding uses one character per phoneme).\nFor example, if you enter .*IN for your search, intending to match all words ending in “…ing”, then no results will be returned, because no single segment will ever match that pattern.\n\n\n\nWe want to search for all instances of the ‘KIT’ vowel, so enter I in the segments pattern box.\nClick Search\nAfter a short delay, you should see a list of results.\n\nYou will see that the results list words that have the ‘KIT’, but in many cases it’s not the main stressed vowel. What if we’re only interested in stressed ‘KIT’ vowels?\nThat’s ok, because we also have stress-marked syllable annotations, so we can add that layer to the search matrix, and identify only stressed vowels …\n\nNote down the number of results returned by your last search.\nBack on the search form, add the syllables layer to the search matrix.\nAs we have seen, stressed syllables are labelled with an apostrophe at the start. Enter a regular expression that will identify all syllables that start with apostrophe.\nIn this way, the results will give use all KIT vowels that are within a stressed syllable.\nClick Search again.\nThis time you will see fewer results returned, because we’ve filtered out the un-stressed version of the vowel.\n\n\nYou can export all these vowel tokens to a CSV file for analysis or further processing. The CSV file can include all kinds of other information, including participant and transcript attributes and other annotations.\n\nOn the results page, next to the CSV Export button there’s a ▼ button. Press it.\nYou will see several columns of checkboxes appear.\nTick the following checkboxes:\n\n\nUnder Participant tick gender, age_category, and syllables per minute\nUnder Span tick topic\nUnder Phrase tick syllables per minute\n\nNow press the CSV Export button above.\nSave and open the resulting file.\nYou will see that the file includes extra columns for the attributes and layers that you ticked (e.g. the topic marked in the original ELAN transcript, the speaker’s articulation rate, the local articulation rate, etc.).\n\nThe CSV file includes whatever annotation you might be interested, so you can go on to do qualitative or statistical analysis with other tools like Microsoft Excel or R. You can even add your own annotations to the CSV file and import them back into LaBB-CAT."
  },
  {
    "objectID": "worksheets/demo/4-alignment.html#acoustic-measurement",
    "href": "worksheets/demo/4-alignment.html#acoustic-measurement",
    "title": "4 - Alignment",
    "section": "",
    "text": "The CSV file also includes the columns “Target segments start” and “Target segments end”; these columns have the start and end time of the matching ‘KIT’ vowel token. Given this information, LaBB-CAT can extract acoustic measurements on the speech sounds using Praat.\n\n\n\n\n\n\nNote\n\n\n\nThe following steps work even if you don’t have Praat installed on your own computer, because Praat is used on the LaBB-CAT server …\n\n\n\nIn LaBB-CAT, click the upload menu option.\nClick the process with praat option.\nClick Browse and select the CSV results that you saved above.\nYou will see a form to fill in, and the first couple of settings (Transcript Name column and Participant column should be already filled in).\nFor the Start Time column, ensure that the Target segment start option is selected.\nFor the End Time column, ensure the Target segment end option is selected.\n\nThese two settings define the start/end times of the phone. For some measurements you might extract from Praat, processing signal that includes surrounding context is usually a good idea. You’ll see there’s a setting for that (which you can leave at the default of 0.025s), and you will see options for various measurements.\nThe default options are for F1 and F2 only, but if you feel like getting other measurements, feel free to tick those options too. You can expand each section with the ⯈ button to reveal more settings, which allow you to specify more detail about how Praat should do its computations. Again, feel free to look at those and try different settings.\n\nClick Process.\nYou will see a progress bar while LaBB-CAT generates Praat scripts and runs them.\nOnce Praat has finished processing the intervals, you will get a CSV file (you might have to click the CSV file with measurements link) - save and open it.\n\nYou will see that it’s a copy of the CSV file you uploaded, with some extra columns added on the right.\nDepending on your settings, this will include at least one column per measurement you selected (the formant columns also include on that contains the time at which the measurements were taken), and a final column called Error which is hopefully blank, but which might contain errors reported back by Praat (e.g. if it couldn’t find the audio file or ran into any other problem during processing).\n\nIn this worksheet you have seen that:\n\nHTK can be used to compute word and phone alignments automatically from your data.\nThe resulting alignments can be inspected and corrected directly from the transcript page.\nArticulation rate can be computed, excluding inter-word pauses.\nInter-word pauses can also be tagged.\nIndividual phone tokens can be searched for and extracted.\nAcoustic measurements for matching phones can also be made."
  },
  {
    "objectID": "worksheets/demo/2-search.html",
    "href": "worksheets/demo/2-search.html",
    "title": "2 - Search",
    "section": "",
    "text": "In addition to storing recordings and orthographic transcripts, the data can also be annotated in various ways with different information. Each type of annotation is stored on its own ‘layer’, so you can display and search on the basis of different aspects of the transcripts, including:\n\nfrequency\nlemma\npart of speech\npronunciation\nspeech rate\npause duration\n…and more.\n\nAnnotations can be made manually, and LaBB-CAT includes modules (called ‘Layer Managers’) for doing certain annotations automatically.\nVarious automatically generated annotation layers have been configured in the demo instance of LaBB-CAT, and we will start to explore some of them in this worksheet.\n\n\nLayered search is usually a two-step process: first you select which participants you want to search, using their participant attributes. And then you specify the pattern you want to search for.\nIf we were interested only in monolingual speakers, for example, we would filter out those that speak various language by setting the attribute values appropriately on the filter page.\n\nFirstly, return to LaBB-CAT’s home page by clicking the Home link on the menu, and then click the Layered Search icon.\nYou will see a page called “Participants”.\nSelect ‘M’ in the Gender box.\nYou will see a list of the male participants only.\nNotice that each participant has a check-box; if we wanted to, we could select specific participants from the list by checking/unchecking the boxes. (But in this case, let’s search all of them, so leave all the boxes un-ticked.)\nPress the Layered Search button at the top of the list.\nYou will see a page that lists the speakers at the top, a number of tickable annotation layers in the middle, a ‘Search Matrix’ below. (It doesn’t look much like a matrix yet, as it only includes the ‘orthography’ layer, but we will be adding rows and columns later on.)\nIn the box labelled “orthography” type the regular expression th[aeiou].+\nAs you saw earlier, [aeiou] means ‘any vowel’, and a full-stop/period means ‘any character’\nThe plus-sign means ‘one or more of the previous thing’, so .+ means ‘at least one character’.\nNow press the Search button at the bottom (or hit Enter).\nA progress bar will appear, and then shortly after that, a new window will open, which has a list of search results in it. Your browser’s popup-blocker might prevent the results page from opening - you can fix that either by allowing the popups in your browser, or by clicking the Display results link that appears after the search finishes.\n\nYou will see that the results include words like “that”, “there”, “then”, etc. - i.e. words that start with “th”, followed by a vowel, followed by at least one more letter.\n\n\n\n\n\n\nTip\n\n\n\n You can get more information about regular expressions by using the online help back on the search page, and also by clicking the the regular expressions link above the tickable layers.\n\n\nAs we previously saw with the ‘easy search’, each match is highlighted and shown within a few words context. However, this results page has a few more options available.\n\nIn the Context drop-down box at the top, select the 5 words option, to show more context in the llist of results.\nEach result line has a ticked checkbox next to it. At the bottom of the list, you’ll see that there are various buttons, which perform operations on the ticked results, including CSV Export, Utterance Export, and Audio Export.\nUntick the “Select all result” checkbox, and then tick a handful of results in the list.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can select a group of matches by ticking the first one, and then holding down the Shift key while ticking the last one.\n\n\n\nClick the Audio Export button.\nSave and open the resulting zip file. \nYou’ll see that extracted wav files are systematically named to include:\n\nthe name of the transcript\nthe start and end time of the extracted utterance\n\nIf you also have Praat installed on your computer, go back to the results page and click Utterance Export button. Save and open the resulting zip file.\nYou’ll see that the TextGrid names match the audio file names in the previous zip file.\nIf you open a TextGrid in Praat, you’ll see it includes a tier for the whole utterance transcript, a tier with an interval for each word, and a target… tier which tags the word that matched the regular expression you searched for.\nBack on the results page, click the CSV Export button.\nSave the resulting file, and open it.\nYou may have to specify some import options, in which case it may be handy to know that the field separator is comma, and the fields are quoted by speech marks.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you’re using Microsoft Excel and you find it doesn’t open all the columns correctly:\n\nCreate a new workbook in Excel.\nClick the ‘Data’ tab.\nOn the “Get External Data” ribbon click ‘From Text’.\nSelect the CSV file you downloaded.\nSelect ‘Delimited’ and click Next.\nEnsure ‘Comma’ is the only delimiter ticked and click Next.\nClick Finish and then OK.\n\n\n\nYou will see a spreadsheet with one line per selected result, and various columns containing information about the speaker, the corpus, the match line and word, and a URL to the interactive transcript for the match.\nWith this spreadsheet, you can work ‘offline’ with the results, tagging them, computing statistics in Excel, R, or any other program that can work with CSV files. We’ll look at a few more uses for the CSV results files later…\n\nClose the CSV file, and got back to the results page.\n\nUp until now, we’ve only been matching against one word at a time. Now we’re going to include patterns for a chain of words. Unlike the simple search, adding a space in the regular won’t work, because each column in the search matrix only matches a single word. To match a chain of two words, we need to have two columns in the search matrix.\n\nOn the search page, next to the orthography box where you entered the regular expression, there’s a ➕ button for adding a column to the matrix. Click it.\n\nNow you will see that our search matrix is one layer high by two words wide.\nChange the entries on the orthography layer so that it will match the word “the” followed immediately by a word that starts with a vowel, and click Search.\n\nCheck the search results are giving you what you expected. You may note that some of the following words start with a vowel in the spelling, even though they are not pronounced with a vowel sound. We will see how to search on the basis of pronunciation in another worksheet.\nNow search for “the” followed, within two words, by a word that starts with a vowel.\n\n\n\n\n\n\n\nTip\n\n\n\n If in doubt about a search option, try the online help page.\n\n\n\n\n\nSo far we have only searched the orthography layer - i.e. the ordinary spellings of words. But LaBB-CAT has been configured to generate a number of other annotation layers.\nLet’s say we’re interested in how rare or common words are in our data.\nLaBB-CAT’s ‘Frequency Layer Manager’ is a module that counts up the number of times each word type appears in the database. It generates a frequency list, and also annotates each word token with its frequency.\nWe’ll now search for tokens of words that appear only once in the database.\nThe annotation layers are grouped into a number of ‘projects’ to avoid clutter. We will initially be interested in the layers related to frequency.\n\nOn the search page, in the box labelled Tick layers to include, there’s a Projects column. Tick the frequency’ project.\nSome addition layers will appear in the layer list on the right.\nTick the word frequency layer.\nSet the word matrix to be 1 word wide again by clicking the ➖ button to the right.\nYou will see that the search matrix now has two layers in it. \n\nUnlike the orthography layer, which has one box for a regular expression, the word frequency layer has two boxes, marked “≥” and “&lt;”. This is because the annotation values are numbers. \nWe want all the words that appeared only once in the database. Enter a number or numbers in the appropriate box (you can leave either box blank) and click Search.\n\n\n\n\n\n\n\nImportant\n\n\n\nEnsure the orthography box is empty, otherwise it will be trying to find in instances of the word “the” that appear only once in the corpus; there are lots of instances of the word “the”, so the search will return no results, as the frequency is greater than 1.\n\n\n\nClick on the first result in the list.\n\n\n\n\nThis displays the ‘layered transcript’ page for the recording. This is similar to the previous ‘easy’ transcript page, but has a number of extra options and functions.\nThe most obvious difference is that each word token has a number above it. This is the frequency of that word, which is displayed because the word frequency layer is selected; there’s a list of layers at the top of the transcript, and you can see that both word frequency and word are ticked.\n\nUntick the word frequency layer.\nAfter a short delay, the transcript will be displayed again, with only the transcript text visible.\n\nThe transcript also includes any noises (e.g. “tuts”), comments, and other events that were put in the transcript in ELAN.\n\nThe video is the top right corner as before; click the play button. Again you will see a shaded rectangle following the participant’s speech.\nTry clicking the magnifying-glass icon  below the video, to see what it does.\nNow click on any word in the transcript.\nYou will see a menu appear.\nClick the play option in the menu to see what it does.\nClick on the formats link under the title.\nYou will see a menu, which includes various formats for exporting the transcript.\nSelect Plain Text Document\nSave the resulting file on your desktop, and then open it.\nYou will see the transcript in plain-text form.\nIf you have Praat installed on your computer, click the formats link, and select the Praat Text Grid option. Save the resulting file on your desktop, and then open it with Praat.\n\nYou will see that the TextGrid has various tiers, one for whole utterances (or two if there are two speakers), and one for individual words (or two if there are two speakers). \n\n(You will see that each individual word has a ‘default’ alignment - i.e. the words are evenly spread out during the duration of the line they’re in. In a later exercise we will look at ways to make these word alignments actually line up with the words in the audio signal)\n\nUsing frequencies of full wordforms can be useful, but in some circumstances it may be more informative to group together different forms of the same word; e.g. treat “damage”, “damaged” and “damaging” as variants of the same thing for the purposes of frequency-counting.\nWe’ll see a way to do that in the next worksheet.\n\nIn this worksheet you have seen that:\n\nAnnotations can be automatically added to transcripts in layers, using Layer Managers.\nThe Frequency Layer Manager can tag words with their frequencies, and maintains a frequency list.\nAnnotation layers can be searched using the search matrix, using numeric value or regular expressions.\nLayers can be optionally displayed in transcripts."
  },
  {
    "objectID": "worksheets/demo/2-search.html#layered-search-matrix",
    "href": "worksheets/demo/2-search.html#layered-search-matrix",
    "title": "2 - Search",
    "section": "",
    "text": "Layered search is usually a two-step process: first you select which participants you want to search, using their participant attributes. And then you specify the pattern you want to search for.\nIf we were interested only in monolingual speakers, for example, we would filter out those that speak various language by setting the attribute values appropriately on the filter page.\n\nFirstly, return to LaBB-CAT’s home page by clicking the Home link on the menu, and then click the Layered Search icon.\nYou will see a page called “Participants”.\nSelect ‘M’ in the Gender box.\nYou will see a list of the male participants only.\nNotice that each participant has a check-box; if we wanted to, we could select specific participants from the list by checking/unchecking the boxes. (But in this case, let’s search all of them, so leave all the boxes un-ticked.)\nPress the Layered Search button at the top of the list.\nYou will see a page that lists the speakers at the top, a number of tickable annotation layers in the middle, a ‘Search Matrix’ below. (It doesn’t look much like a matrix yet, as it only includes the ‘orthography’ layer, but we will be adding rows and columns later on.)\nIn the box labelled “orthography” type the regular expression th[aeiou].+\nAs you saw earlier, [aeiou] means ‘any vowel’, and a full-stop/period means ‘any character’\nThe plus-sign means ‘one or more of the previous thing’, so .+ means ‘at least one character’.\nNow press the Search button at the bottom (or hit Enter).\nA progress bar will appear, and then shortly after that, a new window will open, which has a list of search results in it. Your browser’s popup-blocker might prevent the results page from opening - you can fix that either by allowing the popups in your browser, or by clicking the Display results link that appears after the search finishes.\n\nYou will see that the results include words like “that”, “there”, “then”, etc. - i.e. words that start with “th”, followed by a vowel, followed by at least one more letter.\n\n\n\n\n\n\nTip\n\n\n\n You can get more information about regular expressions by using the online help back on the search page, and also by clicking the the regular expressions link above the tickable layers.\n\n\nAs we previously saw with the ‘easy search’, each match is highlighted and shown within a few words context. However, this results page has a few more options available.\n\nIn the Context drop-down box at the top, select the 5 words option, to show more context in the llist of results.\nEach result line has a ticked checkbox next to it. At the bottom of the list, you’ll see that there are various buttons, which perform operations on the ticked results, including CSV Export, Utterance Export, and Audio Export.\nUntick the “Select all result” checkbox, and then tick a handful of results in the list.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can select a group of matches by ticking the first one, and then holding down the Shift key while ticking the last one.\n\n\n\nClick the Audio Export button.\nSave and open the resulting zip file. \nYou’ll see that extracted wav files are systematically named to include:\n\nthe name of the transcript\nthe start and end time of the extracted utterance\n\nIf you also have Praat installed on your computer, go back to the results page and click Utterance Export button. Save and open the resulting zip file.\nYou’ll see that the TextGrid names match the audio file names in the previous zip file.\nIf you open a TextGrid in Praat, you’ll see it includes a tier for the whole utterance transcript, a tier with an interval for each word, and a target… tier which tags the word that matched the regular expression you searched for.\nBack on the results page, click the CSV Export button.\nSave the resulting file, and open it.\nYou may have to specify some import options, in which case it may be handy to know that the field separator is comma, and the fields are quoted by speech marks.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you’re using Microsoft Excel and you find it doesn’t open all the columns correctly:\n\nCreate a new workbook in Excel.\nClick the ‘Data’ tab.\nOn the “Get External Data” ribbon click ‘From Text’.\nSelect the CSV file you downloaded.\nSelect ‘Delimited’ and click Next.\nEnsure ‘Comma’ is the only delimiter ticked and click Next.\nClick Finish and then OK.\n\n\n\nYou will see a spreadsheet with one line per selected result, and various columns containing information about the speaker, the corpus, the match line and word, and a URL to the interactive transcript for the match.\nWith this spreadsheet, you can work ‘offline’ with the results, tagging them, computing statistics in Excel, R, or any other program that can work with CSV files. We’ll look at a few more uses for the CSV results files later…\n\nClose the CSV file, and got back to the results page.\n\nUp until now, we’ve only been matching against one word at a time. Now we’re going to include patterns for a chain of words. Unlike the simple search, adding a space in the regular won’t work, because each column in the search matrix only matches a single word. To match a chain of two words, we need to have two columns in the search matrix.\n\nOn the search page, next to the orthography box where you entered the regular expression, there’s a ➕ button for adding a column to the matrix. Click it.\n\nNow you will see that our search matrix is one layer high by two words wide.\nChange the entries on the orthography layer so that it will match the word “the” followed immediately by a word that starts with a vowel, and click Search.\n\nCheck the search results are giving you what you expected. You may note that some of the following words start with a vowel in the spelling, even though they are not pronounced with a vowel sound. We will see how to search on the basis of pronunciation in another worksheet.\nNow search for “the” followed, within two words, by a word that starts with a vowel.\n\n\n\n\n\n\n\nTip\n\n\n\n If in doubt about a search option, try the online help page."
  },
  {
    "objectID": "worksheets/demo/2-search.html#searching-other-layers",
    "href": "worksheets/demo/2-search.html#searching-other-layers",
    "title": "2 - Search",
    "section": "",
    "text": "So far we have only searched the orthography layer - i.e. the ordinary spellings of words. But LaBB-CAT has been configured to generate a number of other annotation layers.\nLet’s say we’re interested in how rare or common words are in our data.\nLaBB-CAT’s ‘Frequency Layer Manager’ is a module that counts up the number of times each word type appears in the database. It generates a frequency list, and also annotates each word token with its frequency.\nWe’ll now search for tokens of words that appear only once in the database.\nThe annotation layers are grouped into a number of ‘projects’ to avoid clutter. We will initially be interested in the layers related to frequency.\n\nOn the search page, in the box labelled Tick layers to include, there’s a Projects column. Tick the frequency’ project.\nSome addition layers will appear in the layer list on the right.\nTick the word frequency layer.\nSet the word matrix to be 1 word wide again by clicking the ➖ button to the right.\nYou will see that the search matrix now has two layers in it. \n\nUnlike the orthography layer, which has one box for a regular expression, the word frequency layer has two boxes, marked “≥” and “&lt;”. This is because the annotation values are numbers. \nWe want all the words that appeared only once in the database. Enter a number or numbers in the appropriate box (you can leave either box blank) and click Search.\n\n\n\n\n\n\n\nImportant\n\n\n\nEnsure the orthography box is empty, otherwise it will be trying to find in instances of the word “the” that appear only once in the corpus; there are lots of instances of the word “the”, so the search will return no results, as the frequency is greater than 1.\n\n\n\nClick on the first result in the list."
  },
  {
    "objectID": "worksheets/demo/2-search.html#layered-transcript",
    "href": "worksheets/demo/2-search.html#layered-transcript",
    "title": "2 - Search",
    "section": "",
    "text": "This displays the ‘layered transcript’ page for the recording. This is similar to the previous ‘easy’ transcript page, but has a number of extra options and functions.\nThe most obvious difference is that each word token has a number above it. This is the frequency of that word, which is displayed because the word frequency layer is selected; there’s a list of layers at the top of the transcript, and you can see that both word frequency and word are ticked.\n\nUntick the word frequency layer.\nAfter a short delay, the transcript will be displayed again, with only the transcript text visible.\n\nThe transcript also includes any noises (e.g. “tuts”), comments, and other events that were put in the transcript in ELAN.\n\nThe video is the top right corner as before; click the play button. Again you will see a shaded rectangle following the participant’s speech.\nTry clicking the magnifying-glass icon  below the video, to see what it does.\nNow click on any word in the transcript.\nYou will see a menu appear.\nClick the play option in the menu to see what it does.\nClick on the formats link under the title.\nYou will see a menu, which includes various formats for exporting the transcript.\nSelect Plain Text Document\nSave the resulting file on your desktop, and then open it.\nYou will see the transcript in plain-text form.\nIf you have Praat installed on your computer, click the formats link, and select the Praat Text Grid option. Save the resulting file on your desktop, and then open it with Praat.\n\nYou will see that the TextGrid has various tiers, one for whole utterances (or two if there are two speakers), and one for individual words (or two if there are two speakers). \n\n(You will see that each individual word has a ‘default’ alignment - i.e. the words are evenly spread out during the duration of the line they’re in. In a later exercise we will look at ways to make these word alignments actually line up with the words in the audio signal)\n\nUsing frequencies of full wordforms can be useful, but in some circumstances it may be more informative to group together different forms of the same word; e.g. treat “damage”, “damaged” and “damaging” as variants of the same thing for the purposes of frequency-counting.\nWe’ll see a way to do that in the next worksheet.\n\nIn this worksheet you have seen that:\n\nAnnotations can be automatically added to transcripts in layers, using Layer Managers.\nThe Frequency Layer Manager can tag words with their frequencies, and maintains a frequency list.\nAnnotation layers can be searched using the search matrix, using numeric value or regular expressions.\nLayers can be optionally displayed in transcripts."
  },
  {
    "objectID": "worksheets/demo/5-syntax.html",
    "href": "worksheets/demo/5-syntax.html",
    "title": "5 - Syntax",
    "section": "",
    "text": "5 - Syntax\nThe Stanford Parser is an open-source PCFG parser that can use grammars for a variety of languages including English.\nLaBB-CAT includes a Layer Manager that handles integration with the parser. The Stanford Parser Layer Manager:\n\nextracts chunks of transcripts (ideally sentences or clauses),\ngives them to the Stanford Parser for processing, which produces a ‘best parse’ for the utterance provided, and\nsaves the parse on a ‘tree’ layer, and optionally saves the resulting part-of-speech tags on a word layer.\n\nOne of the problems with parsing speech is that speakers often don’t speak in complete, well-formed sentences. In addition, the demo corpus you are using was not generally transcribed with parsing in mind, and so grammatically complete units have not been marked with full-stops, commas, etc. (Instead, full-stop has been used to mark short pauses in speech).\nFor these reasons, the parses you will see in this data may not be perfect. However, it’s possible to get a sense of the kinds of things that could be achieved with well-formed written texts, or speech that has been transcribed with grammatical punctuation included.\n\nClick the transcripts link on the menu.\n\nOne transcript in the database has delimiters inserted which divide the transcript into more or less grammatical units. As the ‘full-stop’ symbol is already being used to mark pauses, the ‘vertical bar’ symbol | has been used as a grammatical delimiter.\nThe transcript is called BR2044_OllyOhlson.eaf\n\nIn the Transcript box, type olly.\nAfter a short pause, BR2044_OllyOhlson.eaf will be the only transcript in the filtered list.\nClick BR2044_OllyOhlson.eaf\nUntick all except the word layer, to avoid clutter.\nWhen the transcript re-loads, tick the syntax project.\nThis reveals three layers.\nTick the parseable layer.\n\nWhen the transcript re-loads, you will see that almost all words in the transcript have been tagged with their own orthography. However, some words have not been tagged:\n\nfilled pauses like “um”, “ah , etc. and\nincomplete words like “re~”, “na~”, etc. - i.e. cases where the participant started saying something but changed their mind.\n\nThese have been identified by the Pattern Matcher Layer Manager, which can pick out words by regular expression, and has been configured to tag as ‘parseable’ all tokens except those that matching the following patterns:\n\na+h+ - e.g. “ah”, “aah”, “ahh”, …\nm+h*m+ - e.g. “mm”, “mmm”, “mhmm”, …\ne+r+ - e.g. “er”, “err”, “eeerr”, …\nu+m+ - e.g. “um”, “uum”, “ummm”, …\n.+~ - e.g. “re~”, “na~”, “w~”, ...\n\nThe result is that the parseable layer includes all words except filled pauses and incomplete words. This is the layer that is passed to the Stanford Parser for syntactic parsing.\nOne of the results of parsing is that each word token is tagged with its part-of-speech.\n\nUntick the parseable layer and tick the pos layer.\nYou will see that most of the words have been tagged with a syntactic category:\n\n“CC” Coordinating conjunction\n“DT” Determiner\n“JJ” Adjective\n“NN” Noun\n“NNS” Plural noun\n“NNP” Proper noun\n“PRP” Personal pronoun\n“PRP$” Possessive pronoun\n“VB” Verb\n“VBD” Past tense verb\n“VBG” Gerund\n“VBZ” 3rd person singular present verb\n…etc.\n\n\nThese, like any other word tags you have seen, can be included in searches or exported to CSV results files.\n\nThe other result from parsing is, of course, a ‘parse tree’ of each utterance.\n\nUntick the pos layer  and tick the parse layer.\nYou will see that above the words, there are bracketing annotations that are labelled with parts-of-speech or phrase labels.\nEach of these brackets represents a syntactic constituent constructed by the parser, smaller constituents at the bottom, building into larger constituents going up.\nClick on any constituent label (e.g. “NP” or “S”).\nA new window will open, which shows the selected utterance, using the familiar ‘upside-down tree’ representation.\nIf the tree appears far off and small, you can make it larger by widening the window, or ‘zooming in’ with you mouse wheel.\n\nYou can also search the parses themselves.\nClose the parse tree window.\nClick the search option on the menu.\nTick the syntax project.\nTick the parse and pos layers.\n\nOn the parse layer you’ll see that you can enter a search expression for annotations on that layer, just like any other.\nHowever, it also has a checkbox before and after the pattern. If you tick the checkbox before the pattern, it will anchor the search to the first word in the matching constituent. Similarly the checkbox after the pattern anchors to the last word in the constituent.\nLet’s say you want all the noun phrases that don’t start with a determiner like “the”, “this”, “a”, etc.\n\nEnter a pattern that matches NP (noun phrase) on the parse layer…\n...and tick the checkbox to anchor to the first word in the consituent.\nEnter a pattern that would match DT (determiner) on the pos layer…\n...and in the dropdown box before the pattern, select doesn’t match.\n\nThis will match words who have an annotation that doesn’t match “DT” on the *pos* layer.\n\n\nPress Search.\n\nWhen the search finishes, you should see that lots of nouns are returned, but nothing preceded by “the”, “a”, “that”, or any other determiner.\n\nIn this worksheet you have seen that:\n\nthe Stanford Parser can we used to annotate transcripts with part-of-speech tags and constituent annotations, and\nthe resulting annotations can be included in syntax-based searches.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023-2024 NZILBB"
  },
  {
    "objectID": "worksheets/demo/index.html",
    "href": "worksheets/demo/index.html",
    "title": "LaBB-CAT Demo Session",
    "section": "",
    "text": "LaBB-CAT Demo Session\n1 - Exploration\n2 - Search\n3 - CELEX\n4 - Alignment\n5 - Syntax\n6 - Other Processing\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023-2024 NZILBB"
  },
  {
    "objectID": "worksheets/demo/6-other-processing.html",
    "href": "worksheets/demo/6-other-processing.html",
    "title": "6 - Other Processing",
    "section": "",
    "text": "You have seen in a previous worksheet that articulation rate can be calculated over the words in individual utterances, and also over all the words uttered by each participant. There are other useful computations that can be computed over different scopes.\n\nSelect the transcripts link on the menu.\nYou may have previously noticed that the top of the page includes a form with various transcript attributes. This form allows you to both filter and sort the list of transcripts by transcript attribute values.\nFor the Word Count attribute, there are two boxes which you can use to specify a range of values.\nIn the right-hand To box, enter 1000.\nWhen the list reloads, you will see a list of all transcripts that have up to 1000 words.\nThis word count was computed by the Statistics Layer Manager, which has also been configured to compute speech duration in seconds and save the result in the “Duration” transcript attribute.\n\nAnother simply aggregate calculation is type/token ratio. The Demo LaBB-CAT has been configured to compute the type/token ratio for each participant.\n\nSelect the participants link on the menu.\nFor the corpus attribute, tick the QB option to list only participants recorded in the “Quake Box” portable recording studio.\nAt the top of the participant list, press the Export Attributes button.\nTick the Gender, Age and type/token ratio attributes.\nPress the Participant Data button.\nSave and open the resulting CSV file.\nYou will see that you have a list of their participants, with gender and age, and also a column for type/token ratio; this is the ratio expressed as a percentage.\n\n\n\n\nThe transcripts in this database each have a video and an audio file.\nHowever, some of the recordings have also been processed with a facial feature location algorithm. One of the results of this process was an annotated video; a copy of the original video, with the participant’s face located, along with various facial landmarks (position of the eyes,\nshape of mouth, etc.).\nLaBB-CAT supports having multiple media ‘track’ files for the same transcript, and for some of the transcripts, the annotated video has been uploaded as well as the original video.\n\nOn the transcripts page, list the transcripts with the Quake Face attribute set to 1 - true.\nOpen one of the listed transcripts.\n\nOn the top right of the page, by default, the original video is selected for display, but all the other media files available for the transcript are listed below the video, with a checkbox next to each.\nTick the checkbox next to the media file that ends with “…_face”. The transcript will reload and display the annotated video.\nPress Play to see the annotations (marked in green and red) change with the facial features.\nIt may be easier to see the annotations if you put the video in ‘full screen’ mode.\n\n\n\n\nIn addition to calculating word frequencies for direct analysis, frequencies can be compared to a reference corpus to calculated their ‘keyness’; a measure of whether the word is unusually frequent (a high positive keyness) or unusually infrequent (a low negative keyness).\nThe Demo LaBB-CAT has been configured to compute keyness compared to the frequencies available in the CELEX lexicon, which come from the Cobuild corpus.\n\nSelect the home link on the menu.\nClick the ‘Keyness’ icon.\nYou will see a form that allows you to search for particular spelling patterns, or export a list.\nPress the Search button without filling in the Pattern box, to list all words above the default Keyness threshold.\nA list of words will be displayed, each word with its keyness metric. The high-positive words (which are unusually frequent) are listed first, with the low-negative words (unusually infrequent) below.\n\nUnsurprisingly for this speech corpus, as compared to the mostly-written Cobuild corpus, words with high keyness include filled pauses like “um” and “ahh”, other words more likely in informal speech like “gonna” and “yeah”, topic-specific words like “earthquake” and “aftershocks”, and Canterbury place-names like “Christchurch” and “Brooklands”.\nThe Frequency Layer Manager can be configured to compute keyness of the data compared to any corpus for which you have word frequency data, or if you have several corpora within one LaBB-CAT database, each corpus can be compared to all the rest.\n\n\n\nLinguistic Inquiry and Word Count (LIWC) text analysis can be done with the LIWC Layer Manager and categorised word lists.\nSee: https://liwc.wpengine.com/how-it-works/\nOr: Tausczik & Pennebaker (2010) “The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods” Journal of Language and Social Psychology 29 (1) 24-54\nLIWC involves calculating the percentage of words in different categories. Categorised word lists can be purchased from liwc.wpengine.com, or can be compiled by hand.\nLIWC text analysis has been done on the Demo LaBB-CAT database, and also on the Cobuild corpus as a comparison corpus.\n\nClick the home link on the menu.\nClick the ‘LIWC’ icon.\nYou will see a horizontal bar graph: each bar represents category of words, with the bar length representing the percentage of that category’s usage in the database.\nTick the Cobuild checkbox.\nBars representing the percentages for the Cobuild corpus will be added to the graph, for comparison.\nPress the Export button.\nSave and open the resulting CSV file.\nYou will see that the file contains the list of categories, with two percentages for each category, first the percentage for the LaBB-CAT data, and then the percentage for the Cobuild corpus.\n\n\n\n\nLaBB-CAT can also integrate with the IBM Watson Personality Insights service.\n(https://www.ibm.com/watson/services/personality-insights/)\nThis is a web service that, given texts, provides personality metrics on the author (or speaker) of the text.\nThe transcripts in the Demo LaBB-CAT have been processed by the Personality Insights service. The results can be listed and visualised per speaker.\n\nSelect the home link on the menu.\nClick the ‘Personality’ icon.\nYou will see a list of participants that have been analysed.\nClick on the name of a participant.\nYou will see a ‘sunburst’ style visualisation of the participant’s personality metrics.\nBelow the visualisation, the categorised metrics are listed in a table.\n\n\nIn this worksheet you have seen that:\n\nthe Statistics Layer Manager can provide a variety of summary computations,\nthe transcript list can be filtered and sorted by transcript attributes,\ntranscripts can be linked to multiple media files,\nunusually frequent or infrequent words can be identified,\nLIWC text analysis can be automatically performed, and\npersonality metrics can be obtained for participants, based on their utterances."
  },
  {
    "objectID": "worksheets/demo/6-other-processing.html#aggregate-measures",
    "href": "worksheets/demo/6-other-processing.html#aggregate-measures",
    "title": "6 - Other Processing",
    "section": "",
    "text": "You have seen in a previous worksheet that articulation rate can be calculated over the words in individual utterances, and also over all the words uttered by each participant. There are other useful computations that can be computed over different scopes.\n\nSelect the transcripts link on the menu.\nYou may have previously noticed that the top of the page includes a form with various transcript attributes. This form allows you to both filter and sort the list of transcripts by transcript attribute values.\nFor the Word Count attribute, there are two boxes which you can use to specify a range of values.\nIn the right-hand To box, enter 1000.\nWhen the list reloads, you will see a list of all transcripts that have up to 1000 words.\nThis word count was computed by the Statistics Layer Manager, which has also been configured to compute speech duration in seconds and save the result in the “Duration” transcript attribute.\n\nAnother simply aggregate calculation is type/token ratio. The Demo LaBB-CAT has been configured to compute the type/token ratio for each participant.\n\nSelect the participants link on the menu.\nFor the corpus attribute, tick the QB option to list only participants recorded in the “Quake Box” portable recording studio.\nAt the top of the participant list, press the Export Attributes button.\nTick the Gender, Age and type/token ratio attributes.\nPress the Participant Data button.\nSave and open the resulting CSV file.\nYou will see that you have a list of their participants, with gender and age, and also a column for type/token ratio; this is the ratio expressed as a percentage."
  },
  {
    "objectID": "worksheets/demo/6-other-processing.html#other-media",
    "href": "worksheets/demo/6-other-processing.html#other-media",
    "title": "6 - Other Processing",
    "section": "",
    "text": "The transcripts in this database each have a video and an audio file.\nHowever, some of the recordings have also been processed with a facial feature location algorithm. One of the results of this process was an annotated video; a copy of the original video, with the participant’s face located, along with various facial landmarks (position of the eyes,\nshape of mouth, etc.).\nLaBB-CAT supports having multiple media ‘track’ files for the same transcript, and for some of the transcripts, the annotated video has been uploaded as well as the original video.\n\nOn the transcripts page, list the transcripts with the Quake Face attribute set to 1 - true.\nOpen one of the listed transcripts.\n\nOn the top right of the page, by default, the original video is selected for display, but all the other media files available for the transcript are listed below the video, with a checkbox next to each.\nTick the checkbox next to the media file that ends with “…_face”. The transcript will reload and display the annotated video.\nPress Play to see the annotations (marked in green and red) change with the facial features.\nIt may be easier to see the annotations if you put the video in ‘full screen’ mode."
  },
  {
    "objectID": "worksheets/demo/6-other-processing.html#keyness",
    "href": "worksheets/demo/6-other-processing.html#keyness",
    "title": "6 - Other Processing",
    "section": "",
    "text": "In addition to calculating word frequencies for direct analysis, frequencies can be compared to a reference corpus to calculated their ‘keyness’; a measure of whether the word is unusually frequent (a high positive keyness) or unusually infrequent (a low negative keyness).\nThe Demo LaBB-CAT has been configured to compute keyness compared to the frequencies available in the CELEX lexicon, which come from the Cobuild corpus.\n\nSelect the home link on the menu.\nClick the ‘Keyness’ icon.\nYou will see a form that allows you to search for particular spelling patterns, or export a list.\nPress the Search button without filling in the Pattern box, to list all words above the default Keyness threshold.\nA list of words will be displayed, each word with its keyness metric. The high-positive words (which are unusually frequent) are listed first, with the low-negative words (unusually infrequent) below.\n\nUnsurprisingly for this speech corpus, as compared to the mostly-written Cobuild corpus, words with high keyness include filled pauses like “um” and “ahh”, other words more likely in informal speech like “gonna” and “yeah”, topic-specific words like “earthquake” and “aftershocks”, and Canterbury place-names like “Christchurch” and “Brooklands”.\nThe Frequency Layer Manager can be configured to compute keyness of the data compared to any corpus for which you have word frequency data, or if you have several corpora within one LaBB-CAT database, each corpus can be compared to all the rest."
  },
  {
    "objectID": "worksheets/demo/6-other-processing.html#linguistic-inquiry-and-word-count",
    "href": "worksheets/demo/6-other-processing.html#linguistic-inquiry-and-word-count",
    "title": "6 - Other Processing",
    "section": "",
    "text": "Linguistic Inquiry and Word Count (LIWC) text analysis can be done with the LIWC Layer Manager and categorised word lists.\nSee: https://liwc.wpengine.com/how-it-works/\nOr: Tausczik & Pennebaker (2010) “The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods” Journal of Language and Social Psychology 29 (1) 24-54\nLIWC involves calculating the percentage of words in different categories. Categorised word lists can be purchased from liwc.wpengine.com, or can be compiled by hand.\nLIWC text analysis has been done on the Demo LaBB-CAT database, and also on the Cobuild corpus as a comparison corpus.\n\nClick the home link on the menu.\nClick the ‘LIWC’ icon.\nYou will see a horizontal bar graph: each bar represents category of words, with the bar length representing the percentage of that category’s usage in the database.\nTick the Cobuild checkbox.\nBars representing the percentages for the Cobuild corpus will be added to the graph, for comparison.\nPress the Export button.\nSave and open the resulting CSV file.\nYou will see that the file contains the list of categories, with two percentages for each category, first the percentage for the LaBB-CAT data, and then the percentage for the Cobuild corpus."
  },
  {
    "objectID": "worksheets/demo/6-other-processing.html#personality",
    "href": "worksheets/demo/6-other-processing.html#personality",
    "title": "6 - Other Processing",
    "section": "",
    "text": "LaBB-CAT can also integrate with the IBM Watson Personality Insights service.\n(https://www.ibm.com/watson/services/personality-insights/)\nThis is a web service that, given texts, provides personality metrics on the author (or speaker) of the text.\nThe transcripts in the Demo LaBB-CAT have been processed by the Personality Insights service. The results can be listed and visualised per speaker.\n\nSelect the home link on the menu.\nClick the ‘Personality’ icon.\nYou will see a list of participants that have been analysed.\nClick on the name of a participant.\nYou will see a ‘sunburst’ style visualisation of the participant’s personality metrics.\nBelow the visualisation, the categorised metrics are listed in a table.\n\n\nIn this worksheet you have seen that:\n\nthe Statistics Layer Manager can provide a variety of summary computations,\nthe transcript list can be filtered and sorted by transcript attributes,\ntranscripts can be linked to multiple media files,\nunusually frequent or infrequent words can be identified,\nLIWC text analysis can be automatically performed, and\npersonality metrics can be obtained for participants, based on their utterances."
  },
  {
    "objectID": "worksheets/demo/3-celex.html",
    "href": "worksheets/demo/3-celex.html",
    "title": "3 - CELEX",
    "section": "",
    "text": "In some circumstances it can be useful to group together different forms of the same word; e.g. treat “damage”, “damaged” and “damaging” as variants of the same thing for the purposes of frequency-counting and other analyses.\nThe demo database has been configured to tag each token with it’s root form or ‘Lemma’. To do this, LaBB-CAT has been integrated with the CELEX lexicon, which can be purchased from the Linguistic Data Consortium (LDC) and includes lemma, part of speech, morphological,  phonological, and frequency information for English, German, and Dutch.\nThere is a lemma layer configured, which looks up each word token in the CELEX lexicon, and tags it with its lemma.\nFor words that are missing from CELEX, LaBB-CAT is configured to instead tag the word with it’s ‘stem’ according to the Porter Algorithm (Porter, 1980, An algorithm for suffix stripping, Program, Vol. 14, no. 3, pp 130-137, or http://www.tartarus.org/~martin/PorterStemmer).\n\nIn the transcript you have open, under the “projects” heading, tick the celex project.\nA number of extra layer options will appear in the layer list.\nTick the lemma layer.\nWhen the transcript re-loads, you’ll see that each word is tagged with its lemma; in some cases the lemma is the same as the word-form, and in other cases, the lemma has suffixes stripped off, etc.\nSearch for the word “damage” on the page (in most browsers, Ctrl + F or some similar keyboard combination allows you to search for text on the current page).\nYou should see that variants like “damage”, “damaged” and “damaging” are all tagged with the same lemma: “damage”.\nNow tick the frequency project.\nThree layer options appear; word frequency, lemma frequency, and liwc. You have already seen (and searched) the word frequency layer.\nThe lemma frequency layer is similarly generated by the Frequency Layer Manager, but instead of counting up raw word forms from the orthography layer, it counts based on the lemma layer.\nTick both word frequency and lemma frequency\nFind the word “damaging” in the text.\nYou’ll see that, although the word-form is very low frequency, the lemma frequency is somewhat higher (as you’d expect in a corpus of earthquake stories!).\n\nThe Frequency Layer Manager also keeps a straight word-list with word counts for each corpus…\n\nClick the home menu option at the top.\nClick the Frequency Layer Manager icon.\nYou will see a drop-down box with each frequency layer in it.\nSelect Lemma Frequency and press Select.\nPress the Export button at the bottom.\nSave and open the resulting CSV file.\nYou will see an alphabetical list of all the distinct lemmas in the database, and next to each, a count of the number of tokens of that type.\n\n\n\n\nThere is other information in CELEX that can be used to tag words. Let’s say you’re interested in the morphological suffix ~ing. If we search for .*ing on the orthoghraphy layer, we’ll get a number of false positives ...\n\nSelect the search menu option.\nSearch for .*ing on the orthography layer.\nYou will see that the results include words like like “thing” and “everything” whose “ing” is part of the base word, not a morphological affix.\nLeave the results tab open, so you can compare these results with the next search …\nSwap back to the search matrix page, tick the ‘celex’ project, and add the morphology layer to the search matrix.\nNow do a search on the morphology layer of words ending in +ing, and compare the results with the orthography-based search.\n\n\n\n\n\n\n\nImportant\n\n\n\nRemember that in regular expressions the ‘plus’ character + has a special meaning - it means “one or more of the previous thing”. But we are now searching for actual literal + characters.\nIn order to search for a literal “+” in the annotation, you have to ‘escape’ the +. Consult the regular expressions help page to figure out how to do that.\n\n\nThe results should now contain only words for which the ~ing is a morphological suffix.\n\n\n\nThe CELEX lexicon includes phonological information, so we can tag each word with its phonemic transcription, and view/search the pronunciations of words.\n\nClick the transcripts link on the menu.\nClick the name of the first transcript listed, to display the transcript text.\nTick the celex project.\nTick the phonemes layer.\n\nYou will see that each word is tagged with its phonemic transcription using the International Phonetic Alphabet (IPA). However, CELEX doesn’t use IPA symbols directly, it actually uses the ‘DISC’ encoding for phonemes, which uses ordinary ‘typewriter’ characters (ASCII), and uses exactly one character per phoneme.\nThe IPA symbols are being displayed by LaBB-CAT to provide a linguist-friendly representation of the phonemic transcription. But you can see the underlying DISC characters by selecting the ‘ASCII’ option on the layer in the transcript.\n\nSelect ‘ASCII’ on the phonemes layer, to see what CELEX is actually producing.\nYou may find that this is somewhat harder to read. Diphthongs are generally represented by digits, schwa is “@”, and various other characters are used to represent affricates, etc.\n\nIt’s nice to display the IPA symbols, but it’s important to understand the DISC symbols (shown in the table below), because they are what we have to use when searching on the phonemes layer, which we are going to try now.\n\n\n\nIPA\nDISC\n \n \nIPA\nDISC\n \n\n\np\np\npat\n \nɪ\nI\nKIT\n\n\nb\nb\nbad\n \nε\nE\nDRESS\n\n\nt\nt\ntack\n \næ\n{\nTRAP\n\n\nd\nd\ndad\n \nʌ\nV\nSTRUT\n\n\nk\nk\ncad\n \nɒ\nQ\nLOT\n\n\ng\ng\ngame\n \nʊ\nU\nFOOT\n\n\nŋ\nN\nbang\n \nə\n@\nanother\n\n\nm\nm\nmat\n \ni:\ni\nFLEECE\n\n\nn\nn\nnat\n \nα: \n#\nfather\n\n\nl\nl\nlad\n \nɔ:\n$\nTHOUGHT\n\n\nr\nr\nrat\n \nu:\nu\nGOOSE\n\n\nf\nf\nfat\n \nɜ:\n3\nNURSE\n\n\nv\nv\nvat\n \neɪ\n1\nFACE\n\n\nθ\nT\nthin\n \nαɪ\n2\nPRICE\n\n\nð\nD\nthen\n \nɔɪ\n4\nCHOICE\n\n\ns\ns\nsap\n \nəʊ\n5\nGOAT\n\n\nz\nz\nzap\n \nαʊ\n6\nMOUTH\n\n\n∫\nS\nsheep\n \nɪə\n7\nNEAR\n\n\nʒ\nZ\nmeasure\n \nεə\n8\nSQUARE\n\n\nj\nj\nyank\n \nʊə\n9\nCURE\n\n\nx\nx\nloch\n \næ\nc\ntimbre\n\n\nh\nh\nhad\n \nɑ̃ː\nq\ndétente\n\n\nw\nw\nwet\n \næ̃ː\n0\nlingerie\n\n\nʧ\nJ\ncheap\n \nɒ̃ː\n~\nbouillon\n\n\nʤ\n_\njeep\n \n \n \n \n\n\nŋ̩\nC\nbacon\n \n \n \n \n\n\nm̩\nF\nidealism\n \n \n \n \n\n\nn̩\nH\nburden\n \n \n \n \n\n\nl̩\nP\ndangle\n \n \n \n \n\n\n\n\nGo to the search page.\nCreate a search matrix that’s two words wide, and includes the orthography and phonemes layers.\n\nNow we’re going to do a search for the word “the” followed by a word that starts with schwa.\n\nType the in the first orthography box.\nClick the second box on the phonemes layer, but don’t enter anything in the box yet.\nThe box has a little « button to the right of it.\nHover the mouse over it to see what it says, and then click it.\nYou will see that a section opens with a bunch of phoneme symbols on it.\nFind the schwa symbol ə and click it.\nYou will see that a @* symbol appears in the box.\n@* is the DISC symbol for ə, so in order to search for schwa, we have to use it in our search pattern.\nWe want words that start with schwa, so type .* after the @ symbol.\nClick Search.\nYou will see that some of the words being matched are words that you might not normally think start with a schwa. LaBB-CAT is matching words against all their possible phonemic transcriptions, so if CELEX has multiple possible pronunciations for a word, and one of them starts with schwa, it will be matched.\n\nWith the phonemic transcriptions, we can do a better job of the search we tried in an earlier exercise - “the” followed by a word starting with a vowel…\n\nChange your search so that, instead of just @ at the beginning of the word, it matches any vowel.\n\n\n\n\n\n\n\nNote\n\n\n\nYou could use the square-brackets [] at the start of your pattern, and type all vowel symbols inside them - Note that the vowels in the DISC representation extend beyond a, e, i, o, and u - you should add in all the vowels you see in the list that appears when you expand the Phoneme Symbol Selector, including all the diphthongs.\nAlternatively, you can simply click the VOWEL link in the Phoneme Symbol Selector, which will add all the DISC vowels for you, already enclosed in square-brackets.\n\n\n\nRun the search and check that it’s giving you what you expect. Notice that now there are no ‘false positives’ like “the one” that we were getting when searching by orthography alone.\n\nNow that you’ve seen a few different layers, and how the search matrix works, you might want to try out some of the following searches, or invent some others:\n\nWords which have the vowel in DRESS as the second phoneme\nThe word “the” followed by a word beginning with the phoneme /k/\nWords that begin with “k” in their spelling, but begin with the phoneme /n/\nWords that begin with “k” in their spelling, but do not begin with the phoneme /n/\n\n\nIn this worksheet you have seen that:\n\nThe CELEX Layer Manager tags words with information from the CELEX lexicon.\nPhonemic transcription layers can be used to search on the basis of pronunciation.\nAlthough pronunciations can be displayed with IPA symbols, CELEX uses DISC to encode phonemes, so DISC must be used for searches."
  },
  {
    "objectID": "worksheets/demo/3-celex.html#lemma",
    "href": "worksheets/demo/3-celex.html#lemma",
    "title": "3 - CELEX",
    "section": "",
    "text": "In some circumstances it can be useful to group together different forms of the same word; e.g. treat “damage”, “damaged” and “damaging” as variants of the same thing for the purposes of frequency-counting and other analyses.\nThe demo database has been configured to tag each token with it’s root form or ‘Lemma’. To do this, LaBB-CAT has been integrated with the CELEX lexicon, which can be purchased from the Linguistic Data Consortium (LDC) and includes lemma, part of speech, morphological,  phonological, and frequency information for English, German, and Dutch.\nThere is a lemma layer configured, which looks up each word token in the CELEX lexicon, and tags it with its lemma.\nFor words that are missing from CELEX, LaBB-CAT is configured to instead tag the word with it’s ‘stem’ according to the Porter Algorithm (Porter, 1980, An algorithm for suffix stripping, Program, Vol. 14, no. 3, pp 130-137, or http://www.tartarus.org/~martin/PorterStemmer).\n\nIn the transcript you have open, under the “projects” heading, tick the celex project.\nA number of extra layer options will appear in the layer list.\nTick the lemma layer.\nWhen the transcript re-loads, you’ll see that each word is tagged with its lemma; in some cases the lemma is the same as the word-form, and in other cases, the lemma has suffixes stripped off, etc.\nSearch for the word “damage” on the page (in most browsers, Ctrl + F or some similar keyboard combination allows you to search for text on the current page).\nYou should see that variants like “damage”, “damaged” and “damaging” are all tagged with the same lemma: “damage”.\nNow tick the frequency project.\nThree layer options appear; word frequency, lemma frequency, and liwc. You have already seen (and searched) the word frequency layer.\nThe lemma frequency layer is similarly generated by the Frequency Layer Manager, but instead of counting up raw word forms from the orthography layer, it counts based on the lemma layer.\nTick both word frequency and lemma frequency\nFind the word “damaging” in the text.\nYou’ll see that, although the word-form is very low frequency, the lemma frequency is somewhat higher (as you’d expect in a corpus of earthquake stories!).\n\nThe Frequency Layer Manager also keeps a straight word-list with word counts for each corpus…\n\nClick the home menu option at the top.\nClick the Frequency Layer Manager icon.\nYou will see a drop-down box with each frequency layer in it.\nSelect Lemma Frequency and press Select.\nPress the Export button at the bottom.\nSave and open the resulting CSV file.\nYou will see an alphabetical list of all the distinct lemmas in the database, and next to each, a count of the number of tokens of that type."
  },
  {
    "objectID": "worksheets/demo/3-celex.html#morphology",
    "href": "worksheets/demo/3-celex.html#morphology",
    "title": "3 - CELEX",
    "section": "",
    "text": "There is other information in CELEX that can be used to tag words. Let’s say you’re interested in the morphological suffix ~ing. If we search for .*ing on the orthoghraphy layer, we’ll get a number of false positives ...\n\nSelect the search menu option.\nSearch for .*ing on the orthography layer.\nYou will see that the results include words like like “thing” and “everything” whose “ing” is part of the base word, not a morphological affix.\nLeave the results tab open, so you can compare these results with the next search …\nSwap back to the search matrix page, tick the ‘celex’ project, and add the morphology layer to the search matrix.\nNow do a search on the morphology layer of words ending in +ing, and compare the results with the orthography-based search.\n\n\n\n\n\n\n\nImportant\n\n\n\nRemember that in regular expressions the ‘plus’ character + has a special meaning - it means “one or more of the previous thing”. But we are now searching for actual literal + characters.\nIn order to search for a literal “+” in the annotation, you have to ‘escape’ the +. Consult the regular expressions help page to figure out how to do that.\n\n\nThe results should now contain only words for which the ~ing is a morphological suffix."
  },
  {
    "objectID": "worksheets/demo/3-celex.html#phonology",
    "href": "worksheets/demo/3-celex.html#phonology",
    "title": "3 - CELEX",
    "section": "",
    "text": "The CELEX lexicon includes phonological information, so we can tag each word with its phonemic transcription, and view/search the pronunciations of words.\n\nClick the transcripts link on the menu.\nClick the name of the first transcript listed, to display the transcript text.\nTick the celex project.\nTick the phonemes layer.\n\nYou will see that each word is tagged with its phonemic transcription using the International Phonetic Alphabet (IPA). However, CELEX doesn’t use IPA symbols directly, it actually uses the ‘DISC’ encoding for phonemes, which uses ordinary ‘typewriter’ characters (ASCII), and uses exactly one character per phoneme.\nThe IPA symbols are being displayed by LaBB-CAT to provide a linguist-friendly representation of the phonemic transcription. But you can see the underlying DISC characters by selecting the ‘ASCII’ option on the layer in the transcript.\n\nSelect ‘ASCII’ on the phonemes layer, to see what CELEX is actually producing.\nYou may find that this is somewhat harder to read. Diphthongs are generally represented by digits, schwa is “@”, and various other characters are used to represent affricates, etc.\n\nIt’s nice to display the IPA symbols, but it’s important to understand the DISC symbols (shown in the table below), because they are what we have to use when searching on the phonemes layer, which we are going to try now.\n\n\n\nIPA\nDISC\n \n \nIPA\nDISC\n \n\n\np\np\npat\n \nɪ\nI\nKIT\n\n\nb\nb\nbad\n \nε\nE\nDRESS\n\n\nt\nt\ntack\n \næ\n{\nTRAP\n\n\nd\nd\ndad\n \nʌ\nV\nSTRUT\n\n\nk\nk\ncad\n \nɒ\nQ\nLOT\n\n\ng\ng\ngame\n \nʊ\nU\nFOOT\n\n\nŋ\nN\nbang\n \nə\n@\nanother\n\n\nm\nm\nmat\n \ni:\ni\nFLEECE\n\n\nn\nn\nnat\n \nα: \n#\nfather\n\n\nl\nl\nlad\n \nɔ:\n$\nTHOUGHT\n\n\nr\nr\nrat\n \nu:\nu\nGOOSE\n\n\nf\nf\nfat\n \nɜ:\n3\nNURSE\n\n\nv\nv\nvat\n \neɪ\n1\nFACE\n\n\nθ\nT\nthin\n \nαɪ\n2\nPRICE\n\n\nð\nD\nthen\n \nɔɪ\n4\nCHOICE\n\n\ns\ns\nsap\n \nəʊ\n5\nGOAT\n\n\nz\nz\nzap\n \nαʊ\n6\nMOUTH\n\n\n∫\nS\nsheep\n \nɪə\n7\nNEAR\n\n\nʒ\nZ\nmeasure\n \nεə\n8\nSQUARE\n\n\nj\nj\nyank\n \nʊə\n9\nCURE\n\n\nx\nx\nloch\n \næ\nc\ntimbre\n\n\nh\nh\nhad\n \nɑ̃ː\nq\ndétente\n\n\nw\nw\nwet\n \næ̃ː\n0\nlingerie\n\n\nʧ\nJ\ncheap\n \nɒ̃ː\n~\nbouillon\n\n\nʤ\n_\njeep\n \n \n \n \n\n\nŋ̩\nC\nbacon\n \n \n \n \n\n\nm̩\nF\nidealism\n \n \n \n \n\n\nn̩\nH\nburden\n \n \n \n \n\n\nl̩\nP\ndangle\n \n \n \n \n\n\n\n\nGo to the search page.\nCreate a search matrix that’s two words wide, and includes the orthography and phonemes layers.\n\nNow we’re going to do a search for the word “the” followed by a word that starts with schwa.\n\nType the in the first orthography box.\nClick the second box on the phonemes layer, but don’t enter anything in the box yet.\nThe box has a little « button to the right of it.\nHover the mouse over it to see what it says, and then click it.\nYou will see that a section opens with a bunch of phoneme symbols on it.\nFind the schwa symbol ə and click it.\nYou will see that a @* symbol appears in the box.\n@* is the DISC symbol for ə, so in order to search for schwa, we have to use it in our search pattern.\nWe want words that start with schwa, so type .* after the @ symbol.\nClick Search.\nYou will see that some of the words being matched are words that you might not normally think start with a schwa. LaBB-CAT is matching words against all their possible phonemic transcriptions, so if CELEX has multiple possible pronunciations for a word, and one of them starts with schwa, it will be matched.\n\nWith the phonemic transcriptions, we can do a better job of the search we tried in an earlier exercise - “the” followed by a word starting with a vowel…\n\nChange your search so that, instead of just @ at the beginning of the word, it matches any vowel.\n\n\n\n\n\n\n\nNote\n\n\n\nYou could use the square-brackets [] at the start of your pattern, and type all vowel symbols inside them - Note that the vowels in the DISC representation extend beyond a, e, i, o, and u - you should add in all the vowels you see in the list that appears when you expand the Phoneme Symbol Selector, including all the diphthongs.\nAlternatively, you can simply click the VOWEL link in the Phoneme Symbol Selector, which will add all the DISC vowels for you, already enclosed in square-brackets.\n\n\n\nRun the search and check that it’s giving you what you expect. Notice that now there are no ‘false positives’ like “the one” that we were getting when searching by orthography alone.\n\nNow that you’ve seen a few different layers, and how the search matrix works, you might want to try out some of the following searches, or invent some others:\n\nWords which have the vowel in DRESS as the second phoneme\nThe word “the” followed by a word beginning with the phoneme /k/\nWords that begin with “k” in their spelling, but begin with the phoneme /n/\nWords that begin with “k” in their spelling, but do not begin with the phoneme /n/\n\n\nIn this worksheet you have seen that:\n\nThe CELEX Layer Manager tags words with information from the CELEX lexicon.\nPhonemic transcription layers can be used to search on the basis of pronunciation.\nAlthough pronunciations can be displayed with IPA symbols, CELEX uses DISC to encode phonemes, so DISC must be used for searches."
  },
  {
    "objectID": "worksheets/demo/1-exploration.html",
    "href": "worksheets/demo/1-exploration.html",
    "title": "1 - Exploration",
    "section": "",
    "text": "LaBB-CAT is a speech/language corpus management system that:\n\nstores transcripts with audio/video\n\nsupporting a variety of formats\nand the definition of speech elicitation tasks;\n\nallows the addition of different layers of annotation, which can\n\nbe manual or automatic, and\nhave different granularities, from topic tagging to individual phones;\n\nsupports forced alignment to phone level using a speech recognition toolkit called “HTK”, or the Montreal Forced Aligner, or the WebMAUS service provided by BAS Web Services;\nallows cross-layer regular-expression search;\nsearch results are exportable to CSV for further analysis;\nbatch acoustic measurement of segments using Praat is also supported, and\ntranscripts and fragments of them are exportable in a variety of formats.\n\nIn this worksheet you will start exploring a demo LaBB-CAT corpus, to get a general idea of how to find your way around LaBB-CAT and how the language data is presented.\nThe demo corpus contains a collection of videos of people telling stories about their experiences during the earthquakes that struck Canterbury during 2010 and 2011. They have been orthographically transcribed using a tool called ELAN, so they have been time aligned to the utterance level; i.e. the start and end time of each line in the transcript has been manually synchronized with the recording. The ELAN transcripts, and their video and audio files, have been uploaded into LaBB-CAT.\nLaBB-CAT is a browser-based system so the first thing to do is access it with your web browser. Generally, any modern browser should be fine (although some features you’ll see in later worksheets are only supported by Mozilla Firefox or Google Chrome).\n\nIn your web browser, type in the following URL:\nhttps://labbcat.canterbury.ac.nz/demo\nYou will be asked for a username and password.\n\n\n\n\n\n\n\nImportant\n\n\n\nIf typing this out manually, ensure you enter ‘https’ not ‘http’\n\n\n\nThe username is demo and the password is demo\nThe very first time you access LaBB-CAT, you will see its licence agreement.\nScroll to the bottom of the page and click I Agree to continue.\nYou will see a page called “LaBB-CAT Demo” which has a menu of links along the top and a number of icons. Below the icons is some information about the corpus. This is the LaBB-CAT home page.\nClick the where do I start? icon on the left.\nThe help page that pops up includes a brief description of LaBB-CAT and some tips for navigation and getting more information.\nRead through the page, and then close the browser tab to return to the home page.\n\nThere are two main ways to use LaBB-CAT:\n\neasy exploration and plain text search (this worksheet)\nlayered filtering and search (the following worksheets)\n\n\n\nUsing the ‘easy’ method for exploring LaBB-CAT is simple, as annotation layers are largely ignored; each transcript is essentially treated as a plain text that you can search and display based on ordinary orthographic spelling.\n\nOn the LaBB-CAT home page, click the explore icon to access the easy exploration pages.\nYou will see a similar home page, with Browse icon, Easy Search, and Layered Search icons.\nClick the Browse icon.\nYou will see a page that lists the collections of recordings (or ‘corpora’) in the LaBB-CAT database. Each corpus contains a number of recordings.\nClick the first corpus listed.\nYou will see a page that lists the first 20 recordings in the corpus. The recording names are on the left, followed by some meta data (called ‘participant attributes’) about the participant in the recording. At the bottom is a list of pages, so you can access further recordings in the corpus.\nClick the name of the first recording.\nYou will see a page with transcript text, and the video appears in the top right corner of the page.\nHover the mouse over the video.\nThe video pane grows larger.\nPress the play button.\nAs the video plays, you will see the current utterance highlighted in the transcript. You will also see that the current utterance appears as closed captions in the video. You can use the video controls as normal, including the full-screen button in the bottom right, to make the video occupy the whole screen.\nPause the recording.\nMove the mouse over one of the utterances further down the transcript.\nYou will notice that the video pane shrinks again, and that the mouse pointer becomes a play button.\nClick the utterance.\nYou will see that playback starts at that utterance. Playback will stop when the participant finishes the utterance.\nAt the top of the page, you will see a tab button labelled General; click it.\nYou will see some meta-data about the transcript and recording.\nLaBB-CAT attaches meta-data both to transcripts (called ‘transcript attributes’), and also to participants (‘participant attributes’).\nBelow the transcript attributes is the name of the participant. Click their name.\nYou will see a page with the participant attributes, and a list of the recordings they appear in. In this case, they appear in only one recording; if you were to click the name of the recording, you would be taken back to the transcript page you’ve just seen.\n\n\n\n\n\nOn the menu at the top of the page, there’s a Search option. Click it.\nYou will see a search form with a “Text” search box at the top, and options for meta data below.\nIn the “Text” box enter the word quake and press the Search button at the bottom.\nYou will see a list of hits, with the name of the transcript on the left and the matched word on the right, highlighted within its immediate context.\nClick the Search again link (or the Search link on the menu at the top)\nYou’ll see that the search form remembers the last search text.\nSelect ‘Female’ from the Gender drop-down box, leave the word quake in the “Text” box, and click Search.\nThis time the results are narrowed down to only female participants.\nClick the first result.\nYou will see the transcript page, as we saw earlier, but with each match from the search highlighted.\n\n\n\n\nYou can also search across multiple words, and search for patterns as well as exact spellings.\nFor example, let’s say you want to investigate how the pronunciation of the word ‘the’ changes when the following word starts with a vowel. You can search for this pattern using the search form:\n\nClick the Search option on the menu.\nSearch for the word the\nYou will see that there are lots of results, including many where ‘the’ is followed by a word that starts with a consonant.\nGo back to the Search page.\nNow search for: the [aeiou].*\nThis is a ‘regular expression’ that allows you to identify a pattern, with the following parts:\n\nthe word ‘the’\nfollowed by a space\nfollowed by any vowel ([aeiou])\nfollowed anything at all - . in a regular expression means ‘any character’, and * means ‘zero or more of the previous thing’, so .* means ‘zero or more characters’\n\nYou will see that the results include only instances where the word that follows ‘the’ starts with a vowel.\n\nSee if you can create a search for all words ending in ‘ing’\n\n\nIn this worksheet you have seen that:\n\nLaBB-CAT is a repository for recordings and their transcripts;\nTranscripts are grouped together into corpora;\nMeta-data can be attached to transcripts (transcript attributes) and to participants (participant attributes);\nYou can search the texts of the transcripts;\nYou can filter the search results on the basis of meta-data;\nYou can search for patterns as well as exact spelling, by using regular expressions."
  },
  {
    "objectID": "worksheets/demo/1-exploration.html#easy-exploration",
    "href": "worksheets/demo/1-exploration.html#easy-exploration",
    "title": "1 - Exploration",
    "section": "",
    "text": "Using the ‘easy’ method for exploring LaBB-CAT is simple, as annotation layers are largely ignored; each transcript is essentially treated as a plain text that you can search and display based on ordinary orthographic spelling.\n\nOn the LaBB-CAT home page, click the explore icon to access the easy exploration pages.\nYou will see a similar home page, with Browse icon, Easy Search, and Layered Search icons.\nClick the Browse icon.\nYou will see a page that lists the collections of recordings (or ‘corpora’) in the LaBB-CAT database. Each corpus contains a number of recordings.\nClick the first corpus listed.\nYou will see a page that lists the first 20 recordings in the corpus. The recording names are on the left, followed by some meta data (called ‘participant attributes’) about the participant in the recording. At the bottom is a list of pages, so you can access further recordings in the corpus.\nClick the name of the first recording.\nYou will see a page with transcript text, and the video appears in the top right corner of the page.\nHover the mouse over the video.\nThe video pane grows larger.\nPress the play button.\nAs the video plays, you will see the current utterance highlighted in the transcript. You will also see that the current utterance appears as closed captions in the video. You can use the video controls as normal, including the full-screen button in the bottom right, to make the video occupy the whole screen.\nPause the recording.\nMove the mouse over one of the utterances further down the transcript.\nYou will notice that the video pane shrinks again, and that the mouse pointer becomes a play button.\nClick the utterance.\nYou will see that playback starts at that utterance. Playback will stop when the participant finishes the utterance.\nAt the top of the page, you will see a tab button labelled General; click it.\nYou will see some meta-data about the transcript and recording.\nLaBB-CAT attaches meta-data both to transcripts (called ‘transcript attributes’), and also to participants (‘participant attributes’).\nBelow the transcript attributes is the name of the participant. Click their name.\nYou will see a page with the participant attributes, and a list of the recordings they appear in. In this case, they appear in only one recording; if you were to click the name of the recording, you would be taken back to the transcript page you’ve just seen."
  },
  {
    "objectID": "worksheets/demo/1-exploration.html#basic-search",
    "href": "worksheets/demo/1-exploration.html#basic-search",
    "title": "1 - Exploration",
    "section": "",
    "text": "On the menu at the top of the page, there’s a Search option. Click it.\nYou will see a search form with a “Text” search box at the top, and options for meta data below.\nIn the “Text” box enter the word quake and press the Search button at the bottom.\nYou will see a list of hits, with the name of the transcript on the left and the matched word on the right, highlighted within its immediate context.\nClick the Search again link (or the Search link on the menu at the top)\nYou’ll see that the search form remembers the last search text.\nSelect ‘Female’ from the Gender drop-down box, leave the word quake in the “Text” box, and click Search.\nThis time the results are narrowed down to only female participants.\nClick the first result.\nYou will see the transcript page, as we saw earlier, but with each match from the search highlighted."
  },
  {
    "objectID": "worksheets/demo/1-exploration.html#regular-expressions",
    "href": "worksheets/demo/1-exploration.html#regular-expressions",
    "title": "1 - Exploration",
    "section": "",
    "text": "You can also search across multiple words, and search for patterns as well as exact spellings.\nFor example, let’s say you want to investigate how the pronunciation of the word ‘the’ changes when the following word starts with a vowel. You can search for this pattern using the search form:\n\nClick the Search option on the menu.\nSearch for the word the\nYou will see that there are lots of results, including many where ‘the’ is followed by a word that starts with a consonant.\nGo back to the Search page.\nNow search for: the [aeiou].*\nThis is a ‘regular expression’ that allows you to identify a pattern, with the following parts:\n\nthe word ‘the’\nfollowed by a space\nfollowed by any vowel ([aeiou])\nfollowed anything at all - . in a regular expression means ‘any character’, and * means ‘zero or more of the previous thing’, so .* means ‘zero or more characters’\n\nYou will see that the results include only instances where the word that follows ‘the’ starts with a vowel.\n\nSee if you can create a search for all words ending in ‘ing’\n\n\nIn this worksheet you have seen that:\n\nLaBB-CAT is a repository for recordings and their transcripts;\nTranscripts are grouped together into corpora;\nMeta-data can be attached to transcripts (transcript attributes) and to participants (participant attributes);\nYou can search the texts of the transcripts;\nYou can filter the search results on the basis of meta-data;\nYou can search for patterns as well as exact spelling, by using regular expressions."
  },
  {
    "objectID": "worksheets/course/4-searching.html",
    "href": "worksheets/course/4-searching.html",
    "title": "4. Searching",
    "section": "",
    "text": "4. Searching\nNow that you have some transcripts in your database, we’ll try out LaBB-CAT’s search functions a little.\nSearching broadly involves the following steps:\n\nSelecting participants whose utterances you want to search,\nSpecifying one or more patterns to search for, and\nExploring or extracting the search results.\n\n\nWe’ll start with a very simple search - all the instances of the word “the” uttered by monolingual English-speaking males.\n\nIn LaBB-CAT, click on the participants link on the menu.\nThis takes you to a page listing all participants, where you can filter participants by their attributes. You can see various participant attributes listed across the top of page.\nWe’re interested in male participants, so under the Gender attribute, select M.\nAfter a short delay the page will display a list of all the male participants in the database.\nWe want the participants who speak only English, so enter English under Languages Spoken\nThe page will then display a list of male participants who include English in their languages. It also includes participants who speak other languages, who we want to eliminate.\nThe Languages filter box accepts a ‘regular expression’ so if we enter ^English$ in the box, only those with English as their sole language will be listed. This is because, in regular expressions, ^ means “the beginning” and $ means “the end”, so ^English$ means, “English at the beginning, and at the end”\nClick the Layered Search button at the top of the list.\nYou will see the participants you selected listed at the top, above a list of annotation layers. Below that, there’s a “Search Matrix”, although it doesn’t look much like a matrix yet, because it’s only one layer high and one word wide…\nIn the box under the word “orthography” type the word the\n\nNow press the Search button at the bottom (or hit Enter).\nA progress bar will appear, and then shortly after that, a new tab will open, which has a list of search results in it. Your browser’s popup-blocker might prevent the results page from opening - you can fix that either by allowing the popups in your browser, or by clicking the Display results link that appears after the search finishes.\nEach match is highlighted and shown with some context (the previous word and the following word in the transcript). The amount of context is controlled by a drop-down list at the top.\nSelect 5 words to see more context around each match.\nClick on the first match.\nYou will see that the interactive transcript page opens in a new tab, with the match at the top, and highlighted. You will also see that all the other matches from the same transcript are also highlighted.\nWe’ve already seen what can be done in the interactive transcript page, so close the tab to return to the results page.\nEach result line has a ticked checkbox next to it. Scroll to the bottom of the list.\nYou’ll see that there are buttons at the bottom, which perform operations on the ticked results, including CSV Export, Utterance Export, and Audio Export.\nUn-tick the “Select all results” checkbox, and then tick a handful of results in the list.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can select a group of matches by ticking the first one, and then holding down the Shift key while ticking the last one.\n\n\n\nPress the Audio Export button.\nSave and open the resulting zip file.\nYou’ll see that the files are systematically named to include:\n\nthe name of the transcript\nthe start and end time of the extracted utterance\n\nNow go back to the results page and tick the Prefix Names checkbox.\nPress the Audio Export button again.\nSave and open the resulting zip file.\nThis time you’ll see that the files are also prefixed by the result number.\nYou may notice that there are more audio files this time; that’s because there were multiple results in the same utterance. Previously, only one copy of the utterance was exported, but this time, each match has its own copy of the utterance audio, prefixed by the result number.\nNow go back to the results page and un-tick the Prefix Names checkbox.\nClick the Utterance Export button.\nSave and open the resulting zip file.\nYou’ll see that the TextGrid names match the audio file names in the first zip file.\nOpen one of the TextGrids in Praat.\nYou’ll see that the TextGrid includes a tier named target… which indicates which token(s) in the word… tier matched the search pattern.\nBack on the results page, click the CSV Export button.\nSave the resulting file, and open it.\nYou may have to specify some import options, in which case it may be handy to know that the field separator is comma, and the fields are quoted by speech marks.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you’re using Microsoft Excel and you find it doesn’t open all the columns correctly:\n\nCreate a new workbook in Excel.\nClick the ‘Data’ tab.\nOn the “Get External Data” ribbon click ‘From Text’.\nSelect the CSV file you downloaded.\nSelect ‘Delimited’ and click Next.\nEnsure ‘Comma’ is the only delimiter ticked and click Next.\nClick Finish and then OK.\n\n\n\nYou will see a spreadsheet with one line per selected result, and various columns containing information about the speaker, the corpus, the match line and word, and a URL to the interactive transcript for the match.\nWith this spreadsheet, you can work ‘offline’ with the results, tagging them, computing statistics in Excel, R, or any other program that can work with CSV files. We’ll look at a few more uses for the CSV results files later…\n\nClose the CSV file, and the results page, and go back to the search matrix page.\n\nWe’ve seen that you can search for exact word matches, but you can also search for patterns, using ‘regular expressions’. Now we’re going to search for words beginning with “the…”\n\nChange the orthography search text to the.* (i.e. after the word “the”, append a full-stop and an asterisk.\n\nThe full-stop means “any character at all”, and the asterisk means “zero or more of the previous thing”, so .* means “zero or more characters”.\nClick Search.\nYou will see that now the search results include the word “the” and also words like “then”, “there”, “they”, etc.\nNow go back to the search page, and change the asterisk to a plus-sign, which means “one or more of the previous thing”\n\nClick Search\nYou will see that now the search results exclude the word “the”, only including words where the initial “the...” is followed by at least one character.\nNow change your search by replacing the e in “the” with [aeiou] - so your search pattern will be:\nth[aeiou].+\nThe square-brackets mean “any one of the things inside the brackets”, so [aeiou] means “any vowel”\n\n\n\n\n\n\n\nNote\n\n\n\nWhile you are typing the regular expression, you may notice that the text goes red; this means that what’s currently in the box is not a valid regular expression. That’s fine while you’re still typing, but when you’re ready to search, if the text is red, the search will likely fail. If the regular expression text is red, you can see what the problem with it is by hovering your mouse over the red text; a ‘tip’ will appear showing an error message\n\n\n\nClick Search.\nYou will now see that the results include words like “think”, “that”, “thought”, etc.\n\n\n\n\n\n\n\nTip\n\n\n\n You can get more information about regular expressions by using the online help on the search page, and also by clicking the the regular expressions link above the search matrix.\n\n\n\nUp until now, we’ve only been matching against one word at a time. Now we’re going to include patterns for a chain of words…\n\n\nOn the search page, to the right of the search matrix, there’s a + button. Click it.\n    \nNow you will see that our search matrix is one layer high by two words wide.\nChange the entries on the orthography layer so that it will match the word “the” followed immediately by a word that starts with a vowel, and click Search.\nCheck the search results are giving you what you expected.\nNow search for “the” followed, within two words, by a word that starts with a vowel.\nDream up some other searches that interest you, and try out other options on the search page.\n\n\n\n\n\n\n\nTip\n\n\n\n If in doubt about a search option, try the online help page.\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023-2024 NZILBB"
  },
  {
    "objectID": "worksheets/course/8-forced-alignment.html",
    "href": "worksheets/course/8-forced-alignment.html",
    "title": "8. Forced Alignment",
    "section": "",
    "text": "8. Forced Alignment\nForced alignment is the process of automatically determining the start and end times of words, and the phones within each word.\nThe options for forced alignment are:\n\nHTK - the Hidden Markov Model Toolkit\nMFA - the Montreal Forced Aligner\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023-2024 NZILBB"
  },
  {
    "objectID": "worksheets/course/6-automatic-annotation.html",
    "href": "worksheets/course/6-automatic-annotation.html",
    "title": "6. Automatic Annotation",
    "section": "",
    "text": "You can configure LaBB-CAT to automatically generate annotations, using ‘layer managers’. Basically, layer managers are automatic annotation modules that take data in one annotation layer, do some kind of computation on it, and save the result to another annotation layer.\nIn this exercise, we will use the following layer managers:\n\nFrequency Layer Manager, which counts tokens of each word type, over a configurable scope.\nPorter Stemmer, which applies the Porter algorithm to word orthographies to compute word stems.\nPattern Matcher, which creates annotations based on matching regular expressions against words.\nStatistics Layer Manager, which computes aggregated information, like word count or duration, over groups of words.\n\nLaBB-CAT comes with number of layer managers pre-installed; you can see a list of installed layer managers by clicking the layer managers menu option. Other layer managers have to be manually installed.\n\nFor this exercise, we’ll pretend we’ve got a couple more mini-research projects:\n\nwe’re interested in looking at how rare or common words are in our data, and\nwe want to study ‘filled pauses’ like “um”, “ah”, etc.\n\n\n\nTo start with, we’ll simply annotate each word token in the database with the count of how many times that word appears in the database...\n\nFirst of all, create a new project called frequency, using the steps we saw before.\nSelect the word layers menu option.\nYou will see a list of word layers (including the ‘custom’ layer for the “the” project we created earlier).\nAdd a new layer, with the following settings:\n\nLayer ID: frequency\nType: Number\nAlignment: None\nManager: Frequency Layer Manager\nGenerate: Always\nProject: frequency\nDescription: Count of tokens of the same type within each corpus\nPress the New button\n\nYou will see the layer configuration form. Fill it in with the following details:\n\nSummary: Raw Count\nLayer to summarize: orthography\nScope of Summary: Corpus (leave the box next to that with the [each corpus] option selected)\nMain participants only: ticked\nParticipants: un-ticked\nFilter Layer: un-ticked\nWord pairs: un-ticked\nPause Markers: [leave this blank]\nTranscript types: un-tick wordlist (as counting word list tokens would artificially inflate frequencies of those words)\nAnnotate tokens: ticked\n\n\n\n\n\n\n\n\nNote\n\n\n\n If you want more information about what these options mean, check the online help page.\n\n\n\nPress Save\nYou will see a message asking you if you want generate the layer data now.\nPress Regenerate.\nYou will see a progress bar moving across the page while the counts are being generated. When it is finished, you will see a message saying “Layer complete…”\n\nNow each word in each transcript is annotated with the count of the number of instances of that word with the corpus of the transcript.\nTo see what that looks like…\n\nSelect the transcripts menu option.\nSelect the name of the first transcript in the list.\nAt the top of the transcript there is now a list of projects. Tick the “frequency” project.\nThis will reveal the frequency layer in the list of layers.\nTick the frequency layer.\nWhen the transcript reloads, you will see that above each word is a number. That number is the number of times that word appears in the transcript’s corpus.\ne.g. if the word “and” has 669 above it, and the transcript is in the QB corpus, that means that the word “and” appears in the QB corpus 669 times.\n\nThe newly-generated annotations are also searchable…\n\nSelect the search menu option.\nIf the frequency project and layer are not already ticked, tick them to add the frequency layer to the search matrix.\n\nIn the search matrix, you will notice that, unlike the orthography layer, which has one box for a regular expression, the frequency layer has two boxes, marked “≥” and “&lt;”. For a layer of type Number (which is what you specified above), instead of a regular expression, you can match by numeric range.\nWe want all the words that appeared only once in their corpus. Enter a number or numbers in the appropriate box (you can leave either box blank) and press Search.\nPress ▼ 20 More Matches a couple of times, to get a good idea of the range of results.\n\nThe results you see may contain words that don’t seem rare at all. That they only appear once is a product of two factors:\n\nthere isn’t that much data in our example database, and\n\nthese are counts of ‘wordforms’ - i.e. the surface spelling of the word; e.g. the word “damaging” might be quite rare, even though there are more instances of words from the same stem like “damage”, and “damaged”. This second factor will be addressed soon…\n\nYou can also extract the annotations into CSV results from other searches…\n\nOn the search page, do a search for “the” followed by a word that starts with a vowel.\nWhen the results page appears, click the ▼ button next to the CSV Export button.\nUnder the list of Word layers, tick the frequency layer.\nClick the CSV Export button.\nSave and open the resulting CSV file.\nYou will notice that in the spreadsheet there are two columns:\n\nMatch frequency: this lists the frequency of each word that matched, in order. i.e. in this case two numbers, the frequency of “the”, followed by the frequency of the word after it.\nTarget frequency: this contains a single frequency, in this case the frequency of the first word that matched a pattern - i.e. “the”\n\nAs an aside, you can also select other layers to include in the CSV file. For example, some of the transcripts include topic-tags that were made in the original ELAN transcript.\nExport your search results to CSV again, this time including the topic layer, and see what that looks like.\n\n\nThe Frequency Layer Manager also keeps a word-list with token counts for each corpus…\n\nClick the layer managers menu option.\nOn the “Frequency Layer Manager” row, click the Extensions button.\nYou will see a drop-down box with each corpus in it.\nSelect QB and click Export.\nSave and open the resulting CSV file.\nYou will see an alphabetical list of all the distinct word types in the QB corpus, and next to each, a count of the number of tokens of that type in the QB corpus.\n\n\n\n\nAs pointed out above, although the ‘wordform’ counts might be useful, it also may be useful to lump together different forms of the same stem for the counts. e.g. if there’s 1 “damaging” token, 28 “damage” token, and 18 “damaged” token, it may be useful to count these all together as 47 tokens of the same stem.\nIn order to achieve this, we first need to ‘stem’ all the words in the database - i.e. reduce all the wordforms so that tokens like “damaging”, “damage”, “damaged”, and “damages” all have the same ‘stem’ annotation. Then we can gather frequency statistics on the stems.\nThe Porter Stemmer Layer Manager is one way to achieve this. First, we need to install this layer manager (which only works on English data, so it’s not installed by default).\n\nSelect the layer managers menu option.\nNear the bottom of the page, select the List of layer managers that are not yet installed link.\nFind the “Porter Stemmer” in the list, and press its Install button, and then Install again to continue.\nAfter it is installed, a tab appears with some information about what the layer manager does. You may wish to read this page for your information. Afterwards, you can close the tab to take you back to the LaBB-CAT browser tab.\nSelect the word layers menu option.\nAdd a new layer with the following attributes:\n\nLayer ID: stem\nType: Text\nAlignment: None\nManager: Porter Stemmer\nGenerate: Always\nProject: frequency\nDescription: The stem of the word according to the Porter algorithm\n\nClick the New button.\n\nThe Porter Stemmer’s default configuration is fine for our purposes, so press Set Parameters.\nPress Regenerate.\nYou will see a progress bar, and once it’s finished, you will see a message saying “Layer complete…”\nSelect the transcripts menu option.\nClick the name of the first transcript.\nTick the stem layer we just added.\nWhen the transcript refreshes, you will see, above each word, its ‘stem’ according to the Porter algorithm.\n\nYou will notice that, although the stems are not what you might regard as being the ‘lemma’ of each word (i.e. not necessarily valid words of English in themselves), they nevertheless generally strip off plural and 3rd-person-present suffixes, such that different wordforms of the same lemma will have the same ‘stem’.\nNow that we have generated a layer of ‘stems’ for the wordforms on the orthography layer, we can generate frequency data from the stem layer as well…\n\nClick the word layers menu option.\nAdd a new layer with the following attributes:\n\nLayer ID: stemFrequency\nType: Number\nAlignment: None\nManager: Frequency Layer Manager\nProject: frequency\nDescription: Count of tokens of the same stem within each corpus\n\nPress the New button.\n\nConfigure the layer exactly as before, except this time, set the Layer to summarize: setting to the stem layer we created above. Save your settings and press Regenerate.\nThe layer will be generated.\nDo a search of all speakers, for words with a value of 1 on your new stemFrequency layer.\nYou should notice that the variety of words returned seem a little ‘rarer’ that those returned previously when you were searching the wordform frequency layer.\n\n\n\n\nWe will now create some automatic annotations of a different kind. Let’s suppose that we’re interested in ‘filled pauses’ – words like “um”, “ah”, “er”, “mmm”, etc. You can actually identify them using regular expressions…\n\nDo a search of all speakers, for the word ah. Select the no matches, only a summary of results option.\nNote the number of results you get back.\nNow do a similar search, for the pattern: a+h+ i.e. 1 or more a’s followed by one or more h’s.\nNote the number of result you get back is more than in the previous search. It turns out the transcribers, when transcribing the word “ah” weren’t entirely consistent in their spelling of that word. That’s ok, because with a little imagination, we can invent searches that will identify filled pauses like “um”, “ah”, and “mm”, even if they’ve been spelt “umm”, “ahh”, or “mmm”.\n(It turns out that there’s a good reason to prefer “mmm” over “mm”, but we’ll see that in a later exercise)\nTry out a few different searches to see if you can identify different ways that transcribers have spelt filled pauses like this.\n\nWe could annotate these as filled pauses by searching, annotating a CSV file, and uploading the CSV annotations, as we did previously. However, there is a layer manager that can do this for us, for all the existing data, and for any new transcripts that might be uploaded in the future: the “Pattern Matcher” layer manager.\n\nFirst of all, create a new project called pauses.\nNow create a new word layer, with the following attributes:\n\nLayer ID: pause\nType: Text\nAlignment: None\nManager: Pattern Matcher\nGenerate: Always\nProject: pauses\nDescription: Filled pauses annotated by regular expression\n\nPress the New button\n\nSet the Source Layer to be orthography.\nThe Destination Layer and language-related settings can be left with their default values.\n\nBelow this, there is a currently empty list of “Mappings”. We are going to add regular expressions to this list, which will identify filled pauses.\n\nOn the new empty row that’s already in the list by default, click on the box labelled “Source pattern”, and enter: u+m+\nTo the right of this, click in the “Destination Label” box and enter: um\nThis will make the layer manager find any instances of words that match the pattern “u+m+” on the orthography layer, and in each case, save the annotation”um” on our new pause layer.\nPress the + button to add a new blank row, and add another regular expression:\n\nSource Pattern: a+h+\nDestination Label: ah\n\nPress the + button again, and add another regular expression:\n\nSource Pattern: mm+\nDestination Label: mm\n\nAdd any more regular expressions you think might help identify filled pauses.\nUnder the patterns, select the option to Delete annotations in target layer whose source matches no pattern\n\n\n\n\n\n\n\nTip\n\n\n\nⓘ If you would like more information about the pattern configuration and what kinds of target annotations you can create, you will find that clicking on the brief description of the layer manager above the form expands to provide more detail.\n\n\n\nPress Set Parameters and Regenerate to generate the layer.\nYou will see a progress bar while the layer manager annotates all the filled pauses in the database.\nTo see what this looks like in a transcript, perform a search for um on your new pause layer, and click on the first match.\nYou should see that each instance of the word “um” (or its variants) has been annotated, as have instances of “ah” and “mm”.\n\nNow that these filled pauses are automatically annotated, there are various things you might do with the annotations. You could:\n\ninclude them in the context of multi-word searches, for example you might want to study the effects of a filled pause on the following or preceding word, or\nsearch for only the pauses themselves, for selected speakers, in order to study what kinds of filled pauses are used by which speakers in what contexts, what their durations are, etc.\n\n\n\n\nIn fact, we can use another layer manager to automatically count them for each speaker, and for each utterance in the transcript. In order to do this, we are going to create a ‘phrase layer’, which is a layer that can contain annotations over groups of words (as opposed to against individual words). The layer manager we will use can also annotate participants…\n\nClick the phrase layers option on the menu.\nYou will see a list of phrase layers that are already set up, including language and (named) entity.\nAdd a new layer with the following characteristics:\n\nLayer ID: pauseCount\nType: Number\nAlignment: Intervals\nManager: Statistics Layer Manager\nGenerate: Always\nProject: pauses\nDescription: Count of filled pauses, for the utterance and the speaker\n\nClick New\n\nYou will see a form for the layer’s configuration. Fill in the details as follows:\n\nLayer to summarize: pause\nStatistic: Token Count\nPattern to match: [leave this blank]\nContext: [leave this blank]\nPause Threshold: [leave this blank]\nMain-participant utterances only: ticked\nScopes: tick Utterances, and under Participants:, select the option add new attribute called pauseCount\nTranscript types: leave all the options ticked\n\n\n\n\n\n\n\n\nTip\n\n\n\n If you would like more information about what these settings and the other options do, try the online help for this page.\n\n\n\nSave the layer configuration, and then press Regenerate.\nYou will see a progress bar while the layer manager annotates all the transcripts in the database.\nTo see what this looks like in the transcripts, select the transcripts option on the menu and open the first transcript in the list.\nUnder the list of projects, if the pauses project isn’t already ticked, tick it, which will reveal the pauseCount layer in the list of layers.\nTick the pauseCount layer.\nScrolling down the transcript, you will see that, wherever there is a filled pause like “um”, the entire utterance in which it appears has a bracket across the top of the words, labelled with the number of filled-pauses that occurs in that utterance.\nScroll to the top of the transcript, and click on the name of the main participant.\nYou will see the participant’s attributes page, which now includes the participant’s pauseCount attribute.\nBoth the local utterance count, and the participant’s overall count, can also be exported to CSV search results files.\nSelect search and perform a search involving the pause layer.\nAt the bottom of the results page, click the ▼ button next to the CSV Export button, to reveal the layer options.\nUnder Participant layers tick the pauseCount attribute.\nUnder Phrase layers tick the pauseCount layer.\nPress CSV Export, and save and open the resulting file.\nYou will notice that there is a column called “participant_pauseCount” with the participant’s global count, and another called “Target pauseCount” with the local utterance count.\n\nThe Statistics Layer Manager can also incorporate time information in its computation, so it can be used to compute speech-rate. We could use it on our example database to compute words-per-minute for utterances, turns, speakers, etc.\nIf you like, you can try to figure out how to set up a “words-per-minute” layer now.\nHowever, normally speech-rate is expressed in syllables per minute. We don’t have any way to get syllable-counts for our words yet, but we will be doing that in a later exercise...\nIn this exercise, you’ve seen how layer managers can be used to compute new annotations automatically from existing annotations, e.g.\n\nWords can be tagged with their frequency in the LaBB-CAT database, or its corpora.\nWords can be tagged with their ‘stem’ using the Porter Stemmer.\nWords can be tagged with annotations on the basis of regular expressions.\nGroups of words can be tagged with aggregated information like word count or rate over time."
  },
  {
    "objectID": "worksheets/course/6-automatic-annotation.html#frequency",
    "href": "worksheets/course/6-automatic-annotation.html#frequency",
    "title": "6. Automatic Annotation",
    "section": "",
    "text": "To start with, we’ll simply annotate each word token in the database with the count of how many times that word appears in the database...\n\nFirst of all, create a new project called frequency, using the steps we saw before.\nSelect the word layers menu option.\nYou will see a list of word layers (including the ‘custom’ layer for the “the” project we created earlier).\nAdd a new layer, with the following settings:\n\nLayer ID: frequency\nType: Number\nAlignment: None\nManager: Frequency Layer Manager\nGenerate: Always\nProject: frequency\nDescription: Count of tokens of the same type within each corpus\nPress the New button\n\nYou will see the layer configuration form. Fill it in with the following details:\n\nSummary: Raw Count\nLayer to summarize: orthography\nScope of Summary: Corpus (leave the box next to that with the [each corpus] option selected)\nMain participants only: ticked\nParticipants: un-ticked\nFilter Layer: un-ticked\nWord pairs: un-ticked\nPause Markers: [leave this blank]\nTranscript types: un-tick wordlist (as counting word list tokens would artificially inflate frequencies of those words)\nAnnotate tokens: ticked\n\n\n\n\n\n\n\n\nNote\n\n\n\n If you want more information about what these options mean, check the online help page.\n\n\n\nPress Save\nYou will see a message asking you if you want generate the layer data now.\nPress Regenerate.\nYou will see a progress bar moving across the page while the counts are being generated. When it is finished, you will see a message saying “Layer complete…”\n\nNow each word in each transcript is annotated with the count of the number of instances of that word with the corpus of the transcript.\nTo see what that looks like…\n\nSelect the transcripts menu option.\nSelect the name of the first transcript in the list.\nAt the top of the transcript there is now a list of projects. Tick the “frequency” project.\nThis will reveal the frequency layer in the list of layers.\nTick the frequency layer.\nWhen the transcript reloads, you will see that above each word is a number. That number is the number of times that word appears in the transcript’s corpus.\ne.g. if the word “and” has 669 above it, and the transcript is in the QB corpus, that means that the word “and” appears in the QB corpus 669 times.\n\nThe newly-generated annotations are also searchable…\n\nSelect the search menu option.\nIf the frequency project and layer are not already ticked, tick them to add the frequency layer to the search matrix.\n\nIn the search matrix, you will notice that, unlike the orthography layer, which has one box for a regular expression, the frequency layer has two boxes, marked “≥” and “&lt;”. For a layer of type Number (which is what you specified above), instead of a regular expression, you can match by numeric range.\nWe want all the words that appeared only once in their corpus. Enter a number or numbers in the appropriate box (you can leave either box blank) and press Search.\nPress ▼ 20 More Matches a couple of times, to get a good idea of the range of results.\n\nThe results you see may contain words that don’t seem rare at all. That they only appear once is a product of two factors:\n\nthere isn’t that much data in our example database, and\n\nthese are counts of ‘wordforms’ - i.e. the surface spelling of the word; e.g. the word “damaging” might be quite rare, even though there are more instances of words from the same stem like “damage”, and “damaged”. This second factor will be addressed soon…\n\nYou can also extract the annotations into CSV results from other searches…\n\nOn the search page, do a search for “the” followed by a word that starts with a vowel.\nWhen the results page appears, click the ▼ button next to the CSV Export button.\nUnder the list of Word layers, tick the frequency layer.\nClick the CSV Export button.\nSave and open the resulting CSV file.\nYou will notice that in the spreadsheet there are two columns:\n\nMatch frequency: this lists the frequency of each word that matched, in order. i.e. in this case two numbers, the frequency of “the”, followed by the frequency of the word after it.\nTarget frequency: this contains a single frequency, in this case the frequency of the first word that matched a pattern - i.e. “the”\n\nAs an aside, you can also select other layers to include in the CSV file. For example, some of the transcripts include topic-tags that were made in the original ELAN transcript.\nExport your search results to CSV again, this time including the topic layer, and see what that looks like.\n\n\nThe Frequency Layer Manager also keeps a word-list with token counts for each corpus…\n\nClick the layer managers menu option.\nOn the “Frequency Layer Manager” row, click the Extensions button.\nYou will see a drop-down box with each corpus in it.\nSelect QB and click Export.\nSave and open the resulting CSV file.\nYou will see an alphabetical list of all the distinct word types in the QB corpus, and next to each, a count of the number of tokens of that type in the QB corpus."
  },
  {
    "objectID": "worksheets/course/6-automatic-annotation.html#porter-stemmer",
    "href": "worksheets/course/6-automatic-annotation.html#porter-stemmer",
    "title": "6. Automatic Annotation",
    "section": "",
    "text": "As pointed out above, although the ‘wordform’ counts might be useful, it also may be useful to lump together different forms of the same stem for the counts. e.g. if there’s 1 “damaging” token, 28 “damage” token, and 18 “damaged” token, it may be useful to count these all together as 47 tokens of the same stem.\nIn order to achieve this, we first need to ‘stem’ all the words in the database - i.e. reduce all the wordforms so that tokens like “damaging”, “damage”, “damaged”, and “damages” all have the same ‘stem’ annotation. Then we can gather frequency statistics on the stems.\nThe Porter Stemmer Layer Manager is one way to achieve this. First, we need to install this layer manager (which only works on English data, so it’s not installed by default).\n\nSelect the layer managers menu option.\nNear the bottom of the page, select the List of layer managers that are not yet installed link.\nFind the “Porter Stemmer” in the list, and press its Install button, and then Install again to continue.\nAfter it is installed, a tab appears with some information about what the layer manager does. You may wish to read this page for your information. Afterwards, you can close the tab to take you back to the LaBB-CAT browser tab.\nSelect the word layers menu option.\nAdd a new layer with the following attributes:\n\nLayer ID: stem\nType: Text\nAlignment: None\nManager: Porter Stemmer\nGenerate: Always\nProject: frequency\nDescription: The stem of the word according to the Porter algorithm\n\nClick the New button.\n\nThe Porter Stemmer’s default configuration is fine for our purposes, so press Set Parameters.\nPress Regenerate.\nYou will see a progress bar, and once it’s finished, you will see a message saying “Layer complete…”\nSelect the transcripts menu option.\nClick the name of the first transcript.\nTick the stem layer we just added.\nWhen the transcript refreshes, you will see, above each word, its ‘stem’ according to the Porter algorithm.\n\nYou will notice that, although the stems are not what you might regard as being the ‘lemma’ of each word (i.e. not necessarily valid words of English in themselves), they nevertheless generally strip off plural and 3rd-person-present suffixes, such that different wordforms of the same lemma will have the same ‘stem’.\nNow that we have generated a layer of ‘stems’ for the wordforms on the orthography layer, we can generate frequency data from the stem layer as well…\n\nClick the word layers menu option.\nAdd a new layer with the following attributes:\n\nLayer ID: stemFrequency\nType: Number\nAlignment: None\nManager: Frequency Layer Manager\nProject: frequency\nDescription: Count of tokens of the same stem within each corpus\n\nPress the New button.\n\nConfigure the layer exactly as before, except this time, set the Layer to summarize: setting to the stem layer we created above. Save your settings and press Regenerate.\nThe layer will be generated.\nDo a search of all speakers, for words with a value of 1 on your new stemFrequency layer.\nYou should notice that the variety of words returned seem a little ‘rarer’ that those returned previously when you were searching the wordform frequency layer."
  },
  {
    "objectID": "worksheets/course/6-automatic-annotation.html#pattern-matcher",
    "href": "worksheets/course/6-automatic-annotation.html#pattern-matcher",
    "title": "6. Automatic Annotation",
    "section": "",
    "text": "We will now create some automatic annotations of a different kind. Let’s suppose that we’re interested in ‘filled pauses’ – words like “um”, “ah”, “er”, “mmm”, etc. You can actually identify them using regular expressions…\n\nDo a search of all speakers, for the word ah. Select the no matches, only a summary of results option.\nNote the number of results you get back.\nNow do a similar search, for the pattern: a+h+ i.e. 1 or more a’s followed by one or more h’s.\nNote the number of result you get back is more than in the previous search. It turns out the transcribers, when transcribing the word “ah” weren’t entirely consistent in their spelling of that word. That’s ok, because with a little imagination, we can invent searches that will identify filled pauses like “um”, “ah”, and “mm”, even if they’ve been spelt “umm”, “ahh”, or “mmm”.\n(It turns out that there’s a good reason to prefer “mmm” over “mm”, but we’ll see that in a later exercise)\nTry out a few different searches to see if you can identify different ways that transcribers have spelt filled pauses like this.\n\nWe could annotate these as filled pauses by searching, annotating a CSV file, and uploading the CSV annotations, as we did previously. However, there is a layer manager that can do this for us, for all the existing data, and for any new transcripts that might be uploaded in the future: the “Pattern Matcher” layer manager.\n\nFirst of all, create a new project called pauses.\nNow create a new word layer, with the following attributes:\n\nLayer ID: pause\nType: Text\nAlignment: None\nManager: Pattern Matcher\nGenerate: Always\nProject: pauses\nDescription: Filled pauses annotated by regular expression\n\nPress the New button\n\nSet the Source Layer to be orthography.\nThe Destination Layer and language-related settings can be left with their default values.\n\nBelow this, there is a currently empty list of “Mappings”. We are going to add regular expressions to this list, which will identify filled pauses.\n\nOn the new empty row that’s already in the list by default, click on the box labelled “Source pattern”, and enter: u+m+\nTo the right of this, click in the “Destination Label” box and enter: um\nThis will make the layer manager find any instances of words that match the pattern “u+m+” on the orthography layer, and in each case, save the annotation”um” on our new pause layer.\nPress the + button to add a new blank row, and add another regular expression:\n\nSource Pattern: a+h+\nDestination Label: ah\n\nPress the + button again, and add another regular expression:\n\nSource Pattern: mm+\nDestination Label: mm\n\nAdd any more regular expressions you think might help identify filled pauses.\nUnder the patterns, select the option to Delete annotations in target layer whose source matches no pattern\n\n\n\n\n\n\n\nTip\n\n\n\nⓘ If you would like more information about the pattern configuration and what kinds of target annotations you can create, you will find that clicking on the brief description of the layer manager above the form expands to provide more detail.\n\n\n\nPress Set Parameters and Regenerate to generate the layer.\nYou will see a progress bar while the layer manager annotates all the filled pauses in the database.\nTo see what this looks like in a transcript, perform a search for um on your new pause layer, and click on the first match.\nYou should see that each instance of the word “um” (or its variants) has been annotated, as have instances of “ah” and “mm”.\n\nNow that these filled pauses are automatically annotated, there are various things you might do with the annotations. You could:\n\ninclude them in the context of multi-word searches, for example you might want to study the effects of a filled pause on the following or preceding word, or\nsearch for only the pauses themselves, for selected speakers, in order to study what kinds of filled pauses are used by which speakers in what contexts, what their durations are, etc."
  },
  {
    "objectID": "worksheets/course/6-automatic-annotation.html#statistics-layer-manager",
    "href": "worksheets/course/6-automatic-annotation.html#statistics-layer-manager",
    "title": "6. Automatic Annotation",
    "section": "",
    "text": "In fact, we can use another layer manager to automatically count them for each speaker, and for each utterance in the transcript. In order to do this, we are going to create a ‘phrase layer’, which is a layer that can contain annotations over groups of words (as opposed to against individual words). The layer manager we will use can also annotate participants…\n\nClick the phrase layers option on the menu.\nYou will see a list of phrase layers that are already set up, including language and (named) entity.\nAdd a new layer with the following characteristics:\n\nLayer ID: pauseCount\nType: Number\nAlignment: Intervals\nManager: Statistics Layer Manager\nGenerate: Always\nProject: pauses\nDescription: Count of filled pauses, for the utterance and the speaker\n\nClick New\n\nYou will see a form for the layer’s configuration. Fill in the details as follows:\n\nLayer to summarize: pause\nStatistic: Token Count\nPattern to match: [leave this blank]\nContext: [leave this blank]\nPause Threshold: [leave this blank]\nMain-participant utterances only: ticked\nScopes: tick Utterances, and under Participants:, select the option add new attribute called pauseCount\nTranscript types: leave all the options ticked\n\n\n\n\n\n\n\n\nTip\n\n\n\n If you would like more information about what these settings and the other options do, try the online help for this page.\n\n\n\nSave the layer configuration, and then press Regenerate.\nYou will see a progress bar while the layer manager annotates all the transcripts in the database.\nTo see what this looks like in the transcripts, select the transcripts option on the menu and open the first transcript in the list.\nUnder the list of projects, if the pauses project isn’t already ticked, tick it, which will reveal the pauseCount layer in the list of layers.\nTick the pauseCount layer.\nScrolling down the transcript, you will see that, wherever there is a filled pause like “um”, the entire utterance in which it appears has a bracket across the top of the words, labelled with the number of filled-pauses that occurs in that utterance.\nScroll to the top of the transcript, and click on the name of the main participant.\nYou will see the participant’s attributes page, which now includes the participant’s pauseCount attribute.\nBoth the local utterance count, and the participant’s overall count, can also be exported to CSV search results files.\nSelect search and perform a search involving the pause layer.\nAt the bottom of the results page, click the ▼ button next to the CSV Export button, to reveal the layer options.\nUnder Participant layers tick the pauseCount attribute.\nUnder Phrase layers tick the pauseCount layer.\nPress CSV Export, and save and open the resulting file.\nYou will notice that there is a column called “participant_pauseCount” with the participant’s global count, and another called “Target pauseCount” with the local utterance count.\n\nThe Statistics Layer Manager can also incorporate time information in its computation, so it can be used to compute speech-rate. We could use it on our example database to compute words-per-minute for utterances, turns, speakers, etc.\nIf you like, you can try to figure out how to set up a “words-per-minute” layer now.\nHowever, normally speech-rate is expressed in syllables per minute. We don’t have any way to get syllable-counts for our words yet, but we will be doing that in a later exercise...\nIn this exercise, you’ve seen how layer managers can be used to compute new annotations automatically from existing annotations, e.g.\n\nWords can be tagged with their frequency in the LaBB-CAT database, or its corpora.\nWords can be tagged with their ‘stem’ using the Porter Stemmer.\nWords can be tagged with annotations on the basis of regular expressions.\nGroups of words can be tagged with aggregated information like word count or rate over time."
  },
  {
    "objectID": "worksheets/course/3-uploading-data.html",
    "href": "worksheets/course/3-uploading-data.html",
    "title": "3. Uploading Data",
    "section": "",
    "text": "3. Uploading Data\nIn this exercise you will:\n\nUpload a transcript manually\nUpload many transcripts at once using the batch uploader\nImport participant data from a CSV file\nDefine a speech elicitation task for gathering data\n\nAfter this you will have a small corpus in your LaBB-CAT database.\nBefore you start, download and unzip QuakeStories.zip so you've got the demonstration data for uploading to your corpus.\n\n\nManual Upload\n\nIn LaBB-CAT, click the upload option in the menu.\nClick the first option, upload transcripts.\nClick the left-hand Choose File button and select the file in the “QuakeStories” folder called:\n“BR178LK_MargaretSpencer.eaf”\nWhen you select a file, a new row of Choose File buttons will appear below the first. This is for adding more transcripts in the ‘episode’. An ‘episode’ is a set of transcripts that belong together because they were recorded during the same session. In our case, each recording session has only one recording.\nNext to Media on the first row, click Choose File\nEach transcript has an audio file and a video file, and you want to upload both.\nClick the file called “BR178LK_MargaretSpencer.mp4”, then hold down the Shift key on your keyboard and click the file called “BR178LK_MargaretSpencer.wav”. Then click Open.\nEnsure the Corpus option is QB\nEnsure the Type option is interview\nClick Upload\n\nEach ELAN transcript has a number of Tiers defined in it:\n\none for the participant's utterances,\nanother for an ‘interviewer’ if there is one,\none for noise annotations,\none for transcriber comments, and\none for topic annotations.\n\nEach tier must be mapped to a LaBB-CAT annotation layer.\n\nLaBB-CAT has analysed the structure of the ELAN transcript and pre-selected some default options for layer mappings. For the demo data, these defaults are correct, so you needn’t change anything.\nClick Next to continue.\nThis will display a page listing all the speakers in the transcript, so you can select which one is the ‘main participant’, which is the speaker selected by default for searches and other processing.\nEnsure that BR178LK_MargaretSpencer is ticked, and the interviewer is not ticked, and click Set Main Participants.\nThis will display a page with the name of transcript you uploaded, with an edit meta-data link, and a progress bar (which may have already finished).\nClick edit meta data\nThis will display the attributes for the transcript.\nCheck that you remembered to set Type to interview. If not, you can fix that on this page, and press the Save button that appears when you make changes.\nBelow the transcript attributes is a Participants link – click it.\nThis will list both participants in the recording, the main participant, and the interviewer.\nClick BR178LK_MargaretSpencer.\nThis will display the participant attributes we defined in an earlier exercise.\nBR178LK_MargaretSpencer is an ‘English’-speaking ‘female’ who is between ‘66 and 75 years’ old, who grew up in ‘Christchurch’, in the ‘North Canterbury’ region of ’New Zealand'.\nSet her attributes to reflect that, and click Save.\nBelow the participant attributes, there is a Transcripts link – click it.\nYou will see a list of transcripts that the speaker appears in (in this case, only one).\nEach has various icons on the right; hover your mouse over each icon, and a ‘tip’ will appear that describes what the link does.\nClick name of the BR178LK_MargaretSpencer.eaf transcript.\n\nYou will now see LaBB-CAT's ‘interactive transcript’ page for the transcript.\nAt the top there is a heading, a list of speakers, and then below this, the lines from the transcript, their speakers in the margin. This includes the words the participants utter, and also any noises, comments, and other events that were put in the transcript in ELAN.\n\nIn the top right corner are some playback controls; click the play button. You will see a shaded rectangle following the participant's speech.\nTry the other controls to see what they do.\nNow click on any word in the transcript.\nYou will see a menu appear, with options for the ‘Utterance’ (the line), and the word.\nClick the play option in the menu to see what it does.\nClick on the formats link under the title.\nYou will see a menu, which includes various formats for exporting the transcript.\nSelect ‘Plain Text Document’\nSave the resulting file on your desktop, and then open it.\nYou will see the transcript in plain-text form.\nClick the formats link, and select the ‘Praat Text Grid’ option.\nSave the resulting file on your desktop, and then open it with Praat.\nYou will see that the TextGrid has various tiers, two for full utterances (one for each speaker), and two for individual words (one for each speaker).\n(You will see that each individual word has a ‘default’ alignment - i.e. the words are evenly spread out during the duration of the line they’re in. In a later exercise we will look at ways to make these word alignments actually line up with the words in the audio signal.)\n\n\n\nBatch Upload\nIf you already have a collection of transcripts and media files (which we have for these exercises), and they are systematically organized (which they are), you may be able to save some manual uploading work by uploading them using the ‘batch upload’ utility.\n\nIn LaBB-CAT, click the upload option on the menu.\nClick the upload transcript batch link.\nThis shows a window with a large blank area in the middle with various buttons above it.\nOpen Windows Explorer or Finder, and navigate to the LaBB-CAT Workshop data folder.\nDrag the folder called “QuakeStories”, and drop it on to LaBB-CAT, on to the blank area below the buttons.\nThe previously blank area will contain a list of transcripts. Each transcript should have a value filled in for each column - Transcript, Media, Corpus, and Episode.\nMost of the transcripts are monologues, so set Type to monologue on the top left.\nOne of the transcript is highlighted in orange, and the Status says “Already exists” - this is the transcript you manually uploaded. We don’t need to upload it again, so remove it from the list by using the x button on the right hand side of that row.\nClick the Upload button above the list.\nYou will see that in the Status column, the text changes to “Uploading…” for the first transcript. The progress bar progresses, and once it's complete, the next transcript changes to “Transferring”, and so on.\n\n\n\n\n\n\n\nTip\n\n\n\nWhile the files are uploading, click  the online help link next to the upload transcript batch link you clicked above and read the conditions that must be met in order to use the batch uploader.\n\n\n\nOnce the uploader is finished, you will receive a CSV report file that lists the files you uploaded and their upload status. (If there had been any problems with the upload, the resulting error messages would be included in this report for following up.)\nYou can verify that all the transcripts are there by clicking the transcripts option on the menu in LaBB-CAT.\nYou should see a list of twenty transcripts.\nUse the Transcript box to find UC013AM_Dom.eaf (You can type just part of the name if you like)\nClick the Attributes icon for UC013AM_Dom.eaf (the one with the spanner/wrench 🔧 on it).\nChange Transcript type to interview and click Save.\nSimilarly, the following transcripts are interviews, so change their type accordingly\n\nUC215YW_DanielaMaoate-Cox.eaf\nUC226AD.eaf\n\n\n\n\nParticipant Data Import\nThe transcripts are now in the database, but the meta-data for the participants hasn't been set yet (because it’s not contained in the ELAN files). We could manually add this for each speaker, but fortunately we have it stored in a spreadsheet (actually, a CSV text file) that we can upload in one go.\n\nIn LaBB-CAT, select the upload option on the menu.\nSelect the upload participant data option.\nClick Choose File, and select the file in the LaBB-CAT Exercises data folder called participants.csv\nClick Upload\nYou will now see a list of the columns from the spreadsheet.\nFirstly, ensure that the Participant identity column is set to name. This ensures that the “name” column in the spreadsheet will be used to match names of participants in the LaBB-CAT database.\nBelow that is listed each column from the spreadsheet, with an arrow pointing to a dropdown box. The box contains various options, including each of the participant attributes set up in LaBB-CAT, an ignore option, and create a new attribute option.\nMost likely, the correct options are already selected, as we’ve already set up the correct participant attributes, but just check that they are as follows:\n\n\nThe CSV column name: → ignore because it's the Participant Identity Column identified above\nThe CSV column gender: → the Gender LaBB-CAT attribute\nThe CSV column age_category: → the Age LaBB-CAT attribute\nThe CSV column ethnicity: → the Ethnicity LaBB-CAT attribute\nThe CSV column grew_up: → the grew_up LaBB-CAT attribute\nThe CSV column grew_up_region: → the grew_up_region LaBB-CAT attribute\nThe CSV column grew_up_town: → the grew_up_town LaBB-CAT attribute\nThe CSV column languages_spoken: → the languages_spoken LaBB-CAT attribute\n\nClick import.\nYou should see a page with information about the import, including the columns that were ignored, and the number of participants that were added.\n\nTo check the participant attributes really are now set:\n\nClick the participants option on the menu. You will see a list of speakers, and page links at the bottom.\n\nThe page also includes participant attribute values where they are known.\nYou can also filter the list by these values, using the column headings above the list:\n\nUnder Gender, select the F option.\nThe page now lists only those with ‘Female’ set for the Gender attribute.\n\n\n\nElicitation Tasks\nLaBB-CAT can also make recordings of speech directly from the browser.\nLet’s suppose you want to record a number of participants reading lists of words. You can define an ‘Elicitation Task’ that includes a series of steps, one for each set of words you want participants to read.\nFirst we’re going to create a corpus to receive our recordings, and a transcript type to mark the recordings as word lists …\n\nIn LaBB-CAT, select the corpora option on the menu.\nAdd a corpus called CC with a description Canterbury Corpus.\nClick the transcript types option on the menu.\nAdd a transcript type called wordlist.\n\nNow we’ll create the elicitation task, which defines what prompts and texts the participant sees during the task.\n\nClick the elicitation tasks option on the menu. The page you see is a list of elicitation tasks defined, which is currently empty.\nFill in the blank form with the following details:\n\nID: nze-wordlist\ndescription: New Zealand English Word List\ncorpus: CC (the corpus you just created)\ntranscript type: wordlist (the transcript type you just created)\npreamble: “In this task your speech will be recorded. Please ensure you’re in a quiet place.”\nThis is the first text the participant sees when they access the task, before giving consent or going through the steps.\nconsent: “I give consent for the use of my speech data for this research.”\nThis is the text of the participant's consent for their participation and the use of their data. Before starting the task steps, they must 'sign' this consent by typing their name in a box at the bottom. The text, with their name and the date incorporated, with be made into a PDF file which is uploaded with their recordings, and is made available for them to download.\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor both the preamble and the consent form, you can format the text with bold, italic, and underlined text, etc. by using the controls above the text area.\n\n\n\n\n\n\n\n\nTip\n\n\n\n Check the online help on this page for further details about settings and important information about browser limitations.\n\n\n\nPress New to add the task.\nPress Define Steps.\n\nOn this page you are going to add steps for the task. The first step, called “Welcome”, has already been added, and we’ll use it for giving the participant some detailed instructions about what follows. We'll add a series of steps after the “Welcome” step, one for each group of words we want the participant to read.\n\nThe form you can see defines the details of the first “Welcome” step.\n Check the online help on this page for further details about this page and the options on it.\nClose the online help page to return to the “define elicitation steps” page.\nFill in the following details:\n\nShow: Always\nCountdown Seconds: 0\nTitle: Instructions\nPrompt: Please read aloud the following sets of words. Press “Next” after each set.\nElicit: Nothing\nTranscript: (leave this box blank)\nImage/Video: no image/video\n\n\n\nNext we’ll define what demographic information we will ask each participant before they start recording. In this case, we will ask for their gender and what languages they speak.\n\nClick the  button to add a new step.\nFill in the following details:\n\nShow: Always\nCountdown Seconds: 0\nTitle: Languages\nPrompt: What languages do you speak?\nElicit: Attribute Value\nAttribute: participant_languages_spoken\nImage/Video: no image/video\n\nClick the  button to add a new step\nFill in the following details:\n\nShow: Always\nCountdown Seconds: 0\nTitle: Gender\nPrompt: What is your gender?\nElicit: Attribute Value\nAttribute: participant_gender\nImage/Video: no image/video\n\n\nNow we can defined some prompts for them to read aloud.\n\nClick the  button to add a new step\nFill in the following details:\n\nShow: Always\nCountdown Seconds: 0\nTitle: (leave this box blank)\nPrompt: Please read the following aloud:\nElicit: Audio\nTranscript: 1. hit hid hint\nMax Seconds: 30\nNext Button: Shown\nImage/Video: no image/video\n\nClick the  button to add a new step\nFill in the same details as the previous step, except:\nTranscript: 2. boot booed boo tune dune\nAdd a new step for Transcript: 3. bird curt burn\nAdd a new step for Transcript: 4. bat bad back bag ban\nAdd a new step for Transcript: 5. bet bed beck beg ben\nAdd one last step, with the following details:\n\nShow: Always\nCountdown Seconds: 0\nTitle: Finished\nPrompt: Thanks for your participation!\nElicit: Nothing\nTranscript: (leave this box blank)\nImage/Video: no image/video\n\n\nThis last step is what is displayed to the participant when they’ve finished all the steps.\n\nClick Save Changes\n\n\n\nYour task is almost ready. We just need to define which options for gender they can see.\n\nSelect the elicitation tasks option on the menu.\nPress Participant Attributes.\nSelect Options\nThis displays a list of the gender options that are visible to the participant. As you can see it's currently empty. In this case, we want to display all options for them to select.\nPress Add All\nYou will see that all the options (M, F, and ‘(not specified)’) have been added to the list. If you wanted to, you could edit the “description” of the individual items (e.g. translate them to another language if your participants don’t speech English), or delete options you don’t want them to be able to select.\nPress the Delete button next to the ‘(not specified)’ option, and click OK to confirm.\nPress Save Changes.\n\nYour task is now fully defined and ready to go.\nNow you’re going to run through the elicitation task yourself …\n\nSelect the elicitation tasks option on the menu.\nPress the Elicitation Task button on the bottom right.\nYou should see a page that displays the task’s ‘preamble’ that you defined earlier.\n\n\nClick Next.\nYou should see a page that displays the task’s consent form that you defined earlier, with a box to enter your name in order to ‘sign’ the consent.\nEnter your name and click Next.\nYou will be given the chance to save your copy of the consent form.\nSave the consent form and open it to check the contents.\nClose the consent form to return to the task.\nYou should see a page with some text about enabling your microphone.\nIf you don’t, and instead see a message about your browser not being supported, this means that your web browser doesn’t support recording sound. In this case, copy the address of the page at the top, and paste it into another browser (e.g. Google Chrome or Mozilla Firefox).\nClick Next and follow the instructions.\n\nOnce you've enabled your browser for access to your microphone, you will be asked for the demographic details you defined earlier.\nAfter you enter these, the task steps will begin, and you should follow the instructions, reading the prompts aloud and clicking Next after each group of words.\nEach time somebody performs the task, they're assigned a unique Participant ID, which is linked to their demographic data and the recordings.\n\nPress the Back button on your browser to return to the define elicitation tasks page in LaBB-CAT.\nPress the participants option on the menu.\nUnder the Corpus heading, select the CC option.\nYou will see one participant; the one you just created by doing the task.\nPress the participant ID to open their attributes page, and check that the demographic information you entered has been saved.\nPress the participant ID to open their attributes page, and check You will see that the participant has five transcripts, one for each of the task steps where audio was recorded.\nPress the Transcripts link at the bottom to lilst the transcripts.\nOpen the first transcript.\nYou will see that the transcript starts with a comment, which is the prompt text you were shown during the step, and that the transcript contains one utterance.\nPlay the audio to ensure it was recorded correctly.\n(If the last transcript you looked at had video, you may need to tick the checkbox next to the “wav” option in the top right corner, in order to select audio for playback.)\n\nAlthough these ‘task step’ transcripts are very short, they behave the same as any other transcript; they can be exported, annotated, searched, etc.\n\nYou now have a small database with a number of speakers in it, so we can start creating some annotations and doing some searches ... \n \n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023-2024 NZILBB"
  },
  {
    "objectID": "worksheets/course/7b-lexicon-cmudict.html",
    "href": "worksheets/course/7b-lexicon-cmudict.html",
    "title": "7b. CMU Pronouncing Dictionary",
    "section": "",
    "text": "7b. CMU Pronouncing Dictionary\nLaBB-CAT can be integrated with the CMU Pronouncing Dictionary, which is a free pronunciation dictionary of English maintained by the Speech Group in the School of Computer Science at Carnegie Mellon University. The pronunciations are based on ‘American English’, so are suitable for ‘American English’ recordings.\nIt can also serve as a free alternative to the CELEX lexicon (which is based on ‘British English’), for those that have not purchased CELEX, although is less ideal for ‘non-rhotic’ varieties of English.\nIn this exercise you will:\n\nInstall the CMU Pronouncing Dictionary layer manager\nUse it to create new annotations for word pronunciations\nIncorporate the new layers in more sophisticated searches\n\n\nThe first thing we’re going to do is install the CMU Dict layer manager…\n\nSelect the layer managers menu option.\nFollow the List of layer managers that are not yet installed link near the bottom.\nFind “CMU Pronouncing Dictionary” in the list, and press its Install button, Install again, and then the Configure button. You will see a progress bar while the layer manager loads the data from the dictionary file into the LaBB-CAT database. This will take a minute or so.\nOnce it’s finished, you will see a page with information about the CMU Pronouncing Dictionary layer manager.\n\nNow that we’ve installed the layer manager, we’ll create a layer that contains word pronunciations.\n\nAdd a word layer managed by the CMU Pronouncing Dictionary for word pronunciation - i.e.:\n\nLayer ID: phonemes\nType: Phonological\nAlignment: None\nManager: CMU Pronouncing Dictionary\nDescription: CMU Pronouncing Dictionary pronunciations\n...configured with the Encoding: field set to CELEX DISC, and the default values for everything else.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you’re curious about what the configuration options do, hover your mouse over each option to see a ‘tool tip’ that describes what the option is for.\n\n\n\nOnce the layer has finished generating, select the transcripts menu option, and find and open NB926_IsobelleDoig.eaf.\nTick your new phonemes layer.\nYou will see that each word is tagged with a phonemic transcription.\n\nYou will notice that the annotations are displayed using IPA symbols. However, the layer manager doesn’t use IPA symbols directly, it actually uses the ‘DISC’ encoding for phonemes, which uses ordinary ‘typewriter’ characters (ASCII), and uses exactly one character per phoneme.\nThe IPA symbols are being displayed by LaBB-CAT to provide a linguist-friendly representation of the phonemic transcription. But you can see the underlying DISC characters by selecting the ASCII option on the layer in the transcript.\n\nSelect ASCII on the phonemes layer, to see what the layer manager is actually producing.\n\nYou may find that this is somewhat harder to read. Diphthongs are generally represented by digits, and various other characters are used to represent affricates, etc.\nIt’s nice to display the IPA symbols, but it’s important to understand the DISC symbols (shown in the table below), because they are what we have to use when searching on the phonemes layer, which we are going to try now.\nAs you may have seen on the layer configuration page, there is another possible representation of the pronunciations, called ‘ARPABET’; this is what is used in the original dictionary file published by CMU, and uses up to three uppercase characters per phoneme. While we’re not using ARPABET in this exercise, you can use it if you like, and the ARPABET symbols are included in the table. In the table, you will see that there are gaps where no ARPABET version of the phoneme is shown; this means that the CMU Pronouncing Dictionary contains no entries that include that phoneme.\n\n\n\n\nIPA\nDISC\nARPABET\n \n \nIPA\nDISC\nARPABET\n \n\n\np\np\nP\npat\n \nɪ\nI\nIH\nKIT\n\n\nb\nb\nB\nbad\n \nε\nE\nEH\nDRESS\n\n\nt\nt\nT\ntack\n \næ\n{\nAE\nTRAP\n\n\nd\nd\nD\ndad\n \nʌ\nV\nAH\nSTRUT\n\n\nk\nk\nK\ncad\n \nɒ\nQ\nAH\nLOT\n\n\ng\ng\nG\ngame\n \nʊ\nU\nUH\nFOOT\n\n\nŋ\nN\nNG\nbang\n \nə\n@\n[vowel ending in 0]\nanother\n\n\nm\nm\nM\nmat\n \ni:\ni\nIY\nFLEECE\n\n\nn\nn\nN\nnat\n \nα: \n#\nAA\nfather\n\n\nl\nl\nL\nlad\n \nɔ:\n$\nAO\nTHOUGHT\n\n\nr\nr\nR\nrat\n \nu:\nu\nUW\nGOOSE\n\n\nf\nf\nF\nfat\n \nɜ:\n3\nER\nNURSE\n\n\nv\nv\nV\nvat\n \neɪ\n1\nEY\nFACE\n\n\nθ\nT\nTH\nthin\n \nαɪ\n2\nAY\nPRICE\n\n\nð\nD\nDH\nthen\n \nɔɪ\n4\nOY\nCHOICE\n\n\ns\ns\nS\nsap\n \nəʊ\n5\nOW\nGOAT\n\n\nz\nz\nZ\nzap\n \nαʊ\n6\nAW\nMOUTH\n\n\n∫\nS\nSH\nsheep\n \nɪə\n7\n \nNEAR\n\n\nʒ\nZ\nZH\nmeasure\n \nεə\n8\n \nSQUARE\n\n\nj\nj\nY\nyank\n \nʊə\n9\n \nCURE\n\n\nx\nx\n \nloch\n \næ\nc\n \ntimbre\n\n\nh\nh\nHH\nhad\n \nɑ̃ː\nq\n \ndétente\n\n\nw\nw\nW\nwet\n \næ̃ː\n0\n \nlingerie\n\n\nʧ\nJ\nCH\ncheap\n \nɒ̃ː\n~\n \nbouillon\n\n\nʤ\n_\nJH\njeep\n \n \n \n \n \n\n\nŋ̩\nC\n \nbacon\n \n \n \n \n \n\n\nm̩\nF\n \nidealism\n \n \n \n \n \n\n\nn̩\nH\n \nburden\n \n \n \n \n \n\n\nl̩\nP\n \n dangle\n \n \n \n \n \n\n\n\nIn the transcript, you may notice there are gaps in the layer - i.e. words that are not tagged with a pronunciation.\nFor example, around the middle of the transcript, the word “compactums” is not tagged, because the CMU Pronouncing Dictionary has no entry for that word.\nThere are various possible solutions for this, but one is to tag word tokens with their pronunciations directly in the transcript. This has been done in the case of “compactums”; manual pronunciation tags are saved on the pronounce layer\n\nScroll to the top of the transcript, un-tick the phonemes layer and tick the pronounce layer.\nWhen the transcript re-loads to show the pronounce layer tags, find “compactums” again.\n\nYou will see it has been tagged with an annotation labelled “kəmpæktəmz”, which was manually added by the transcriber of the transcript, in the original ELAN file.\nWe want all pronunciations to be present on the phonemes layer, which is currently managed by the CMU Pronouncing Dictionary layer manager. LaBB-CAT allows layers to have more than one layer manager, however; a layer can have a main layer manager, and a number of ‘auxiliary’ managers that perform extra annotation tasks.\nWe are going to add an auxiliary layer manager to the phonemes layer, which will copy any pronounce annotations it finds to the phonemes layer. This will fill in the gaps in the CMU Pronouncing dictionary, at least for the tokens that have manual pronounce tags.\n\nSelect the word layers option on the menu.\nOn the phonemes layer row, there are a number of buttons on the right, including one with a  icon. Hover your mouse over this button to see what it does, and then click it.\nYou will see a page explaining that will copy any manually tagged pronunciations from the from the pronounce layer into the phonemes.\nClick yes to continue.\nYou will see a progress bar while the auxiliary layer manager copies the pronounce annotations to the phonemes layer.\nWhen it’s finished, select the transcripts menu option, and open NB926_IsobelleDoig.eaf again.\nTick the phonemes layer.\nFind the word “compactums” in the transcript.\n\nYou will see it now has a phonemes tag, just like the rest of the word tokens.\n\nSelect the search option from the menu.\nSearch your new phonemes layer for words that start with h\n\nYou will see that the results contain words that you might not expect, like “where”, “which” and “when”.\n\nClick one of these unexpected results, to open the transcript.   \nYou will see that, in the transcript, the pronunciation appears to start with /w/, not with /h/.\nClick on the word and select the Edit option on the menu that appears.\nNow look for the phonemes layer. You will see that, in addition to the pronunciation that starts with /w/, there’s another annotation that starts with /h/, which is invisible on the transcript.\n\n\nThese are all the possible phonemic transcriptions for the word. Only the first one is displayed in the transcript, but when you do searches, all of them are searched. This can result in unexpected matches like this, but it can be useful, as it ensures that when you search for a particular phonemic pattern, all possible tokens are returned, not just those that match on the most ‘normal’ transcription.\nNow we’re going to try to do a search for the word “the” followed by a word that starts with schwa.\n\nSelect the search option from the menu.\nCreate a search matrix that’s two words wide, and includes the orthography and phonemes layers.\nType the in the first orthography box.\nClick the second box on the phonemes layer, but don’t enter anything in the box yet.\nThe box has a little « button to the right of it.\nHover the mouse over it to see what it says, and then click it.\nYou will see that a section opens with a bunch of phoneme symbols on it.\nFind the schwa symbol ə and click it.\nYou will see that an @ symbol appears in the box.\n@ is the DISC symbol for /ə/, so in order to search for schwa, we have to use it in our search pattern.\nWe want words that start with schwa, so type .* after the @ symbol.\nClick Search.\n\nYou will see that some of the words being matched are words that you might not normally think start with a schwa. LaBB-CAT is matching words against all their possible phonemic transcriptions, so if the CMU dictionary has multiple possible pronunciations for a word, and one of them starts with schwa, it will be matched.\nYou can check this by clicking on a match, and then clicking on the word in the transcript and selecting Edit, which displays all the annotations for the given token.\nIf you check the table above, you will see that ə has no specific representation in ARPABET. This means that no CMU Pronouncing Dictionary pronunciations include schwa explicitly. Instead, ‘unstressed’ versions of other vowels are used. For example, the word “transcription” is transcribed T R AE2 N S K R IH1 P SH AH0 N in the original dictionary file; the final vowel AH is the ‘STRUT’ vowel, and the 0 means it’s ‘unstressed’. The layer manager translates this to DISC as tr{nskrIpS@n.\nNow that we have phonemic transcripts, we can do abetter job of the search we tried in the first exercise - “the” followed by a word starting with a vowel…\n\nChange your search so that, instead of just @ at the beginning of the word, it matches any vowel.\n\nYou could use the square-brackets [] at the start of your pattern, and type all vowel symbols inside them - Note that the vowels in the DISC representation extend beyond a, e, i, o, and u - you should add in all the vowels you see in the list that appears when you expand the IPA helper, including all the diphthongs.\nAlternatively, you can simply click the VOWEL link in the ‘IPA helper’, which will add all the DISC vowels for you, already enclosed in square-brackets.\n\nRun the search and check that it’s giving you what you expect. Notice that now there are no ‘false positives’ like “the one” that we were getting when searching by orthography alone.\n\nNow that you’ve generated a few different layers, and have seen how the search matrix works, you might want to try out some of the following searches, or invent some others:\n\nWords which have the DRESS vowel as the second phoneme\nThe word “the” followed by a word beginning with the phoneme /k/\nWords ending with a front vowel, followed by words beginning with /p/ or /b/\nWords that begin with “k” in their spelling, but begin with the phoneme /n/\nWords that begin with “k” in their spelling, but do not begin with the phoneme /n/\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023-2024 NZILBB"
  },
  {
    "objectID": "worksheets/course/1-installation.html",
    "href": "worksheets/course/1-installation.html",
    "title": "LaBB-CAT Documentation",
    "section": "",
    "text": "In this exercise you will install the LaBB-CAT software.\n\n\n\n\n\n\nNote\n\n\n\nYou should only follow these steps if you will be running LaBB-CAT on your own computer.\nIf you are using a LaBB-CAT server that’s already been installed for you elsewhere, you can skip with exercise.\n\n\nAfter this you will have an empty LaBB-CAT database set up ready to set up.\n\nYou have a file called install-labbcat.jar - double click this file to start the installer.\nIf you are using OS X, you may see a message that the file can’t be opened:\n\nIf this happens:\n\nClick the Apple icon in the top left corner of the screen.\nSelect System Preferences\nClick Security & Privacy\nNear the bottom it says “install-labbcat.jar’’ was blocked from opening because it is not from an identified developer.\n\nClick Open Anyway\nYou may see another warning about the program being downloaded from the internet\n\nClick Open\n\n\n\nClick Start\nYou will see the progress bar move as files are installed. Once this is finished, you’ll see a message saying Installation complete.\nClick Finished to close the installer.\n\nThe software is now installed. LaBB-CAT is a browser-based system, which means that it works as a mini web server on your computer, and you need to access it using your web browser.\nEach time you want to use LaBB-CAT, you must start it up, and which you’ve finished, you close it down again.\nTo start LaBB-CAT, click the LaBB-CAT icon in your applications area.\n\nOn Windows, open the Start menu and type LaBB-CAT.\nOn OS X you will find LaBB-CAT in your Applications folder.\n\nA window called “LaBB-CAT Server” will open, and after a short delay, your default web browser will open on a page called “LaBB-CAT” (The first time only, this page will initially display the LaBB-CAT licence).\n\nNow that the software is installed, we will set up a basic structure for receiving data, in the following exercise."
  },
  {
    "objectID": "worksheets/course/1-installation.html#installation",
    "href": "worksheets/course/1-installation.html#installation",
    "title": "LaBB-CAT Documentation",
    "section": "",
    "text": "In this exercise you will install the LaBB-CAT software.\n\n\n\n\n\n\nNote\n\n\n\nYou should only follow these steps if you will be running LaBB-CAT on your own computer.\nIf you are using a LaBB-CAT server that’s already been installed for you elsewhere, you can skip with exercise.\n\n\nAfter this you will have an empty LaBB-CAT database set up ready to set up.\n\nYou have a file called install-labbcat.jar - double click this file to start the installer.\nIf you are using OS X, you may see a message that the file can’t be opened:\n\nIf this happens:\n\nClick the Apple icon in the top left corner of the screen.\nSelect System Preferences\nClick Security & Privacy\nNear the bottom it says “install-labbcat.jar’’ was blocked from opening because it is not from an identified developer.\n\nClick Open Anyway\nYou may see another warning about the program being downloaded from the internet\n\nClick Open\n\n\n\nClick Start\nYou will see the progress bar move as files are installed. Once this is finished, you’ll see a message saying Installation complete.\nClick Finished to close the installer.\n\nThe software is now installed. LaBB-CAT is a browser-based system, which means that it works as a mini web server on your computer, and you need to access it using your web browser.\nEach time you want to use LaBB-CAT, you must start it up, and which you’ve finished, you close it down again.\nTo start LaBB-CAT, click the LaBB-CAT icon in your applications area.\n\nOn Windows, open the Start menu and type LaBB-CAT.\nOn OS X you will find LaBB-CAT in your Applications folder.\n\nA window called “LaBB-CAT Server” will open, and after a short delay, your default web browser will open on a page called “LaBB-CAT” (The first time only, this page will initially display the LaBB-CAT licence).\n\nNow that the software is installed, we will set up a basic structure for receiving data, in the following exercise."
  },
  {
    "objectID": "worksheets/course/9-aligned-data.html",
    "href": "worksheets/course/9-aligned-data.html",
    "title": "9. Aligned Data",
    "section": "",
    "text": "In a previous exercise, we force-aligned words and their segments, and also did a little hand-correction of the alignments. Now that we’ve got some relatively reliable phone-level alignments, we’re going to explore their search and annotation possibilities.\nIn this exercise you will\n\nsee how the alignments affect speech rate computations,\nautomatically annotate pauses between words,\nsearch for tokens of individual, time-aligned segments,\ncreate time-aligned segment annotations via Praat, and\nautomatically extract features from search-result intervals using Praat\n\n\n\n\nWhen inspecting alignments on the segment layer, you will have noticed that the forced aligner introduces pauses between the words – almost always at the beginnings and ends of utterances, and also sometimes in between. These pauses will affect the speech-rate statistics that are computed by the Statistics layer manager for the speech rate layer that we set up earlier. Now that there are pauses, these are excluded (i.e. not counted as speech) for the speech-rate statistics.\nHowever, for this change to be affected, the speech rate layer needs to be regenerated.\n\nGo to the transcripts page and open the transcript we force-aligned earlier, UC207YW.eaf, if you don’t still have it open.\nTick the speechRate layer for display, and make a note of some of the speech rate annotations (which ones are there depends on what you selected as the scopes for the layer).\nAt the top of the transcript, in the formats menu, select the regenerate layers option.\nThis will display a list of managed layers.\nSelect the speechRate layer and press Regenerate.\nYou will see a progress bar while the statistics for the transcript are computed again.\nOnce that’s finished, go back to the transcript and check the speech rate annotations again. You should see that at least some of them are different.\n\nThese pauses can also be directly annotated, in case you’re interesting in finding them for analysis.\n\nAdd a new word layer with the following attributes:\n\nLayer ID: previousPause\nType: Number\nAlignment: None\nManager: Context Layer Manager\nGenerate: Always\nProject: pauses\nDescription: Length in seconds of the preceding pause\n\nConfigure the layer with the following settings:\n\nSource layer: word\nSelect the Pause Detection option\nMinimum Pause Length: unticked\nMaximum Pause Length: unticked\nOnly pauses within the same turn: unticked\nLeave the Annotate the word following the pause option selected.\n\n\n\n\n\n\n\n\nTip\n\n\n\n You may be interested in looking at the online help to find out what kinds of annotations the Context layermanager can create.\n\n\n\nPress Save and then Regenerate.\nOnce the layer is generated, go back to the UC207YW.eaf transcript and display the previousPause layer (you might have to tick the pauses project to make the layer option visible).\n(You also might want to un-tick some of the other layers, to avoid clutter.)\nYou should see that a subset of words are annotated with a number, which is the length of the pause before that word.\nOpen one or two such utterances in Praat to check that the lengths are accurate.\n\nYou may notice that pauses in the middle of utterances are always right, but the pause before the first word in the utterance seems wrong. See if you can figure out why.\n\n\n\nNow we’re going to search for some instances of vowels of interest (the FLEECE vowel in this case), and annotate them with formant measurements.\n\nFirst of all, create a new project called formants\nNow select the segment layers menu option.\n\nWe’re going to add a layer that annotates segments - i.e. individual phones within words.\n\nFill in the new-layer form at the bottom with the following details:\n\nLayer ID: F1\nType: Number\nAlignment: Instants\nManager: (don’t select any of the options, this is a manual annotation layer)\nGenerate: (not relevant as it’s not a managed layer)\nProject: formants\nDescription: First Formant\nPress New.\n\nSelect search on the menu.\nTick the segment layer.\n\nThe segments layer contains annotations at the sub-word level - i.e. there are potentially multiple annotations per word, each annotation representing a phone of the word. You will see that, as with other layers, there is a box on the segments layer for a regular expression.\nAs with other patterns in the search matrix, the pattern that you enter in the box is matched against individual annotations. So if you enter i in the in the box, it will match each FLEECE vowel segment in each word in the database.\n\n\n\n\n\n\nImportant\n\n\n\nIt’s important to realise that if you enter a pattern that would match more than a single phoneme symbol on this layer then no search results will be returned, because each annotation on this layer is only a single character long (remember the DISC encoding uses one character per phoneme).\nFor example, if you enter .*IN for your search, intending to match all words ending in “…ing”, then no results will be returned, because no single segment will ever match that pattern.\n\n\n\nWe want to search for all instances of the FLEECE vowel, so use the label selector - the button labelled « - to find the right symbol for the FLEECE vowel and click it.\nIf you used HTK for forced alignment, this will be i\nIf you used MFA for forced alignment, this will be iː\n\nIn our particular database, if there’s any annotation on the segments layer, then we can be sure that it’s been aligned at least by HTK or MFA (if not manually corrected). However, there are configurations of CELEX layers that would put unaligned annotations on that layer.\n\nTo make sure we only get words that have been aligned by HTK/MFA or manually, tick the only match words that are aligned option.\nPress Search.\nOnce the search is finished, you should notice that the only transcripts returned are those that include speakers you’ve done force-alignment on.\nClick on the first result, to open its transcript.\nScroll to the top and tick the include empty layers option.\nTick the formants project option.\nThis will reveal the (empty) F1 layer below segments in the list of layers.\nTick the F1 layer.\nThe transcript will reload to include that layer (even though it’s currently empty).\nAlso tick the segments layer if it’s currently un-ticked.\nClick on the first search result (which is highlighted in green).\nSelect the Open TextGrid in Praat option.\nThis will open the audio of the line, with a TextGrid that includes the aligned words and segments.\nThere’s also a tier for the F1 layer.\n\n\n\n\n\n\n\nTip\n\n\n\nIf there’s no F1 tier in the TextGrid, it’s because the alignment setting for the layer is set to Not aligned instead of Instants.\nYou can fix that using the segments layers option in the menu.\n\n\n\nIn Praat, find our instance of the FLEECE vowel, and click on a good point in the spectrogram for measuring it’s F1 value.\nGet the F1 value (hit the F1 key on your keyboard)\nCopy the value that is displayed on to the clipboard (i.e. select it and hit Ctrl + C on your keyboard)\nBack in the TextGrid, add a boundary on the F1 tier at that point (if it’s the fourth tier, hit Ctrl + F4 on your keyboard)\nPaste the F1 value that you copied earlier (i.e. hit Ctrl + V on your keyboard)\nNow you’ve annotated the vowel with its F1 value, and we want to save that annotation back to the LaBB-CAT database.\nClick back on the LaBB-CAT transcript window.\nClick the Import Changes button that has appeared to the left ofthe line with the first match.\nYou should see a message indicating that the annotations has been saved.\nRepeat the above steps for the next few matches in the transcript.\nOnce you’ve added at least a handful of annotations on the F1 layer, refresh the interactive transcript page (i.e. use the reload button in your browser, or the F5 key).\nYou will see that for each match you’ve annotated, the F1 value you entered appears below the corresponding word. The transcript doesn’t display the time-alignment information, but that is also stored in the database.\nOpen the TextGrid for one of the lines you’ve annotated.\nYou should see the annotation that you made is in the TextGrid, at the corresponding point in time.\n\nThese manual annotations are also searchable (so for example you could search for all the FLEECE vowels with an F1 measure within a particular range), and can be exported in CSV search results. To see that in action:\n\nRepeat the segment search we did before (i.e. all aligned FLEECE vowels).\nOnce you see the results page, click the ▼ button next to the CSV Export button link, and tick the F1 layer.\nPress CVS Export.\nSave and open the resulting file.\nYou will see that there are two columns, one called “Target F1”, which contains the annotations you have made, and the other called “Target F1 start”, which contains the time of the annotation, in seconds from the beginning of the recording.\n\n\n\n\nWe will now see that you can use Praat to automatically extract certain measurements, given start and end times from search results. In order for this to work, we first need to ensure that the LaBB-CAT server knows where Praat is installed:\n\nIn LaBB-CAT, select the system attributes menu option.\nThis shows a form with various options on it, one of which is Praat Path\nIf the Praat Path option is blank, enter the location of Praat on your LaBB-CAT server, and click Save. The setting should be the path to the folder that contains praat, e.g.\n\non Windows, this might be C:\\Program Files\\Praat\non OS X, this might be /Applications\n\n\nLet’s say we want to extract F1 and F2 from all our aligned FLEECE vowels.\nWe are going to use the CSV file you just extracted.\nIf you don’t have it any more, repeat the search and export the results to CSV.\nThe CSV file includes a column called “Target segment”, which contains the annotation that matched the pattern (in this case they will all be “i”), and columns called “Target segment start” and “Target segment end” - these are the start and end times of each matching FLEECE vowel.\nWe are going to use these start/end times to get Praat to take formant measurements for us.\n\nIn LaBB-CAT, click the upload menu option.\nClick the process with praat option.\n\n\n\n\n\n\n\nTip\n\n\n\nIf the option process with praat is not there, it means that the Praat Path attribute is not set. Go back to the steps above and ensure that Praat Path is set before continuing…\n\n\n\nClick Choose File and select the CSV results that you saved above.\nYou will see a form to fill in, and the first couple of settings (Transcript Name column and Participant column should be already filled in.\nFor the Start Time column ensure that the Target segment start option is selected.\nFor the End Time column ensure the Target segment end option is selected.\nThese two settings define the start/end times of the phone.\n\nFor some measurements you might extract from Praat, processing signal that includes surrounding context is usually a good idea. You’ll see there’s a setting for that (which you can leave at the default of 0.025s), and you will see options for various measurements.\nThe default options are for F1 and F2 only, but if you feel like getting other measurements, feel free to tick those options too. You can expand the advanced settings section by clicking the triangular bullet next to “Formants” and other measurements, which allow you to specify more detail about how Praat should do its computations. Again, feel free to look at those and try different settings.\n\nClick Process.\nYou will see a progress bar while LaBB-CAT generates a Praat scripts and runs them with Praat.\nOnce Praat has finished processing the intervals, you will get a CSV file (you might have to click the CSV file with measurements link) - save and open it.\nYou will see that it’s a copy of the CSV file you uploaded, with some extra columns added on the right.\n\nDepending on your settings, this will include at least one column per measurement you selected (the formant columns also include on that contains the time at which the measurements were taken), and a final column called “Error” which is hopefully blank, but which might contain errors reported back by Praat (e.g. if it couldn’t find the audio file or ran into any other problem during processing).\n\n\nDuring this exercise, you have seen that the inter-word pauses created by forced alignment introduce the possibility of more accurate speech-rate statistics, and can themselves be automatically annotated, in case they are of interest for search or analysis.\nYou’ve also seen that you can create manual time-aligned annotation layers, which can be used to annotate phones (they can also be used for words or spans of words, by creating word layers or phrase/span layers), and that you can also use intervals from CSV search results to extract acoustic measurements automatically using Praat.\nObviously these measurements are as reliable as the intervals themselves, so care needs to be taken to maximise the likelihood of good HTK alignments, and to check and possibly manually-correct those alignments."
  },
  {
    "objectID": "worksheets/course/9-aligned-data.html#pauses-and-speech-rate",
    "href": "worksheets/course/9-aligned-data.html#pauses-and-speech-rate",
    "title": "9. Aligned Data",
    "section": "",
    "text": "When inspecting alignments on the segment layer, you will have noticed that the forced aligner introduces pauses between the words – almost always at the beginnings and ends of utterances, and also sometimes in between. These pauses will affect the speech-rate statistics that are computed by the Statistics layer manager for the speech rate layer that we set up earlier. Now that there are pauses, these are excluded (i.e. not counted as speech) for the speech-rate statistics.\nHowever, for this change to be affected, the speech rate layer needs to be regenerated.\n\nGo to the transcripts page and open the transcript we force-aligned earlier, UC207YW.eaf, if you don’t still have it open.\nTick the speechRate layer for display, and make a note of some of the speech rate annotations (which ones are there depends on what you selected as the scopes for the layer).\nAt the top of the transcript, in the formats menu, select the regenerate layers option.\nThis will display a list of managed layers.\nSelect the speechRate layer and press Regenerate.\nYou will see a progress bar while the statistics for the transcript are computed again.\nOnce that’s finished, go back to the transcript and check the speech rate annotations again. You should see that at least some of them are different.\n\nThese pauses can also be directly annotated, in case you’re interesting in finding them for analysis.\n\nAdd a new word layer with the following attributes:\n\nLayer ID: previousPause\nType: Number\nAlignment: None\nManager: Context Layer Manager\nGenerate: Always\nProject: pauses\nDescription: Length in seconds of the preceding pause\n\nConfigure the layer with the following settings:\n\nSource layer: word\nSelect the Pause Detection option\nMinimum Pause Length: unticked\nMaximum Pause Length: unticked\nOnly pauses within the same turn: unticked\nLeave the Annotate the word following the pause option selected.\n\n\n\n\n\n\n\n\nTip\n\n\n\n You may be interested in looking at the online help to find out what kinds of annotations the Context layermanager can create.\n\n\n\nPress Save and then Regenerate.\nOnce the layer is generated, go back to the UC207YW.eaf transcript and display the previousPause layer (you might have to tick the pauses project to make the layer option visible).\n(You also might want to un-tick some of the other layers, to avoid clutter.)\nYou should see that a subset of words are annotated with a number, which is the length of the pause before that word.\nOpen one or two such utterances in Praat to check that the lengths are accurate.\n\nYou may notice that pauses in the middle of utterances are always right, but the pause before the first word in the utterance seems wrong. See if you can figure out why."
  },
  {
    "objectID": "worksheets/course/9-aligned-data.html#time-aligned-segment-annotations",
    "href": "worksheets/course/9-aligned-data.html#time-aligned-segment-annotations",
    "title": "9. Aligned Data",
    "section": "",
    "text": "Now we’re going to search for some instances of vowels of interest (the FLEECE vowel in this case), and annotate them with formant measurements.\n\nFirst of all, create a new project called formants\nNow select the segment layers menu option.\n\nWe’re going to add a layer that annotates segments - i.e. individual phones within words.\n\nFill in the new-layer form at the bottom with the following details:\n\nLayer ID: F1\nType: Number\nAlignment: Instants\nManager: (don’t select any of the options, this is a manual annotation layer)\nGenerate: (not relevant as it’s not a managed layer)\nProject: formants\nDescription: First Formant\nPress New.\n\nSelect search on the menu.\nTick the segment layer.\n\nThe segments layer contains annotations at the sub-word level - i.e. there are potentially multiple annotations per word, each annotation representing a phone of the word. You will see that, as with other layers, there is a box on the segments layer for a regular expression.\nAs with other patterns in the search matrix, the pattern that you enter in the box is matched against individual annotations. So if you enter i in the in the box, it will match each FLEECE vowel segment in each word in the database.\n\n\n\n\n\n\nImportant\n\n\n\nIt’s important to realise that if you enter a pattern that would match more than a single phoneme symbol on this layer then no search results will be returned, because each annotation on this layer is only a single character long (remember the DISC encoding uses one character per phoneme).\nFor example, if you enter .*IN for your search, intending to match all words ending in “…ing”, then no results will be returned, because no single segment will ever match that pattern.\n\n\n\nWe want to search for all instances of the FLEECE vowel, so use the label selector - the button labelled « - to find the right symbol for the FLEECE vowel and click it.\nIf you used HTK for forced alignment, this will be i\nIf you used MFA for forced alignment, this will be iː\n\nIn our particular database, if there’s any annotation on the segments layer, then we can be sure that it’s been aligned at least by HTK or MFA (if not manually corrected). However, there are configurations of CELEX layers that would put unaligned annotations on that layer.\n\nTo make sure we only get words that have been aligned by HTK/MFA or manually, tick the only match words that are aligned option.\nPress Search.\nOnce the search is finished, you should notice that the only transcripts returned are those that include speakers you’ve done force-alignment on.\nClick on the first result, to open its transcript.\nScroll to the top and tick the include empty layers option.\nTick the formants project option.\nThis will reveal the (empty) F1 layer below segments in the list of layers.\nTick the F1 layer.\nThe transcript will reload to include that layer (even though it’s currently empty).\nAlso tick the segments layer if it’s currently un-ticked.\nClick on the first search result (which is highlighted in green).\nSelect the Open TextGrid in Praat option.\nThis will open the audio of the line, with a TextGrid that includes the aligned words and segments.\nThere’s also a tier for the F1 layer.\n\n\n\n\n\n\n\nTip\n\n\n\nIf there’s no F1 tier in the TextGrid, it’s because the alignment setting for the layer is set to Not aligned instead of Instants.\nYou can fix that using the segments layers option in the menu.\n\n\n\nIn Praat, find our instance of the FLEECE vowel, and click on a good point in the spectrogram for measuring it’s F1 value.\nGet the F1 value (hit the F1 key on your keyboard)\nCopy the value that is displayed on to the clipboard (i.e. select it and hit Ctrl + C on your keyboard)\nBack in the TextGrid, add a boundary on the F1 tier at that point (if it’s the fourth tier, hit Ctrl + F4 on your keyboard)\nPaste the F1 value that you copied earlier (i.e. hit Ctrl + V on your keyboard)\nNow you’ve annotated the vowel with its F1 value, and we want to save that annotation back to the LaBB-CAT database.\nClick back on the LaBB-CAT transcript window.\nClick the Import Changes button that has appeared to the left ofthe line with the first match.\nYou should see a message indicating that the annotations has been saved.\nRepeat the above steps for the next few matches in the transcript.\nOnce you’ve added at least a handful of annotations on the F1 layer, refresh the interactive transcript page (i.e. use the reload button in your browser, or the F5 key).\nYou will see that for each match you’ve annotated, the F1 value you entered appears below the corresponding word. The transcript doesn’t display the time-alignment information, but that is also stored in the database.\nOpen the TextGrid for one of the lines you’ve annotated.\nYou should see the annotation that you made is in the TextGrid, at the corresponding point in time.\n\nThese manual annotations are also searchable (so for example you could search for all the FLEECE vowels with an F1 measure within a particular range), and can be exported in CSV search results. To see that in action:\n\nRepeat the segment search we did before (i.e. all aligned FLEECE vowels).\nOnce you see the results page, click the ▼ button next to the CSV Export button link, and tick the F1 layer.\nPress CVS Export.\nSave and open the resulting file.\nYou will see that there are two columns, one called “Target F1”, which contains the annotations you have made, and the other called “Target F1 start”, which contains the time of the annotation, in seconds from the beginning of the recording."
  },
  {
    "objectID": "worksheets/course/9-aligned-data.html#process-with-praat",
    "href": "worksheets/course/9-aligned-data.html#process-with-praat",
    "title": "9. Aligned Data",
    "section": "",
    "text": "We will now see that you can use Praat to automatically extract certain measurements, given start and end times from search results. In order for this to work, we first need to ensure that the LaBB-CAT server knows where Praat is installed:\n\nIn LaBB-CAT, select the system attributes menu option.\nThis shows a form with various options on it, one of which is Praat Path\nIf the Praat Path option is blank, enter the location of Praat on your LaBB-CAT server, and click Save. The setting should be the path to the folder that contains praat, e.g.\n\non Windows, this might be C:\\Program Files\\Praat\non OS X, this might be /Applications\n\n\nLet’s say we want to extract F1 and F2 from all our aligned FLEECE vowels.\nWe are going to use the CSV file you just extracted.\nIf you don’t have it any more, repeat the search and export the results to CSV.\nThe CSV file includes a column called “Target segment”, which contains the annotation that matched the pattern (in this case they will all be “i”), and columns called “Target segment start” and “Target segment end” - these are the start and end times of each matching FLEECE vowel.\nWe are going to use these start/end times to get Praat to take formant measurements for us.\n\nIn LaBB-CAT, click the upload menu option.\nClick the process with praat option.\n\n\n\n\n\n\n\nTip\n\n\n\nIf the option process with praat is not there, it means that the Praat Path attribute is not set. Go back to the steps above and ensure that Praat Path is set before continuing…\n\n\n\nClick Choose File and select the CSV results that you saved above.\nYou will see a form to fill in, and the first couple of settings (Transcript Name column and Participant column should be already filled in.\nFor the Start Time column ensure that the Target segment start option is selected.\nFor the End Time column ensure the Target segment end option is selected.\nThese two settings define the start/end times of the phone.\n\nFor some measurements you might extract from Praat, processing signal that includes surrounding context is usually a good idea. You’ll see there’s a setting for that (which you can leave at the default of 0.025s), and you will see options for various measurements.\nThe default options are for F1 and F2 only, but if you feel like getting other measurements, feel free to tick those options too. You can expand the advanced settings section by clicking the triangular bullet next to “Formants” and other measurements, which allow you to specify more detail about how Praat should do its computations. Again, feel free to look at those and try different settings.\n\nClick Process.\nYou will see a progress bar while LaBB-CAT generates a Praat scripts and runs them with Praat.\nOnce Praat has finished processing the intervals, you will get a CSV file (you might have to click the CSV file with measurements link) - save and open it.\nYou will see that it’s a copy of the CSV file you uploaded, with some extra columns added on the right.\n\nDepending on your settings, this will include at least one column per measurement you selected (the formant columns also include on that contains the time at which the measurements were taken), and a final column called “Error” which is hopefully blank, but which might contain errors reported back by Praat (e.g. if it couldn’t find the audio file or ran into any other problem during processing).\n\n\nDuring this exercise, you have seen that the inter-word pauses created by forced alignment introduce the possibility of more accurate speech-rate statistics, and can themselves be automatically annotated, in case they are of interest for search or analysis.\nYou’ve also seen that you can create manual time-aligned annotation layers, which can be used to annotate phones (they can also be used for words or spans of words, by creating word layers or phrase/span layers), and that you can also use intervals from CSV search results to extract acoustic measurements automatically using Praat.\nObviously these measurements are as reliable as the intervals themselves, so care needs to be taken to maximise the likelihood of good HTK alignments, and to check and possibly manually-correct those alignments."
  },
  {
    "objectID": "worksheets/course/7a-lexicon-celex.html",
    "href": "worksheets/course/7a-lexicon-celex.html",
    "title": "7a. CELEX",
    "section": "",
    "text": "LaBB-CAT can be integrated with the CELEX lexicon, which can be purchased from the Linguistic Data Consortium (LDC) and includes lemma, part of speech, morphological, phonological, and frequency information for English, German, and Dutch.\n(If you don’t have CELEX, there is an alternative version of this exercise that uses the free ‘CMU Pronunouncing Dictionary’ lexicon, which you can work through instead.)\nIn this exercise you will:\n\nInstall the CELEX layer manager\nUse it to create new annotations for word morphology, syntactic category, and phonology\nCompute speech rate in syllables per minute\nIncorporate the new layers in more sophisticated searches\n\n\n\nThe first thing we’re going to do is install the CELEX layer manager.\nThis requires having the LDC’s CELEX data files on your computer, which will be processed by the layer manager in order to insert the data into the LaBB-CAT database. If you received the CELEX files in a ZIP file, you need to unzip that into a folder, and remember the location of that folder, as you’ll need it during the installation process.\n\nFirst of all, create a new project called CELEX with a description: CELEX Annotations\nClick the layer managers menu option.\nClick the List of layer managers that are not yet installed link near the bottom.\nFind “CELEX English” in the list, and click its Install button.\nYou will see a form asking for various details. You can leave most of these with their default values. The one exception is the CELEX  ENGLISH data folder option.\nSet the CELEX ENGLISH data folder parameter to the directory path that leads to the CELEX files on your LaBB-CAT computer.\nClick Install.\nYou will see a progress bar while the layer manager loads the data from the CELEX files into the LaBB-CAT database. This will take a few minutes.\nOnce it’s finished, you will see a new window open with information about the CELEX English layer manager.\nReading this information page, you will see some instructions on how to create CELEX annotation layers - leave this tab open for now, as we’re going to need those instructions next.\n\n\n\n\nNow that we’ve installed the layer manager, we’ll create our first annotations from CELEX - a layer with morphological annotations.\n\nFollow the instructions on the information page to create a layer for word morphology - i.e.:\n\nLayer ID: morphology\nType: Text\nAlignment: None\nManager: CELEX English\nProject: CELEX\nDescript Morphological parses\n...configured with the Morphology option selected, and the default values for everything else.\n\n\n\n\n\n\n\n\nTip\n\n\n\n If you’re curious about what the configuration options do, and how you can test out the results of your configuration, check the online help page when you are configuring the layer.\n\n\n\nOnce the layer has finished generating, select the transcripts menu option, and open the first transcript in the list.\nTick the CELEX project, and then tick your new morphology layer.\nYou will see that each word is tagged with morphological information.\n\nIf you were to do a search for words ending in “ing” on the orthography layer, you would get both gerunds like “coming” and also words like “thing” and “anything” whose “ing” is part of the base word, not a morphological affix. You can now tell these apart in searches, by searching the morphology layer for words that end in “+ing”.\n\nDo a search on the orthography layer of words ending in “ing”. Leave the results tab open, so you can compare these results with the next search …\nNow do a search on the morphology layer of words ending in “+ing”, and compare the results.\n\n\n\n\n\n\n\nImportant\n\n\n\nRemember that in regular expressions the ‘plus’ character + has a special meaning - it means “one or more of the previous thing”.\nIn order to search for a literal “+” in the annotation, you have to ‘escape’ the +. Consult the Regular Expression help page to figure out how to do that.\n\n\n\n\n\nNow we will create a layer for syntactic categories from CELEX.\n\nCreate a new layer for annotating words with their syntactic categories from CELEX:\n\nLayer ID: syntacticCategory\nType: Text\nAlignment: None\nManager: CELEX English\nProject: CELEX\nDescription: All possible syntactic categories\n...configured with the Syntax option ticked, and the default values for everything else.\n\nOnce the layer has finished generating, go to the search page and do a search for the word “fine” on the orthography layer.\nOpen the transcript of the first match.\nTick the syntactic category layer to display the annotations that have just been computed.\nNow find your instance of the word “fine” again (it’s highlighted in the transcript text).\nYou will see that it has “A” for adjective above it.\nClick on the word “fine” and select the Edit option on the menu that appears.\nNow look for the syntacticCategory layer. You will see that, in addition to “A”, there are several other annotations that are invisible on the transcript.\nThese are all the possible syntactic categories for the word “fine” ordered most-frequent first. Only the first one is displayed in the transcript, but when you do searches, all of them are searched.\nOn the search page, do a search for fine on the orthography layer and A on the syntactic category layer in the same column.\nThis has the effect of ‘ANDing’ together the patterns for a single word, so it will give you words that have “fine” on the orthography layer have A on the syntacticCategory* layer.\n\nDo another search, for fine on the orthography layer and V on the syntacticCategory layer.\nNotice that the results are the same. This is because all of the instances of “fine” are marked as ‘possibly an adjective’ and also ‘possibly a verb’.\n\nAs you can see, simply tagging tokens with all possible syntactic categories from the CELEX lexicon leads to search results that are heavy on false positives. In order to tag tokens with a single syntactic category, we would need to perform ‘disambiguation’ by taking the surrounding transcript into account, in order to decide which of all the possibilities is the correct syntactic category. LaBB-CAT has two layer managers that perform such part-of-speech tagging: the “StanfordPosTagger” and the “MorTagger”. Neither of these use the exact same syntactic labels that are used by CELEX.\n\n\n\nCELEX can be used to retrieve syllable-counts for words, which in turn can be used, with duration information, to compute speech rate. For each line in each transcript, we already have the start time and the end time, from which we can calculate the duration of the line. All we need now is the number of syllables per line, and we can compute the syllables-per-minute speech rate for each line.\n\nCreate a new word layer:\n\nLayer ID: syllableCount\nType: Number\nAlignment: None\nManager: CELEX English\nProject: CELEX\nDescription: Number of syllables\n...configured with the Syllable count option ticked\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nEnsure the First match only option is ticked for this layer.\n If you’re not sure why, this is explained in the online help for the layer configuration page.\n\n\n\nOnce the layer has finished generating, have a look at a transcript or two to check the results.\nClick the phrase layers menu option.\nAdd a new phrase layer for speech rate. Key points are:\n\nThe layer manager to use is the Statistics Layer Manager.\nThe layer to summarize should be the syllableCount layer.\nThe statistic to compute is Label-Sum Rate (per minute)\n\n\n\n\n\n\nTip\n\n\n\n If in doubt, the online help may help.\n\n\nYou can calculate over whatever scopes you like, but if you select Utterances this will give you a local speech rate which might be useful when looking at individual search results, and Participants might be interesting if you want to compare speech rate between speakers.\n\nHave a look in a transcript or two, and a participant or two, to see what the annotations you just generated look like.\n\n\n\n\nNow we’re going to create a phonemic-transcription layer.\n\nCreate a new word layer, called phonemes, similar to previous CELEX layers. Key points are:\n\nThe layer type should be set to Phonological.\nThe Phonology option should be selected in the layer configuration.\nMake sure the Pronounce Event Override option is ticked.\nThis means that if the original ELAN transcript contained a ‘pronounce’ annotation for a word (these are marked in ELAN with square brackets), specifying its pronunciation, then the ‘pronounce’ annotation is used instead of the phonemic transcription from CELEX.\nEnsure the Generates Segments option is un-ticked.\nThis option allows the layer manager to create segment (sub-word) annotations from the phonemic transcriptions, but we don’t want this because in the next exercise, we’re going to get HTK to do that instead.\n\nOnce the layer is finished generating, go to a transcript to see what it looks like.\n\nYou will notice that the annotations are displayed using IPA symbols. However, CELEX doesn’t use IPA symbols directly, it actually uses the ‘DISC’ encoding for phonemes, which uses ordinary ‘typewriter’ characters (ASCII), and uses exactly one character per phoneme. The IPA symbols are being displayed by LaBB-CAT to provide a linguist-friendly representation of the phonemic transcription. But you can see the underlying DISC characters by selecting the ASCII option on the layer in the transcript.\n\nSelect ASCII on the phonemes layer, to see what CELEX is actually producing.\n\nYou may find that this is somewhat harder to read. Diphthongs are generally represented by digits, schwa is @, and various other characters are used to represent affricates, etc.\nIt’s nice to display the IPA symbols, but it’s important to understand the DISC symbols (shown in table below, because they are what we have to use when searching on the *phonemes* layer, which we are going to try now.\n\n\n\n\nIPA\nDISC\n \n \nIPA\nDISC\n \n\n\np\np\npat\n \nɪ\nI\nKIT\n\n\nb\nb\nbad\n \nε\nE\nDRESS\n\n\nt\nt\ntack\n \næ\n{\nTRAP\n\n\nd\nd\ndad\n \nʌ\nV\nSTRUT\n\n\nk\nk\ncad\n \nɒ\nQ\nLOT\n\n\ng\ng\ngame\n \nʊ\nU\nFOOT\n\n\nŋ\nN\nbang\n \nə\n@\nanother\n\n\nm\nm\nmat\n \ni:\ni\nFLEECE\n\n\nn\nn\nnat\n \nα: \n#\nfather\n\n\nl\nl\nlad\n \nɔ:\n$\nTHOUGHT\n\n\nr\nr\nrat\n \nu:\nu\nGOOSE\n\n\nf\nf\nfat\n \nɜ:\n3\nNURSE\n\n\nv\nv\nvat\n \neɪ\n1\nFACE\n\n\nθ\nT\nthin\n \nαɪ\n2\nPRICE\n\n\nð\nD\nthen\n \nɔɪ\n4\nCHOICE\n\n\ns\ns\nsap\n \nəʊ\n5\nGOAT\n\n\nz\nz\nzap\n \nαʊ\n6\nMOUTH\n\n\n∫\nS\nsheep\n \nɪə\n7\nNEAR\n\n\nʒ\nZ\nmeasure\n \nεə\n8\nSQUARE\n\n\nj\nj\nyank\n \nʊə\n9\nCURE\n\n\nx\nx\nloch\n \næ\nc\ntimbre\n\n\nh\nh\nhad\n \nɑ̃ː\nq\ndétente\n\n\nw\nw\nwet\n \næ̃ː\n0\nlingerie\n\n\nʧ\nJ\ncheap\n \nɒ̃ː\n~\nbouillon\n\n\nʤ\n_\njeep\n \n \n \n \n\n\nŋ̩\nC\nbacon\n \n \n \n \n\n\nm̩\nF\nidealism\n \n \n \n \n\n\nn̩\nH\nburden\n \n \n \n \n\n\nl̩\nP\ndangle\n \n \n \n \n\n\n\n\nGo to the search page.\nCreate a search matrix that’s two words wide, and includes the orthography and phonemes layers.\n\nNow we’re going to do a search for the word “the” followed by a word that starts with schwa.\n\nType the in the first orthography box.\nClick the second box on the phonemes layer, but don’t enter anything in the box yet.\nThe box has a little « button to the right of it.\nHover the mouse over it to see what it says, and then click it.\nYou will see that a section opens with a bunch of phoneme symbols on it.\nFind the schwa symbol ə and click it.\nYou will see that a @ symbol appears in the box.\n@ is the DISC symbol for ə, so in order to search for schwa, we have to use it in our search pattern.\nWe want words that start with schwa, so type .* after the @ symbol.\nClick Search.\n\nYou will see that some of the words being matched are words that you might not normally think start with a schwa. LaBB-CAT is matching words against all their possible phonemic transcriptions, so if CELEX has multiple possible pronunciations for a word, and one of them starts with schwa, it will be matched.\nYou can check this by clicking on a match, and then clicking on the word in the transcript and selecting Edit, which displays all the annotations for the given token.\nNow that we have phonemic transcripts, we can do a better job of the search we tried in an earlier exercise – “the” followed by a word starting with a vowel…\n\nChange your search so that, instead of just @ at the beginning of the word, it matches any vowel.\n\nYou could use the square-brackets [] at the start of your pattern, and type all vowel symbols inside them - Note that the vowels in the DISC representation extend beyond a, e, i, o, and u - you should add in all the vowels you see in the list that appears when you expand the IPA helper, including all the diphthongs.\nAlternatively, you can simply click the VOWEL link in the ‘Phoneme Symbol Selector’, which will add all the DISC vowels for you, already enclosed in square-brackets.\n\nRun the search and check that it’s giving you what you expect. Notice that now there are no ‘false positives’ like “the one” that we were getting when searching by orthography alone.\n\nNow that you’ve generated a few different layers, and have seen how the search matrix works, you might want to try out some of the following searches, or invent some others:\n\nInstances of an article followed by a noun\nWords which have the DRESS vowel as the second phoneme\nThe word “the” followed by a word beginning with the phoneme /k/\nWords ending with schwa, followed by words beginning with /p/ or /b/\nWords that begin with “k” in their spelling, but begin with the phoneme /n/\nWords that begin with “k” in their spelling, but do not begin with the phoneme /n/\nPlurals that end in /s/ or /z/ or /ɪz/"
  },
  {
    "objectID": "worksheets/course/7a-lexicon-celex.html#installation",
    "href": "worksheets/course/7a-lexicon-celex.html#installation",
    "title": "7a. CELEX",
    "section": "",
    "text": "The first thing we’re going to do is install the CELEX layer manager.\nThis requires having the LDC’s CELEX data files on your computer, which will be processed by the layer manager in order to insert the data into the LaBB-CAT database. If you received the CELEX files in a ZIP file, you need to unzip that into a folder, and remember the location of that folder, as you’ll need it during the installation process.\n\nFirst of all, create a new project called CELEX with a description: CELEX Annotations\nClick the layer managers menu option.\nClick the List of layer managers that are not yet installed link near the bottom.\nFind “CELEX English” in the list, and click its Install button.\nYou will see a form asking for various details. You can leave most of these with their default values. The one exception is the CELEX  ENGLISH data folder option.\nSet the CELEX ENGLISH data folder parameter to the directory path that leads to the CELEX files on your LaBB-CAT computer.\nClick Install.\nYou will see a progress bar while the layer manager loads the data from the CELEX files into the LaBB-CAT database. This will take a few minutes.\nOnce it’s finished, you will see a new window open with information about the CELEX English layer manager.\nReading this information page, you will see some instructions on how to create CELEX annotation layers - leave this tab open for now, as we’re going to need those instructions next."
  },
  {
    "objectID": "worksheets/course/7a-lexicon-celex.html#morphology",
    "href": "worksheets/course/7a-lexicon-celex.html#morphology",
    "title": "7a. CELEX",
    "section": "",
    "text": "Now that we’ve installed the layer manager, we’ll create our first annotations from CELEX - a layer with morphological annotations.\n\nFollow the instructions on the information page to create a layer for word morphology - i.e.:\n\nLayer ID: morphology\nType: Text\nAlignment: None\nManager: CELEX English\nProject: CELEX\nDescript Morphological parses\n...configured with the Morphology option selected, and the default values for everything else.\n\n\n\n\n\n\n\n\nTip\n\n\n\n If you’re curious about what the configuration options do, and how you can test out the results of your configuration, check the online help page when you are configuring the layer.\n\n\n\nOnce the layer has finished generating, select the transcripts menu option, and open the first transcript in the list.\nTick the CELEX project, and then tick your new morphology layer.\nYou will see that each word is tagged with morphological information.\n\nIf you were to do a search for words ending in “ing” on the orthography layer, you would get both gerunds like “coming” and also words like “thing” and “anything” whose “ing” is part of the base word, not a morphological affix. You can now tell these apart in searches, by searching the morphology layer for words that end in “+ing”.\n\nDo a search on the orthography layer of words ending in “ing”. Leave the results tab open, so you can compare these results with the next search …\nNow do a search on the morphology layer of words ending in “+ing”, and compare the results.\n\n\n\n\n\n\n\nImportant\n\n\n\nRemember that in regular expressions the ‘plus’ character + has a special meaning - it means “one or more of the previous thing”.\nIn order to search for a literal “+” in the annotation, you have to ‘escape’ the +. Consult the Regular Expression help page to figure out how to do that."
  },
  {
    "objectID": "worksheets/course/7a-lexicon-celex.html#syntactic-categories",
    "href": "worksheets/course/7a-lexicon-celex.html#syntactic-categories",
    "title": "7a. CELEX",
    "section": "",
    "text": "Now we will create a layer for syntactic categories from CELEX.\n\nCreate a new layer for annotating words with their syntactic categories from CELEX:\n\nLayer ID: syntacticCategory\nType: Text\nAlignment: None\nManager: CELEX English\nProject: CELEX\nDescription: All possible syntactic categories\n...configured with the Syntax option ticked, and the default values for everything else.\n\nOnce the layer has finished generating, go to the search page and do a search for the word “fine” on the orthography layer.\nOpen the transcript of the first match.\nTick the syntactic category layer to display the annotations that have just been computed.\nNow find your instance of the word “fine” again (it’s highlighted in the transcript text).\nYou will see that it has “A” for adjective above it.\nClick on the word “fine” and select the Edit option on the menu that appears.\nNow look for the syntacticCategory layer. You will see that, in addition to “A”, there are several other annotations that are invisible on the transcript.\nThese are all the possible syntactic categories for the word “fine” ordered most-frequent first. Only the first one is displayed in the transcript, but when you do searches, all of them are searched.\nOn the search page, do a search for fine on the orthography layer and A on the syntactic category layer in the same column.\nThis has the effect of ‘ANDing’ together the patterns for a single word, so it will give you words that have “fine” on the orthography layer have A on the syntacticCategory* layer.\n\nDo another search, for fine on the orthography layer and V on the syntacticCategory layer.\nNotice that the results are the same. This is because all of the instances of “fine” are marked as ‘possibly an adjective’ and also ‘possibly a verb’.\n\nAs you can see, simply tagging tokens with all possible syntactic categories from the CELEX lexicon leads to search results that are heavy on false positives. In order to tag tokens with a single syntactic category, we would need to perform ‘disambiguation’ by taking the surrounding transcript into account, in order to decide which of all the possibilities is the correct syntactic category. LaBB-CAT has two layer managers that perform such part-of-speech tagging: the “StanfordPosTagger” and the “MorTagger”. Neither of these use the exact same syntactic labels that are used by CELEX."
  },
  {
    "objectID": "worksheets/course/7a-lexicon-celex.html#syllable-count-and-speech-rate",
    "href": "worksheets/course/7a-lexicon-celex.html#syllable-count-and-speech-rate",
    "title": "7a. CELEX",
    "section": "",
    "text": "CELEX can be used to retrieve syllable-counts for words, which in turn can be used, with duration information, to compute speech rate. For each line in each transcript, we already have the start time and the end time, from which we can calculate the duration of the line. All we need now is the number of syllables per line, and we can compute the syllables-per-minute speech rate for each line.\n\nCreate a new word layer:\n\nLayer ID: syllableCount\nType: Number\nAlignment: None\nManager: CELEX English\nProject: CELEX\nDescription: Number of syllables\n...configured with the Syllable count option ticked\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nEnsure the First match only option is ticked for this layer.\n If you’re not sure why, this is explained in the online help for the layer configuration page.\n\n\n\nOnce the layer has finished generating, have a look at a transcript or two to check the results.\nClick the phrase layers menu option.\nAdd a new phrase layer for speech rate. Key points are:\n\nThe layer manager to use is the Statistics Layer Manager.\nThe layer to summarize should be the syllableCount layer.\nThe statistic to compute is Label-Sum Rate (per minute)\n\n\n\n\n\n\nTip\n\n\n\n If in doubt, the online help may help.\n\n\nYou can calculate over whatever scopes you like, but if you select Utterances this will give you a local speech rate which might be useful when looking at individual search results, and Participants might be interesting if you want to compare speech rate between speakers.\n\nHave a look in a transcript or two, and a participant or two, to see what the annotations you just generated look like."
  },
  {
    "objectID": "worksheets/course/7a-lexicon-celex.html#phonology",
    "href": "worksheets/course/7a-lexicon-celex.html#phonology",
    "title": "7a. CELEX",
    "section": "",
    "text": "Now we’re going to create a phonemic-transcription layer.\n\nCreate a new word layer, called phonemes, similar to previous CELEX layers. Key points are:\n\nThe layer type should be set to Phonological.\nThe Phonology option should be selected in the layer configuration.\nMake sure the Pronounce Event Override option is ticked.\nThis means that if the original ELAN transcript contained a ‘pronounce’ annotation for a word (these are marked in ELAN with square brackets), specifying its pronunciation, then the ‘pronounce’ annotation is used instead of the phonemic transcription from CELEX.\nEnsure the Generates Segments option is un-ticked.\nThis option allows the layer manager to create segment (sub-word) annotations from the phonemic transcriptions, but we don’t want this because in the next exercise, we’re going to get HTK to do that instead.\n\nOnce the layer is finished generating, go to a transcript to see what it looks like.\n\nYou will notice that the annotations are displayed using IPA symbols. However, CELEX doesn’t use IPA symbols directly, it actually uses the ‘DISC’ encoding for phonemes, which uses ordinary ‘typewriter’ characters (ASCII), and uses exactly one character per phoneme. The IPA symbols are being displayed by LaBB-CAT to provide a linguist-friendly representation of the phonemic transcription. But you can see the underlying DISC characters by selecting the ASCII option on the layer in the transcript.\n\nSelect ASCII on the phonemes layer, to see what CELEX is actually producing.\n\nYou may find that this is somewhat harder to read. Diphthongs are generally represented by digits, schwa is @, and various other characters are used to represent affricates, etc.\nIt’s nice to display the IPA symbols, but it’s important to understand the DISC symbols (shown in table below, because they are what we have to use when searching on the *phonemes* layer, which we are going to try now.\n\n\n\n\nIPA\nDISC\n \n \nIPA\nDISC\n \n\n\np\np\npat\n \nɪ\nI\nKIT\n\n\nb\nb\nbad\n \nε\nE\nDRESS\n\n\nt\nt\ntack\n \næ\n{\nTRAP\n\n\nd\nd\ndad\n \nʌ\nV\nSTRUT\n\n\nk\nk\ncad\n \nɒ\nQ\nLOT\n\n\ng\ng\ngame\n \nʊ\nU\nFOOT\n\n\nŋ\nN\nbang\n \nə\n@\nanother\n\n\nm\nm\nmat\n \ni:\ni\nFLEECE\n\n\nn\nn\nnat\n \nα: \n#\nfather\n\n\nl\nl\nlad\n \nɔ:\n$\nTHOUGHT\n\n\nr\nr\nrat\n \nu:\nu\nGOOSE\n\n\nf\nf\nfat\n \nɜ:\n3\nNURSE\n\n\nv\nv\nvat\n \neɪ\n1\nFACE\n\n\nθ\nT\nthin\n \nαɪ\n2\nPRICE\n\n\nð\nD\nthen\n \nɔɪ\n4\nCHOICE\n\n\ns\ns\nsap\n \nəʊ\n5\nGOAT\n\n\nz\nz\nzap\n \nαʊ\n6\nMOUTH\n\n\n∫\nS\nsheep\n \nɪə\n7\nNEAR\n\n\nʒ\nZ\nmeasure\n \nεə\n8\nSQUARE\n\n\nj\nj\nyank\n \nʊə\n9\nCURE\n\n\nx\nx\nloch\n \næ\nc\ntimbre\n\n\nh\nh\nhad\n \nɑ̃ː\nq\ndétente\n\n\nw\nw\nwet\n \næ̃ː\n0\nlingerie\n\n\nʧ\nJ\ncheap\n \nɒ̃ː\n~\nbouillon\n\n\nʤ\n_\njeep\n \n \n \n \n\n\nŋ̩\nC\nbacon\n \n \n \n \n\n\nm̩\nF\nidealism\n \n \n \n \n\n\nn̩\nH\nburden\n \n \n \n \n\n\nl̩\nP\ndangle\n \n \n \n \n\n\n\n\nGo to the search page.\nCreate a search matrix that’s two words wide, and includes the orthography and phonemes layers.\n\nNow we’re going to do a search for the word “the” followed by a word that starts with schwa.\n\nType the in the first orthography box.\nClick the second box on the phonemes layer, but don’t enter anything in the box yet.\nThe box has a little « button to the right of it.\nHover the mouse over it to see what it says, and then click it.\nYou will see that a section opens with a bunch of phoneme symbols on it.\nFind the schwa symbol ə and click it.\nYou will see that a @ symbol appears in the box.\n@ is the DISC symbol for ə, so in order to search for schwa, we have to use it in our search pattern.\nWe want words that start with schwa, so type .* after the @ symbol.\nClick Search.\n\nYou will see that some of the words being matched are words that you might not normally think start with a schwa. LaBB-CAT is matching words against all their possible phonemic transcriptions, so if CELEX has multiple possible pronunciations for a word, and one of them starts with schwa, it will be matched.\nYou can check this by clicking on a match, and then clicking on the word in the transcript and selecting Edit, which displays all the annotations for the given token.\nNow that we have phonemic transcripts, we can do a better job of the search we tried in an earlier exercise – “the” followed by a word starting with a vowel…\n\nChange your search so that, instead of just @ at the beginning of the word, it matches any vowel.\n\nYou could use the square-brackets [] at the start of your pattern, and type all vowel symbols inside them - Note that the vowels in the DISC representation extend beyond a, e, i, o, and u - you should add in all the vowels you see in the list that appears when you expand the IPA helper, including all the diphthongs.\nAlternatively, you can simply click the VOWEL link in the ‘Phoneme Symbol Selector’, which will add all the DISC vowels for you, already enclosed in square-brackets.\n\nRun the search and check that it’s giving you what you expect. Notice that now there are no ‘false positives’ like “the one” that we were getting when searching by orthography alone.\n\nNow that you’ve generated a few different layers, and have seen how the search matrix works, you might want to try out some of the following searches, or invent some others:\n\nInstances of an article followed by a noun\nWords which have the DRESS vowel as the second phoneme\nThe word “the” followed by a word beginning with the phoneme /k/\nWords ending with schwa, followed by words beginning with /p/ or /b/\nWords that begin with “k” in their spelling, but begin with the phoneme /n/\nWords that begin with “k” in their spelling, but do not begin with the phoneme /n/\nPlurals that end in /s/ or /z/ or /ɪz/"
  },
  {
    "objectID": "worksheets/course/index.html",
    "href": "worksheets/course/index.html",
    "title": "LaBB-CAT Course",
    "section": "",
    "text": "LaBB-CAT Course\nThis course is intended to teach participants how to use LaBB-CAT from scratch, including: - installing and setting up the software - uploading data - defining speech-elicitation tasks - manually annotating the transcripts - setting up automatic annotations of different types - forced alignment\nIt is designed to be taught in three two-hour sessions.\nMuch of this material is demonstrated in these YouTube videos\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023-2024 NZILBB"
  },
  {
    "objectID": "worksheets/course/7-lexicon.html",
    "href": "worksheets/course/7-lexicon.html",
    "title": "7. Lexicons",
    "section": "",
    "text": "7. Lexicons\nLaBB-CAT can be integrated with various lexicon to facilitate automatic tagging of word tokens.\nFor our English data, there are two main options:\n\nCELEX, a ‘British English’ lexicon which must be purchased from the LDC, and\nThe CMU Pronouncing Dictionary, an ‘American English’ lexicon that's free to download.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023-2024 NZILBB"
  },
  {
    "objectID": "worksheets/course/8b-forced-alignment-mfa.html",
    "href": "worksheets/course/8b-forced-alignment-mfa.html",
    "title": "8b. MFA",
    "section": "",
    "text": "The Montreal Forced Aligner (MFA) is a 3rd-party tool developed by Michael McAuliffe and others that can use words with phonemic transcriptions, and the corresponding audio, to force-align words and phones; i.e. determine the start and end time of each speech sound within each word, and thus the start/end times of the words.\nThe annotator can work in two modes:\n\nTrain and Align - acoustic models are trained on the data you want to align, which can be in any language as long as you have a pronunciation dictionary for it.\nPre-trained Models/Dictionaries - pre-trained models and pronunciation dictionaries are supplied by the Montreal Forced Aligner and used for forced alignment. Languages for which dictionaries are available include:\n\nEnglish\nFrench\nGerman\nBrazilian Portuguese\nSpanish\nCatalan\n\n\nAs the data we have is in English, we will use the Pre-trained Models/Dictionaries approach in this exercise.\nIn this exercise you will\n\ninstall the MFA Layer Manager,\nforce-align the speech of all of the participants in your database, and\ncheck and manually correct the alignments.\n\n\n\n\n\n\n\nImportant\n\n\n\nIn this exercise, you will set up Praat Integration in your web browser. There is currently no Praat integration support for Microsoft’s Edge’ browser, so if you normally use ‘Edge’ on Windows, you may need to swap to another browser for this exercise - e.g. Google Chrome, or Mozilla Firefox.\n\n\n\n\n\n\nMFA is a 3rd-party tool (https://montrealcorpustools.github.io/Montreal-Forced-Aligner/) that LaBB-CAT integrates with via a Layer Manager module. MFA is not included as part of LaBB-CAT, and so it must be installed on the server you have installed LaBB-CAT on before you can integrate LaBB-CAT with it.\nIf MFA has not been installed already, please follow the following steps, depending on the operatings system of your LaBB-CAT server:\n\n\n\n\n\n\nLinux\n\n\n\n\n\nTo install the Montreal Forced Aligner on Linux systems for all users, so that your web server can access it if required:\n\nDownload Miniconda:\nwget https://repo.anaconda.com/miniconda/Miniconda3-py38\\_4.10.3-Linux-x86\\_64.sh\nStart the installer:\nsudo bash Miniconda3-py38\\_4.10.3-Linux-x86\\_64.sh\nWhen asked the location to install Miniconda, use:\n/opt/conda\nWhen asked whether the installer should initialize Miniconda, this is unnecessary so you can respond no\nChange ownership of the conda files):\nsudo chown -R $USERNAME:$USERNAME /opt/conda\nMake conda accessible to all users (so you web server can access MFA):\nchmod -R go-w /opt/conda\nchmod -R go+rX /opt/conda\nInstall the Montreal Forced Aligner\n/opt/conda/bin/conda create -n aligner -c conda-forge montreal-forced-aligner\n\n\n\n\n\n\n\n\n\n\nWindows\n\n\n\n\n\nTo install the Montreal Forced Aligner on Windows systems for all users, so that your web server can access it if required:\n\nDownload the Miniconda installer:   \nhttps://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe\nStart the installer by double-clicking it.\nWhen asked, select the “Install for all users” option. This will install conda somewhere like\nC:\\ProgramData\\Miniconda3\nWhen asked, tick the add to PATH option.\nInstall the Montreal Forced Aligner by specifying a path to the environment\nconda create -c conda-forge -p C:\\ProgramData\\Miniconda3\\envs\\aligner montreal-forced-aligner\n\n\n\n\n\n\nIf your LaBB-CAT server is installed in a Docker Container, it can download and install Miniconda and MFA itself, as part of the process of installing the MFA Manager (below)\n\n\n\n\nOnce MFA has been installed, you have to install the MFA Manager, which is the LaBB-CAT module that provides MFA with all the data it needs, and then saves to alignments MFA produces back to your database.\n\nSelect the layer managers menu option.\nFollow the List of layer managers that are not yet installed link at the bottom.\nFind MFA Manager in the list, and press its Install button and then press Install again.\n\nAs long as MFA has been installed for all users, you should see a box that’s already filled in with the location that MFA was installed to in the Path to MFA box.\n\nIf the Path to MFA box is empty and there’s an Attempt to Install MFA button, click the button, and LaBB-CAT will try to install MFA on the server for you. This process can take a few minutes, and the Configure button will be disabled until it’s finished.\nPress Configure to continue the layer manager installation.\nYou will see a window open with some information about integrating with MFA, including the information you’ve already read above.\nNow you need to add a phrase layer for the MFA configuration:\n\nLayer ID: mfa\nType: Text\nAlignment: Intervals\nManager: MFA Manager\nGenerate: always\nDescription: MFA alignment time\n\n\n\n\n\n\n\n\nNote\n\n\n\nAfter saving the layer, the MFA configuraion form will appear.\nInitially, there will be a ‘spinner’ and the form will be disabled while LaBB-CAT is making internet requests in order to retrieve lists of available dictionaries and acoustic models.\nThis process may take a few seconds, depending on LaBB-CAT’s connection to the internet.\n\n\n\nWhen you configure the layer, set the following options:\n\nDictionary Name: english_mfa\nPretrained Acoustic Models: english_mfa\nThe rest of the options can be left as their default values.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you’re curious about what the configuration options do, hover your mouse over each option to see a ‘tool tip’ that describes what the option is for.\n\n\n\nPress Set Parameters We will not click Regenerate to force-align the whole corpus just yet. We need to tweak a few other settings, and then align a single participant’s speech.\n\nWhen forced-alignment is done, the resulting aligned phones will be saved on the segment layer. By default, this has it’s type set to Phonological, which assumes that the phoneme symbols will be those defined by the CELEX DISC encoding. However, MFA uses its own phoneme symbols, which are different from the CELEX DISC ones.\nTo make later processing easier, we’re going to change the type of the segment layer to Text, and let LaBB-CAT know what the possible phoneme labels are.\n\nSelect the segment layers menu option.\nYou will see a single layer listed, called segment.\nChange the Type of the segment layer to Text.\nPress Save.\nA new icon appears with a tag  icon - if you hover the mouse over this button, you’ll see it’s for setting Valid labels.\nClick the Valid labels icon\nYou will see a page that allows you to add a list of possible labels.\nBut we don’t know what the valid labels are yet. The labels used by MFA depend on the dictionary/models selected when defining the layer. The details for all dictionaries are in MFA Models documentation.\nOpen the MFA Models documentation for english_mfa.\nYou will see a page that includes information about the dictionary including the Phones used by it.\nSelect and copy the list of phones used by the english_mfa dictionary - i.e. all of these:\na aj aw aː b bʲ c cʰ d dʒ dʲ e ej f fʲ h i iː j k kʰ l m mʲ m̩ n n̩ o ow p pʰ pʲ s t tʃ tʰ tʲ u uː v vʲ w z æ ç ð ŋ ɐ ɑ ɑː ɒ ɒː ɔ ɔj ə əw ɚ ɛ ɛː ɜ ɜː ɝ ɟ ɡ ɪ ɫ ɫ̩ ɱ ɲ ɹ ɾ ʃ ʉ ʉː ʊ ʎ ʒ ʔ θ\n\n\nBack in the Valid Labels page in LaBB-CAT, paste all of the phone symbols into the box labelled Label\n\n\n\nPress New.\nYou will see that all of the labels are separately added to the list of valid labels.\n\n\nDefining this list for the segment layer means that, when you search the segment layer for specific phones, LaBB-CAT can display a clickable list of possibilities.\n\nAt the bottom of the list, there’s a Save button. Click it to save your changes.\n\n\n\n\n\n\nSelect the participants option on the menu.\nFind the participant UC207YW and tick their checkbox.\n\n\n\n\n\n\n\nNote\n\n\n\nAlthough we’re only going to force-align the utterances of a single speaker in this exercise, you can align the utterances of multiple speakers at once, by ticking all their checkboxes on the participants page before continuing with the next step…\n\n\n\nPress the All Utterances button above the list of participants\nPress List.\nYou will see a progress bar while all their utterances are identified. Then a results page will be displayed, listing the first 20 utterances.\nClick the Mfa button at the bottom.\nYou will see a progress bar appear, while LaBB-CAT gathers the files that MFA needs, runs MFA, and parses the resulting alignments. This will take a few minutes.\n\n\n\n\nOnce forced alignment is complete, you can inspect/correct alignments using LaBB-CAT’s integration with Praat.\n\nGo to the transcripts page and open the UC207YW.eaf transcript.\nTick both the mfa layer and the segment layer. \nYou will see which lines have been force-aligned, as they have an MFA timestamp, and have the segment layer filled in.\n\n\nThe interactive transcript page doesn’t show you the alignments of the words or phones, but you can see those using Praat. You can open individual utterances in Praat directly from the transcript page, but first, the LaBB-CAT/Praat integration has to be set up; this only has to be done once:\n\nOn the top-right of the page, above the playback controls, there’s a Praat icon  - click it.\nFollow the instructions that appear (these vary depending on what web browser you use).\nYou may be asked whether to allow the “LaBB-CAT Integration Applet” to run. If you tick the “Do not show this again” option, then this message will not appear every time you open a transcript.\nYou may need to grant a browser extension permission to install, and it’s possible you will need a connection to the internet in order to download this extension.\nYou also may be asked where Praat is installed; Navigate to the location where Praat is installed, and double-click the “Praat.exe” file (on some systems the file may simply be called “Praat”). The Praat program may open, and then immediately close, as LaBB-CAT tests it can communicate with Praat.\n\nNow Praat integration has been set up, and you should be able to access Praat options in the transcript page from now on…\n\nClick on a line that has been aligned, and select the Open Text Grid in Praat option on the menu.\nYou may be asked you if want to allow access to the “LaBB-CAT Integration Applet” - if so, tick “Do not show this again”, and click Allow.\nPraat should open, and show you a spectrogram of the line’s audio, with a TextGrid below that includes the words and the segments.\nIf you click on a word, and hit the tab key, the word’s interval is played. Try out various words, and see what you think about how accurate HTK has been with its alignment.\nTry this out with different lines in the transcript.\nYou will see that in some cases the alignment is pretty good, and in other cases, it’s not so good. In the not-so-good cases, see if you can figure out why HTK got it wrong.\n\n\nYou may have noticed that, each time you open an utterance in Praat, a button appears in the transcript to the left of the line, labelled Import Changes. This button allows you to save any adjustments you might want to make to the alignments back into the LaBB-CAT database.\n\nIf you feel confident using Praat, open an utterance TextGrid, adjust the alignments of the words an phones so that they’re more accurate, and then click the Import Changes button in the transcript.\n\n\n\n\n\n\n\nWarning\n\n\n\nThese changes are flagged as manual edits, so if forced-alignment is run again, they will not be over-written with new bad alignments. Therefore it’s important that the changes you make are actually improvements, because HTK will never change them again.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThere are some rules about what you can change: 1. You’re not allowed to add or delete words (if this is necessary, it should be done by correcting the transcript instead). 2. All the phones must be within the bounds of their own word. 3. The start of the first phone should line up with the start of the word, and the end of the last phone should line up with the end of the word. 4. You should not change the alignment of the utterance itself (which would only be possible if you select the Open Text Grid incl. ± 1 utterance in Praat option).\n\n\nIn this exercise, you have seen how MFA can be used to compute word and phone alignments automatically from your data, and when using a pronunciation dictionary and pre-trained acoustic models, the process is very straightforward.\nSuch dictionaries/models are only available for a limited number of languages, but if you have a pronunciation dictionary for the language your corpus uses, MFA can also be used to train its own acoustic models from your corpus, and then use them for forced alignment. This process involves a fair amount of careful transcription, tagging, and dictionary filling.\nPerfect automatic alignments are not guaranteed, but LaBB-CAT has a mechanism for manually correcting poor alignments."
  },
  {
    "objectID": "worksheets/course/8b-forced-alignment-mfa.html#installation",
    "href": "worksheets/course/8b-forced-alignment-mfa.html#installation",
    "title": "8b. MFA",
    "section": "",
    "text": "MFA is a 3rd-party tool (https://montrealcorpustools.github.io/Montreal-Forced-Aligner/) that LaBB-CAT integrates with via a Layer Manager module. MFA is not included as part of LaBB-CAT, and so it must be installed on the server you have installed LaBB-CAT on before you can integrate LaBB-CAT with it.\nIf MFA has not been installed already, please follow the following steps, depending on the operatings system of your LaBB-CAT server:\n\n\n\n\n\n\nLinux\n\n\n\n\n\nTo install the Montreal Forced Aligner on Linux systems for all users, so that your web server can access it if required:\n\nDownload Miniconda:\nwget https://repo.anaconda.com/miniconda/Miniconda3-py38\\_4.10.3-Linux-x86\\_64.sh\nStart the installer:\nsudo bash Miniconda3-py38\\_4.10.3-Linux-x86\\_64.sh\nWhen asked the location to install Miniconda, use:\n/opt/conda\nWhen asked whether the installer should initialize Miniconda, this is unnecessary so you can respond no\nChange ownership of the conda files):\nsudo chown -R $USERNAME:$USERNAME /opt/conda\nMake conda accessible to all users (so you web server can access MFA):\nchmod -R go-w /opt/conda\nchmod -R go+rX /opt/conda\nInstall the Montreal Forced Aligner\n/opt/conda/bin/conda create -n aligner -c conda-forge montreal-forced-aligner\n\n\n\n\n\n\n\n\n\n\nWindows\n\n\n\n\n\nTo install the Montreal Forced Aligner on Windows systems for all users, so that your web server can access it if required:\n\nDownload the Miniconda installer:   \nhttps://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe\nStart the installer by double-clicking it.\nWhen asked, select the “Install for all users” option. This will install conda somewhere like\nC:\\ProgramData\\Miniconda3\nWhen asked, tick the add to PATH option.\nInstall the Montreal Forced Aligner by specifying a path to the environment\nconda create -c conda-forge -p C:\\ProgramData\\Miniconda3\\envs\\aligner montreal-forced-aligner\n\n\n\n\n\n\nIf your LaBB-CAT server is installed in a Docker Container, it can download and install Miniconda and MFA itself, as part of the process of installing the MFA Manager (below)\n\n\n\n\nOnce MFA has been installed, you have to install the MFA Manager, which is the LaBB-CAT module that provides MFA with all the data it needs, and then saves to alignments MFA produces back to your database.\n\nSelect the layer managers menu option.\nFollow the List of layer managers that are not yet installed link at the bottom.\nFind MFA Manager in the list, and press its Install button and then press Install again.\n\nAs long as MFA has been installed for all users, you should see a box that’s already filled in with the location that MFA was installed to in the Path to MFA box.\n\nIf the Path to MFA box is empty and there’s an Attempt to Install MFA button, click the button, and LaBB-CAT will try to install MFA on the server for you. This process can take a few minutes, and the Configure button will be disabled until it’s finished.\nPress Configure to continue the layer manager installation.\nYou will see a window open with some information about integrating with MFA, including the information you’ve already read above.\nNow you need to add a phrase layer for the MFA configuration:\n\nLayer ID: mfa\nType: Text\nAlignment: Intervals\nManager: MFA Manager\nGenerate: always\nDescription: MFA alignment time\n\n\n\n\n\n\n\n\nNote\n\n\n\nAfter saving the layer, the MFA configuraion form will appear.\nInitially, there will be a ‘spinner’ and the form will be disabled while LaBB-CAT is making internet requests in order to retrieve lists of available dictionaries and acoustic models.\nThis process may take a few seconds, depending on LaBB-CAT’s connection to the internet.\n\n\n\nWhen you configure the layer, set the following options:\n\nDictionary Name: english_mfa\nPretrained Acoustic Models: english_mfa\nThe rest of the options can be left as their default values.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you’re curious about what the configuration options do, hover your mouse over each option to see a ‘tool tip’ that describes what the option is for.\n\n\n\nPress Set Parameters We will not click Regenerate to force-align the whole corpus just yet. We need to tweak a few other settings, and then align a single participant’s speech.\n\nWhen forced-alignment is done, the resulting aligned phones will be saved on the segment layer. By default, this has it’s type set to Phonological, which assumes that the phoneme symbols will be those defined by the CELEX DISC encoding. However, MFA uses its own phoneme symbols, which are different from the CELEX DISC ones.\nTo make later processing easier, we’re going to change the type of the segment layer to Text, and let LaBB-CAT know what the possible phoneme labels are.\n\nSelect the segment layers menu option.\nYou will see a single layer listed, called segment.\nChange the Type of the segment layer to Text.\nPress Save.\nA new icon appears with a tag  icon - if you hover the mouse over this button, you’ll see it’s for setting Valid labels.\nClick the Valid labels icon\nYou will see a page that allows you to add a list of possible labels.\nBut we don’t know what the valid labels are yet. The labels used by MFA depend on the dictionary/models selected when defining the layer. The details for all dictionaries are in MFA Models documentation.\nOpen the MFA Models documentation for english_mfa.\nYou will see a page that includes information about the dictionary including the Phones used by it.\nSelect and copy the list of phones used by the english_mfa dictionary - i.e. all of these:\na aj aw aː b bʲ c cʰ d dʒ dʲ e ej f fʲ h i iː j k kʰ l m mʲ m̩ n n̩ o ow p pʰ pʲ s t tʃ tʰ tʲ u uː v vʲ w z æ ç ð ŋ ɐ ɑ ɑː ɒ ɒː ɔ ɔj ə əw ɚ ɛ ɛː ɜ ɜː ɝ ɟ ɡ ɪ ɫ ɫ̩ ɱ ɲ ɹ ɾ ʃ ʉ ʉː ʊ ʎ ʒ ʔ θ\n\n\nBack in the Valid Labels page in LaBB-CAT, paste all of the phone symbols into the box labelled Label\n\n\n\nPress New.\nYou will see that all of the labels are separately added to the list of valid labels.\n\n\nDefining this list for the segment layer means that, when you search the segment layer for specific phones, LaBB-CAT can display a clickable list of possibilities.\n\nAt the bottom of the list, there’s a Save button. Click it to save your changes."
  },
  {
    "objectID": "worksheets/course/8b-forced-alignment-mfa.html#alignment",
    "href": "worksheets/course/8b-forced-alignment-mfa.html#alignment",
    "title": "8b. MFA",
    "section": "",
    "text": "Select the participants option on the menu.\nFind the participant UC207YW and tick their checkbox.\n\n\n\n\n\n\n\nNote\n\n\n\nAlthough we’re only going to force-align the utterances of a single speaker in this exercise, you can align the utterances of multiple speakers at once, by ticking all their checkboxes on the participants page before continuing with the next step…\n\n\n\nPress the All Utterances button above the list of participants\nPress List.\nYou will see a progress bar while all their utterances are identified. Then a results page will be displayed, listing the first 20 utterances.\nClick the Mfa button at the bottom.\nYou will see a progress bar appear, while LaBB-CAT gathers the files that MFA needs, runs MFA, and parses the resulting alignments. This will take a few minutes."
  },
  {
    "objectID": "worksheets/course/8b-forced-alignment-mfa.html#inspectioncorrection",
    "href": "worksheets/course/8b-forced-alignment-mfa.html#inspectioncorrection",
    "title": "8b. MFA",
    "section": "",
    "text": "Once forced alignment is complete, you can inspect/correct alignments using LaBB-CAT’s integration with Praat.\n\nGo to the transcripts page and open the UC207YW.eaf transcript.\nTick both the mfa layer and the segment layer. \nYou will see which lines have been force-aligned, as they have an MFA timestamp, and have the segment layer filled in.\n\n\nThe interactive transcript page doesn’t show you the alignments of the words or phones, but you can see those using Praat. You can open individual utterances in Praat directly from the transcript page, but first, the LaBB-CAT/Praat integration has to be set up; this only has to be done once:\n\nOn the top-right of the page, above the playback controls, there’s a Praat icon  - click it.\nFollow the instructions that appear (these vary depending on what web browser you use).\nYou may be asked whether to allow the “LaBB-CAT Integration Applet” to run. If you tick the “Do not show this again” option, then this message will not appear every time you open a transcript.\nYou may need to grant a browser extension permission to install, and it’s possible you will need a connection to the internet in order to download this extension.\nYou also may be asked where Praat is installed; Navigate to the location where Praat is installed, and double-click the “Praat.exe” file (on some systems the file may simply be called “Praat”). The Praat program may open, and then immediately close, as LaBB-CAT tests it can communicate with Praat.\n\nNow Praat integration has been set up, and you should be able to access Praat options in the transcript page from now on…\n\nClick on a line that has been aligned, and select the Open Text Grid in Praat option on the menu.\nYou may be asked you if want to allow access to the “LaBB-CAT Integration Applet” - if so, tick “Do not show this again”, and click Allow.\nPraat should open, and show you a spectrogram of the line’s audio, with a TextGrid below that includes the words and the segments.\nIf you click on a word, and hit the tab key, the word’s interval is played. Try out various words, and see what you think about how accurate HTK has been with its alignment.\nTry this out with different lines in the transcript.\nYou will see that in some cases the alignment is pretty good, and in other cases, it’s not so good. In the not-so-good cases, see if you can figure out why HTK got it wrong.\n\n\nYou may have noticed that, each time you open an utterance in Praat, a button appears in the transcript to the left of the line, labelled Import Changes. This button allows you to save any adjustments you might want to make to the alignments back into the LaBB-CAT database.\n\nIf you feel confident using Praat, open an utterance TextGrid, adjust the alignments of the words an phones so that they’re more accurate, and then click the Import Changes button in the transcript.\n\n\n\n\n\n\n\nWarning\n\n\n\nThese changes are flagged as manual edits, so if forced-alignment is run again, they will not be over-written with new bad alignments. Therefore it’s important that the changes you make are actually improvements, because HTK will never change them again.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThere are some rules about what you can change: 1. You’re not allowed to add or delete words (if this is necessary, it should be done by correcting the transcript instead). 2. All the phones must be within the bounds of their own word. 3. The start of the first phone should line up with the start of the word, and the end of the last phone should line up with the end of the word. 4. You should not change the alignment of the utterance itself (which would only be possible if you select the Open Text Grid incl. ± 1 utterance in Praat option).\n\n\nIn this exercise, you have seen how MFA can be used to compute word and phone alignments automatically from your data, and when using a pronunciation dictionary and pre-trained acoustic models, the process is very straightforward.\nSuch dictionaries/models are only available for a limited number of languages, but if you have a pronunciation dictionary for the language your corpus uses, MFA can also be used to train its own acoustic models from your corpus, and then use them for forced alignment. This process involves a fair amount of careful transcription, tagging, and dictionary filling.\nPerfect automatic alignments are not guaranteed, but LaBB-CAT has a mechanism for manually correcting poor alignments."
  },
  {
    "objectID": "worksheets/course/5-manual-annotation.html",
    "href": "worksheets/course/5-manual-annotation.html",
    "title": "5. Manual Annotation",
    "section": "",
    "text": "5. Manual Annotation\nNow we’re going to create our own layer for manual annotations, and explore ways of populating it. Let’s say we’re interested in the pronunciation of the vowel in the word “the” when the following word starts with a vowel. We’re going to:\n\nCreate a layer for annotations on tokens of the word “the”.\nSearch for tokens using word orthography, and identify ‘false positives’ (e.g. cases like “…the one…” where the spelling of the following word starts with a vowel but it’s not pronounced as a vowel).\nFor the ‘true positives’, perform some auditory analysis (i.e. listen to them) and tag each token accordingly.\n\n\n\nTo embark on this mini project, we’re first going to create a ‘project’ in LaBB-CAT to categorize our annotations.\nSelect the projects link on the menu.\nAdd a project called “the” with a description something like Pronunciation of the vowel in the when followed by a word-initial vowel, by filling in the form and pressing the New button.\n\nNow we’re going to create a layer to store our annotations…\n\nSelect on the word layers option on the menu.\nYou will see a list of existing word layers, including the orthography layer, the lexical layer, etc.\nThe row of column headings at the top is also a form for adding a new layer.\nFill in the top row with the following details:\n\nLayer ID: the\nType: Text\nAlignment: None (our annotations are simply tags on words, inheriting their start/end times from the word token they tag)\nManager: don’t select any manager, as we’ll be adding manual annotations, rather than automatically generated ones\nGenerate: don’t select any option (this setting is only relevant for managed layers, so it doesn’t actually matter what you select here)\nProject: the\nDescription: \"the\" followed by a word-initial vowel\n\nPress New to add the layer.\n\nNow we’re ready to find some tokens…\n\nSelect participants on the menu.\nWe’re going to search all male monolinguals, so filter by the appropriate attribute values and then click Layered Search.\nSearch for instances of the word “the” followed immediately by a word starting with a vowel, on the orthography layer.\nExport the results to a CSV file and open it.\n\nNow we’re going to annotate the CSV file to identify false positives.\n\nAdd a column to the right-hand side of the spreadsheet, called “The” - i.e. on the first line, in the cell to the right the last column header, enter the word The\n\nFor each row in the spreadsheet check the contents of the “Match transcript” column, and decide whether the match is a ‘false positive’ or not. False positives are cases like “the one”, where the second word actually starts with a non-vowel sound (i.e. the word “one” actually starts with a /w/ phoneme).\nFor false positives, enter FP in your new “The” column. For all the others, enter TP.\nSave the CSV file.\nYou may be asked if you want to change the format of the file. Resist the temptation to do this - we are going to upload this file into LaBB-CAT, and it can only understand CSV files.\n\nNow that we’ve annotated our results, we’re going to to load our annotations into the new layer we created in LaBB-CAT…\n\nIn LaBB-CAT, select the upload menu option.\nSelect the upload csv annotations option.\nPress Choose File and select the annotated CSV file you just saved.\nPress Upload.\nOn the form that appears, you can leave the default choices for the options. Just ensure that the Tag Words option is selected at the top, and at the bottom the The column in the spreadsheet is mapped to the the layer in LaBB-CAT.\nClick Insert Annotations.\nYou will see a message about how many annotations were added. Now, within LaBB-CAT, each token mentioned in your CSV file has been tagged with either “TP” or “FP”\n\nNow that we’ve seen one way to add annotations to the database, using CSV files, we will try another way - editing word annotations directly from the interactive transcript.\nWe’re going to find our ‘true positive’ tokens of the word “the”, and annotate each depending on how the speaker pronounces it in the recording.\n\nIn LaBB-CAT, select the search menu option, which by default searches utterances of all participants.\nThis time we’re going to search for the true-positive annotations we just inserted.\nUnder the “Tick layers to include” heading, there’s now a “Project” column that includes the “the” project we added at the start. Tick that project, so that the layer associated with it is displayed in the list of Word layers to the right.\nTick your custom layer (called “the”) in the Word column.\nYour search matrix is now two layers high by one word wide.\nSearch for TP on the the layer.\nThe results page should show you all the words you annotated with TP in your CSV file above.\nClick on the first match.\nThis will open the interactive transcript for the match. You’ll be able to see not only the transcript text, but also the TP/FP tags you have added.\nClick on the first match in the transcript, and select the Play option to play the line.\n\nListen carefully to see whether the speaker pronounces the word “the” like “thee” or not. If they do, we’re going to annotate the word with the code i. Otherwise we’re going to annotate it with the code @.\nClick the match word again, and select the last option on the menu: Edit. A window will appear, with a list of layers. Each line has the token’s annotation on the given row. So at the bottom, the value on the word layer is probably “the” or maybe “The” or “the .” or something similar. On the orthography layer, the annotation will be “the”. Other layers may be blank, except for your the layer, whose annotation will be “TP”.\nWe’re going to change the “TP” annotation depending on the pronunciation of the token. So replace “TP” with i or @ as appropriate.\nClick the Save button  to save your annotation to the database.\nClose the “edit word” window.\nBack in the interactive transcript, find the next match result - it will be highlighted.\nAnnotate the next match in exactly the same way - play the utterance, listen to the pronunciation, and change the TP to an appropriate code.\nSimilarly annotate the rest of the matches in the transcript.\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou may notice that, although you’re changing labels to i or @, the tags still appear as TP in the transcript page; this is simply because that’s what the tag was when you opened the transcript. If you refresh the page, you’ll see your new tags instead of the old ones.\n\n\n\nOnce you’ve annotated the last match in the transcript, close the browser’s tab.\nThis will take you back to the search results page.\nYou’ve already annotated all the matches in the first transcript, so move to the next transcript in the results list, and click the first match.\nAnnotate all the “TP” tokens in the transcript.\nRepeat the above steps until you’ve annotated all the matches.\n\nYou’ve now used two methods for annotating words. Although this is a small, toy example, you can hopefully see that you could manage a larger annotation project involving much more tokens, possibly multiple annotators, and working either offline (with a CSV file and maybe extracted WAV files) or online (directly in the interactive transcript page), as preferred.\nThere are other ways to add manual annotations, which relate to concrete points or intervals in time during the recording. We will see how to do this later…\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023-2024 NZILBB"
  },
  {
    "objectID": "worksheets/course/8a-forced-alignment-htk.html",
    "href": "worksheets/course/8a-forced-alignment-htk.html",
    "title": "8a. HTK",
    "section": "",
    "text": "The Hidden Markov Model Toolkit (HTK) is a speech recognition toolkit developed at Cambridge University. It is a set of programs that can be used to build speech recognition systems. Part of the process of building such systems involves force-aligning training data - i.e. automatically lining up phonemic-transcriptions of known words with the audio signal in the training recordings. LaBB-CAT takes advantage of this capability to facilitate forced-alignment for your transcripts.\nIn order to do this, HTK needs the following ingredients:\n\na set of recordings broken up into short utterances,\northographic transcriptions of each utterance, and\nphonemic transcriptions of each of the words in each utterance\n\nYou already have 1. and 2. - i.e. a set of recordings with transcripts that include the start and end times of each line.\nYou also mostly have 3. as well, if you have done a previous exercise which included generating a pronunciation layer generated using a lexicon like CELEX or the CMU Pronunciation Dictionary. However, there are words in your transcripts that aren’t in CELEX, and so we will explore some mechanisms for filling in their pronunciations.\nIn this exercise you will\n\ninstall the HTK Layer Manager,\nprovide some pronunciations that are missing,\nforce-align the speech of one of the participants in your database, and\ncheck and manually correct the alignments.\n\n\n\n\n\n\n\nImportant\n\n\n\nIn this exercise, you will set up Praat Integration in your web browser. There is currently no Praat integration support for Microsoft’s Edge’ browser, so if you normally use ‘Edge’ on Windows, you may need to swap to another browser for this exercise - e.g. Google Chrome, or Mozilla Firefox.\n\n\n\n\nHTK is not free software in the “GNU” sense - i.e. we can not distribute it with LaBB-CAT, instead you have to download it yourself from the Cambridge University website – however it is free in the “no cost” sense, you just need to register on the HTK website, and you can then download and use HTK free of charge.\n\nRegister at http://htk.eng.cam.ac.uk/register.shtml\nDownload the version of HTK that is appropriate for the computer that LaBB-CAT is installed on:\n\nFor Windows systems, there are pre-compiled .exe files that you can download.\nFor Unix-like systems including OS X, you need to download the source code, which you will then install following the provided instructions.\n\nUnzip (for Windows) or compile and install (for Unix-like systems) the downloaded files on the computer that LaBB-CAT is installed on.\n\nIf your LaBB-CAT server is installed in a Docker Container, LaBB-CAT can itself download and compile HTK (i.e. steps 2 and 3), as long has you have a username and password for the HTK website. In this case, these steps are automatically attempted when you install the HTK layer manager (below).\nNow, you have to install the HTK Layer Manager, which is the LaBB-CAT module that provides HTK with all the data it needs, and then saves to alignments HTK produces back to your database.\n\nSelect the layer managers menu option.\nFollow the List of layer managers that are not yet installed link at the bottom.\nFind HTK Manager in the list, and press its Install button, then press Install again.\nYou will see a form with boxes that may already be filled in.\nThe layer manager needs to know where the HTK programs have been saved, which is what you need to enter in the blank Path to HTK tools box.\nIf this box is not blank, it means that LaBB-CAT has already found HTK for you, so you should leave the default value already set.\n(The other two boxes you may see are for the username and password you registered on the HTK website. On some systems, if you have not already installed the HTK tools on your LaBB-CAT computer, providing the username and password allows LaBB-CAT to attempt to download and install HTK itself. This may not work in all cases, as LaBB-CAT may not have the resources or privileges it requires to compile or install software.)\nPress Configure.\nYou will see a window open with some information about the HTK Layer Manager. This page has some useful instructions on it, so keep the page open for now.\nNow you need to add a phrase layer for the HTK configuration:\n\nLayer ID: htk\nType: Text\nAlignment: Intervals\nManager: HTK Manager\nGenerate: Never (note that this may see counter-intuitive, but we will be running HTK manually rather than allowing it to run automatically when transcripts are uploaded.)\nDescription: HTK alignment time\n\nWhen you configure the layer:\n\nYour Pronunciation Layer will be the phonemes layer you created in an earlier exercise.\nTo the list of Pause Markers you should add a full-stop (period).\nThis is because the exercise transcripts use . as a ‘short-pause’ marker, not an ‘end of sentence’ marker. i.e. your Pause Markers should be:\n- .\n(a hyphen, then a space, then a full-stop/period)\nThe rest of the settings should be left with the default values.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you’re curious about what the configuration options do, hover your mouse over each option to see a ‘tool tip’ that describes what the option is for.\n\n\nWe configure the layer to ‘never’ generate, because we’re going to trigger forced-alignments manually, one speaker at a time, once we’re happy that the phonemic transcriptions are in place.\n\n\n\nNow we’re going to check the situation of our pronunciations on the phonemes layer more carefully…\n\nGo to the transcripts page and open the transcript UC207YW\nTick the phonemes layer.\nHave a look through the transcript to see where the missing phonemic transcriptions are.\n\nYou’ll see they divide into various broad types:\n\nTypos like “Febuary”\nSpecialist or invented words like “tarseal”\nContractions like “me’s” and “thing’s”\nProper names like “Bealey”\nPossibly filled pauses like “um”\nHesitations and interrupted words like “exac~”, etc.\n\n\nHTK needs a phonemic transcription for every word on a line in order to force-align that line. So every line where there’s a gap on the phonemes layer would be ignored by the HTK layer manager.\n\nThere’s another problem in this transcript, which isn’t necessarily immediately obvious.\nLook for the hesitiation “w-” and the filled-pause “mm” to see if you can see what it is.\n\n‘False positives’ from the lexicon will also play havoc with forced alignment, as HTK believes what it’s told about the pronunciations given to it, and will do it’s best to find an alignment that includes every phoneme.\n\nEach of these problems needs to be addressed before we do forced alignment, although the solution for each will vary. Some involve improving the transcript, others will involve adding new words to our dictionary.\n\nFor false-positives like “w-” and “mm”, the easiest solution is to transcribe these differently. Hesitations like “w-” are discussed below. We will use “mmm” instead of “mm”.\nFor very short false-starts like “w-”, the CELEX layer manager has been built to give a helping hand. In addition to looking up phonemic transcriptions in CELEX, it will also compute them for very short tagged false-starts. The tag it recognizes is a trailing tilde ~, so we need to change “w-” to “w~” etc. Then the CELEX layer manager will append a schwa to the initial consonant, and save that as the pronunciation (i.e. /ə/).\nFor invented or misspoken words, or longer interrupted words, which we’re not likely to see again in any other transcript like, “me’s” and “exac~”, we will add a ‘pronounce’ tag in the transcript, which includes the correct pronunciation. Again, the CELEX layer manager knows to check for pronounce annotations, and uses the given phonemic transcription instead of looking up the CELEX data.\nFor proper names and contractions like “thing’s” that we’re likely to see over and over again in different transcripts, instead of tagging each one individually, we will add them to the dictionary of pronunciations that the lexicon layer manager looks up.\n\nAs you can see, the first three methods involve editing the transcript. This can be done by editing the original file in ELAN, and then re-uploading it into the database for processing.\nAlternatively, LaBB-CAT has a mechanism for editing the transcript ‘in-situ’; this doesn’t update the original file, but it’s sometimes much more convenient, and this is the method we’ll use for this exercise.\n\nClick on the word “Febuary”, and select the Edit Transcript option from the menu.\nA window will open that allows you to edit that line in the transcript.\nCorrect the spelling of the word to be “February”\nPress Update\nPress Close\nSimilarly change “w-” to be “w~” and “mm” to be “mmm”\nClick on the word “me’s”, and select the Edit Transcript option from the menu.\nIt seems unlikely that anyone else will say “me’s”, so instead of adding it to the lexicon, we’re simply going to tag this token with a pronounce tag. This is achieved by adding the pronunciation we want to tag it with in square brackets, using the DISC phoneme symbols. We have to add the pronounce tag immediately after the word, with no intervening space. (This transcription convention also works if you edit the original transcript in ELAN)\nChange the line text from “...bit of me’s a bit …” to be “…bit of me’s[miz] a bit …” instead.\nClick Update and then Close\nThe pronounce annotation you’ve just added isn’t displayed in the transcript. It’s added to the pronounce layer, which is for this type of manual pronunciation tagging.\nScroll to the top of the transcript and tick the pronounce layer so that it will be displayed.\nWhen the transcript is reloaded, you will be able to see “miz” as an annotation on “me’s”, and that this has been copied into the phonemes layer by its layer manager.\nSimilarly you should tag the word “exac~” with the pronunciation Igz{k\n\nThis method takes care of instances where the transcript is incorrect, and ‘one off’ missing pronunciations. However, for missing words that are likely to appear over and again in the corpus, including names like “Bealey”, “Wainoni”, and “Lyttleton”, and filled pauses like “mmm”, “um”, etc. it’s not efficient to tag every token. Instead, we add these to the lexicon.\n\nSelect the participants menu option.\nFind UC207YW in the list, and click on their name.\nClick the All Utterances link below their attributes.\nLeave the default selections and press List.\nThis displays a page with a list of all the speaker’s utterances, from which you can do various things with all the utterances of a particular participant in the database.\nPress the Htk button.\nYou will see a progress bar while LaBB-CAT identifies missing words. Then a page will appear that lists unknown words.\nBasically you need to fill in the boxes with the pronunciations and click Save Pronunciations.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nYou don’t have to fill them all in at once, you can do a few, and click Save, which will save your work and list what’s left.\nYou don’t have to fill them all in, you can leave some empty and continue with the HTK forced-alignment by clicking Start (HTK will ignore any lines where the remaining unknown words appear, but the ones you filled in will be included).\nSome of the boxes will be initially filled in with a suggestion from the lexicon layer manager - these may or may not be correct, and aren’t saved until you save them.\nThe pronunciations have to be in the ‘DISC’ format - i.e. one character per phoneme, with no spaces. There’s a ‘helper’ link on the right of each pronunciation box - if you click it, it expands into a list of clickable phonemes - just the ones that aren’t ordinary letters, and diphthongs etc.\nThe search button lets you look up the lexicon for similar words - this probably won’t help for place names, but for words like “tarseal”, you can click the lookup button, enter “tar seal” in the box as two separate words, and you’ll get back the DISC pronunciation of each word, with clickable buttons to copy the given pronunciation into the box. This is useful for digits and numbers too, which may not be in the lexicon - so for “1”, search for “one” and copy the pronunciation.\nIf you click on the word itself, the transcript for the first instance of that word is opened, in case you want to listen to it, or in case it’s actually just a typo and you want to correct the transcript.\nIf you’re using CELEX, when you specify the pronunciations, it’s recommended to put syllable separators (-) and primary stress markers (’) too - e.g. for “tarseal” you can put t#sil but it would actually be better to put t#-’sil. These markers are entered into the dictionary even though they’re stripped out for HTK, and they may come in handy later (e.g. the syllable separators are used by the CELEX layer manager to count syllables).\n\n\n\nWhen you add pronunciations this way, they’re added to the dictionary and all the instances of those words in LaBB-CAT are updated with the pronunciations - not just the participant you’re looking at, but all participants in the database. So you only have to come up with a pronunciation for each word once.\n\n\n\n\nOnce you’ve filled in all the missing pronunciations, forced alignment will start automatically. If you want to start forced alignment before you’ve entered all pronunciations, click the Start button at the bottom of the page.\n\nYou should see a progress bar while the forced alignment is running. It will take a few minutes to complete.\n\nOnce HTK has produced the word and segment alignments, it:\n\nsets the start/end times of the words on the transcript layer accordingly,\nadds new phone annotations to the segments layer with the alignments of the phones, and\nsaves a timestamp in the htk layer.\n\nWhen the layer manager has finished, you’ll see a message saying “Complete - words and phones from selected utterances are now aligned.”\n\n\n\nYou can inspect/correct alignments using LaBB-CAT’s integration with Praat.\n\nGo back to the UC207YW.eaf transcript.\nTick both the htk layer and the segments layer.\nYou will see which lines have been force-aligned, as they have an HTK timestamp, and have the segments layer filled in. If it has missed some lines, this is most likely because there is an unknown word, another speaker speaking at the same time, or possibly HTK simply failed to align the line (there are various reasons this happens, including not enough data for training, noisy recordings, inaccurate transcription, etc.).\n\nThe interactive transcript page doesn’t show you the alignments of the words or phones, but you can see those using Praat. You can open individual utterances in Praat directly from the transcript page, but first, the LaBB-CAT/Praat integration has to be set up; this only has to be done once:\n\nOn the top-right of the page, above the playback controls, there’s a Praat icon  - click it.\nFollow the instructions that appear (these vary depending on what web browser you use).\nYou may be asked whether to allow the “LaBB-CAT Integration Applet” to run. If you tick the “Do not show this again” option, then this message will not appear every time you open a transcript.\nYou may need to grant a browser extension permission to install, and it’s possible you will need a connection to the internet in order to download this extension.\nYou also may be asked where Praat is installed; Navigate to the location where Praat is installed, and double-click the “Praat.exe” file (on some systems the file may simply be called “Praat”). The Praat program may open, and then immediately close, as LaBB-CAT tests it can communicate with Praat.\n\nNow Praat integration has been set up, and you should be able to access Praat options in the transcript page from now on…\n\nClick on a line that has been aligned, and select the Open Text Grid in Praat option on the menu.\nYou may be asked you if want to allow access to the “LaBB-CAT Integration Applet” - if so, tick “Do not show this again”, and click Allow.\n\nPraat should open, and show you a spectrogram of the line’s audio, with a TextGrid below that includes the words and the segments.\n\nIf you click on a word, and hit the tab key, the word’s interval is played. Try out various words, and see what you think about how accurate HTK has been with its alignment.\nTry this out with different lines in the transcript.\nYou will see that in some cases the alignment is pretty good, and in other cases, it’s not so good. In the not-so-good cases, see if you can figure out why HTK got it wrong.\n\nYou may have noticed that, each time you open an utterance in Praat, a button appears in the transcript to the left of the line, labelled Import Changes. This button allows you to save any adjustments you might want to make to the alignments back into the LaBB-CAT database.\n\nIf you feel confident using Praat, open an utterance TextGrid, adjust the alignments of the words an phones so that they’re more accurate, and then click the Import Changes button in the transcript.\n\n\n\n\n\n\n\nWarning\n\n\n\nThese changes are flagged as manual edits, so if forced-alignment is run again, they will not be over-written with new bad alignments. Therefore it’s important that the changes you make are actually improvements, because HTK will never change them again.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThere are some rules about what you can change:\n\nYou’re not allowed to add or delete words (if this is necessary, it should be done by correcting the transcript instead).\nAll the phones must be within the bounds of their own word.\nThe start of the first phone should line up with the start of the word, and the end of the last phone should line up with the end of the word.\nYou should not change the alignment of the utterance itself (which would only be possible if you select the Open Text Grid incl. ± 1 utterance in Praat option).\n\n\n\nIn this exercise, you have seen how HTK can be used to compute word and phone alignments automatically from your data, but that there is a fair amount of careful transcription, tagging, and dictionary filling required. Even after all that work, perfect automatic alignments are not guaranteed, but LaBB-CAT has a mechanism for manually correcting poor alignments.\nAll this manual annotation and correction means a lot of work, but obviously somewhat less work than would be involved in aligning by hand the transcripts from scratch!"
  },
  {
    "objectID": "worksheets/course/8a-forced-alignment-htk.html#installation",
    "href": "worksheets/course/8a-forced-alignment-htk.html#installation",
    "title": "8a. HTK",
    "section": "",
    "text": "HTK is not free software in the “GNU” sense - i.e. we can not distribute it with LaBB-CAT, instead you have to download it yourself from the Cambridge University website – however it is free in the “no cost” sense, you just need to register on the HTK website, and you can then download and use HTK free of charge.\n\nRegister at http://htk.eng.cam.ac.uk/register.shtml\nDownload the version of HTK that is appropriate for the computer that LaBB-CAT is installed on:\n\nFor Windows systems, there are pre-compiled .exe files that you can download.\nFor Unix-like systems including OS X, you need to download the source code, which you will then install following the provided instructions.\n\nUnzip (for Windows) or compile and install (for Unix-like systems) the downloaded files on the computer that LaBB-CAT is installed on.\n\nIf your LaBB-CAT server is installed in a Docker Container, LaBB-CAT can itself download and compile HTK (i.e. steps 2 and 3), as long has you have a username and password for the HTK website. In this case, these steps are automatically attempted when you install the HTK layer manager (below).\nNow, you have to install the HTK Layer Manager, which is the LaBB-CAT module that provides HTK with all the data it needs, and then saves to alignments HTK produces back to your database.\n\nSelect the layer managers menu option.\nFollow the List of layer managers that are not yet installed link at the bottom.\nFind HTK Manager in the list, and press its Install button, then press Install again.\nYou will see a form with boxes that may already be filled in.\nThe layer manager needs to know where the HTK programs have been saved, which is what you need to enter in the blank Path to HTK tools box.\nIf this box is not blank, it means that LaBB-CAT has already found HTK for you, so you should leave the default value already set.\n(The other two boxes you may see are for the username and password you registered on the HTK website. On some systems, if you have not already installed the HTK tools on your LaBB-CAT computer, providing the username and password allows LaBB-CAT to attempt to download and install HTK itself. This may not work in all cases, as LaBB-CAT may not have the resources or privileges it requires to compile or install software.)\nPress Configure.\nYou will see a window open with some information about the HTK Layer Manager. This page has some useful instructions on it, so keep the page open for now.\nNow you need to add a phrase layer for the HTK configuration:\n\nLayer ID: htk\nType: Text\nAlignment: Intervals\nManager: HTK Manager\nGenerate: Never (note that this may see counter-intuitive, but we will be running HTK manually rather than allowing it to run automatically when transcripts are uploaded.)\nDescription: HTK alignment time\n\nWhen you configure the layer:\n\nYour Pronunciation Layer will be the phonemes layer you created in an earlier exercise.\nTo the list of Pause Markers you should add a full-stop (period).\nThis is because the exercise transcripts use . as a ‘short-pause’ marker, not an ‘end of sentence’ marker. i.e. your Pause Markers should be:\n- .\n(a hyphen, then a space, then a full-stop/period)\nThe rest of the settings should be left with the default values.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you’re curious about what the configuration options do, hover your mouse over each option to see a ‘tool tip’ that describes what the option is for.\n\n\nWe configure the layer to ‘never’ generate, because we’re going to trigger forced-alignments manually, one speaker at a time, once we’re happy that the phonemic transcriptions are in place."
  },
  {
    "objectID": "worksheets/course/8a-forced-alignment-htk.html#missing-pronunciations",
    "href": "worksheets/course/8a-forced-alignment-htk.html#missing-pronunciations",
    "title": "8a. HTK",
    "section": "",
    "text": "Now we’re going to check the situation of our pronunciations on the phonemes layer more carefully…\n\nGo to the transcripts page and open the transcript UC207YW\nTick the phonemes layer.\nHave a look through the transcript to see where the missing phonemic transcriptions are.\n\nYou’ll see they divide into various broad types:\n\nTypos like “Febuary”\nSpecialist or invented words like “tarseal”\nContractions like “me’s” and “thing’s”\nProper names like “Bealey”\nPossibly filled pauses like “um”\nHesitations and interrupted words like “exac~”, etc.\n\n\nHTK needs a phonemic transcription for every word on a line in order to force-align that line. So every line where there’s a gap on the phonemes layer would be ignored by the HTK layer manager.\n\nThere’s another problem in this transcript, which isn’t necessarily immediately obvious.\nLook for the hesitiation “w-” and the filled-pause “mm” to see if you can see what it is.\n\n‘False positives’ from the lexicon will also play havoc with forced alignment, as HTK believes what it’s told about the pronunciations given to it, and will do it’s best to find an alignment that includes every phoneme.\n\nEach of these problems needs to be addressed before we do forced alignment, although the solution for each will vary. Some involve improving the transcript, others will involve adding new words to our dictionary.\n\nFor false-positives like “w-” and “mm”, the easiest solution is to transcribe these differently. Hesitations like “w-” are discussed below. We will use “mmm” instead of “mm”.\nFor very short false-starts like “w-”, the CELEX layer manager has been built to give a helping hand. In addition to looking up phonemic transcriptions in CELEX, it will also compute them for very short tagged false-starts. The tag it recognizes is a trailing tilde ~, so we need to change “w-” to “w~” etc. Then the CELEX layer manager will append a schwa to the initial consonant, and save that as the pronunciation (i.e. /ə/).\nFor invented or misspoken words, or longer interrupted words, which we’re not likely to see again in any other transcript like, “me’s” and “exac~”, we will add a ‘pronounce’ tag in the transcript, which includes the correct pronunciation. Again, the CELEX layer manager knows to check for pronounce annotations, and uses the given phonemic transcription instead of looking up the CELEX data.\nFor proper names and contractions like “thing’s” that we’re likely to see over and over again in different transcripts, instead of tagging each one individually, we will add them to the dictionary of pronunciations that the lexicon layer manager looks up.\n\nAs you can see, the first three methods involve editing the transcript. This can be done by editing the original file in ELAN, and then re-uploading it into the database for processing.\nAlternatively, LaBB-CAT has a mechanism for editing the transcript ‘in-situ’; this doesn’t update the original file, but it’s sometimes much more convenient, and this is the method we’ll use for this exercise.\n\nClick on the word “Febuary”, and select the Edit Transcript option from the menu.\nA window will open that allows you to edit that line in the transcript.\nCorrect the spelling of the word to be “February”\nPress Update\nPress Close\nSimilarly change “w-” to be “w~” and “mm” to be “mmm”\nClick on the word “me’s”, and select the Edit Transcript option from the menu.\nIt seems unlikely that anyone else will say “me’s”, so instead of adding it to the lexicon, we’re simply going to tag this token with a pronounce tag. This is achieved by adding the pronunciation we want to tag it with in square brackets, using the DISC phoneme symbols. We have to add the pronounce tag immediately after the word, with no intervening space. (This transcription convention also works if you edit the original transcript in ELAN)\nChange the line text from “...bit of me’s a bit …” to be “…bit of me’s[miz] a bit …” instead.\nClick Update and then Close\nThe pronounce annotation you’ve just added isn’t displayed in the transcript. It’s added to the pronounce layer, which is for this type of manual pronunciation tagging.\nScroll to the top of the transcript and tick the pronounce layer so that it will be displayed.\nWhen the transcript is reloaded, you will be able to see “miz” as an annotation on “me’s”, and that this has been copied into the phonemes layer by its layer manager.\nSimilarly you should tag the word “exac~” with the pronunciation Igz{k\n\nThis method takes care of instances where the transcript is incorrect, and ‘one off’ missing pronunciations. However, for missing words that are likely to appear over and again in the corpus, including names like “Bealey”, “Wainoni”, and “Lyttleton”, and filled pauses like “mmm”, “um”, etc. it’s not efficient to tag every token. Instead, we add these to the lexicon.\n\nSelect the participants menu option.\nFind UC207YW in the list, and click on their name.\nClick the All Utterances link below their attributes.\nLeave the default selections and press List.\nThis displays a page with a list of all the speaker’s utterances, from which you can do various things with all the utterances of a particular participant in the database.\nPress the Htk button.\nYou will see a progress bar while LaBB-CAT identifies missing words. Then a page will appear that lists unknown words.\nBasically you need to fill in the boxes with the pronunciations and click Save Pronunciations.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nYou don’t have to fill them all in at once, you can do a few, and click Save, which will save your work and list what’s left.\nYou don’t have to fill them all in, you can leave some empty and continue with the HTK forced-alignment by clicking Start (HTK will ignore any lines where the remaining unknown words appear, but the ones you filled in will be included).\nSome of the boxes will be initially filled in with a suggestion from the lexicon layer manager - these may or may not be correct, and aren’t saved until you save them.\nThe pronunciations have to be in the ‘DISC’ format - i.e. one character per phoneme, with no spaces. There’s a ‘helper’ link on the right of each pronunciation box - if you click it, it expands into a list of clickable phonemes - just the ones that aren’t ordinary letters, and diphthongs etc.\nThe search button lets you look up the lexicon for similar words - this probably won’t help for place names, but for words like “tarseal”, you can click the lookup button, enter “tar seal” in the box as two separate words, and you’ll get back the DISC pronunciation of each word, with clickable buttons to copy the given pronunciation into the box. This is useful for digits and numbers too, which may not be in the lexicon - so for “1”, search for “one” and copy the pronunciation.\nIf you click on the word itself, the transcript for the first instance of that word is opened, in case you want to listen to it, or in case it’s actually just a typo and you want to correct the transcript.\nIf you’re using CELEX, when you specify the pronunciations, it’s recommended to put syllable separators (-) and primary stress markers (’) too - e.g. for “tarseal” you can put t#sil but it would actually be better to put t#-’sil. These markers are entered into the dictionary even though they’re stripped out for HTK, and they may come in handy later (e.g. the syllable separators are used by the CELEX layer manager to count syllables).\n\n\n\nWhen you add pronunciations this way, they’re added to the dictionary and all the instances of those words in LaBB-CAT are updated with the pronunciations - not just the participant you’re looking at, but all participants in the database. So you only have to come up with a pronunciation for each word once."
  },
  {
    "objectID": "worksheets/course/8a-forced-alignment-htk.html#alignment",
    "href": "worksheets/course/8a-forced-alignment-htk.html#alignment",
    "title": "8a. HTK",
    "section": "",
    "text": "Once you’ve filled in all the missing pronunciations, forced alignment will start automatically. If you want to start forced alignment before you’ve entered all pronunciations, click the Start button at the bottom of the page.\n\nYou should see a progress bar while the forced alignment is running. It will take a few minutes to complete.\n\nOnce HTK has produced the word and segment alignments, it:\n\nsets the start/end times of the words on the transcript layer accordingly,\nadds new phone annotations to the segments layer with the alignments of the phones, and\nsaves a timestamp in the htk layer.\n\nWhen the layer manager has finished, you’ll see a message saying “Complete - words and phones from selected utterances are now aligned.”"
  },
  {
    "objectID": "worksheets/course/8a-forced-alignment-htk.html#inspectioncorrection",
    "href": "worksheets/course/8a-forced-alignment-htk.html#inspectioncorrection",
    "title": "8a. HTK",
    "section": "",
    "text": "You can inspect/correct alignments using LaBB-CAT’s integration with Praat.\n\nGo back to the UC207YW.eaf transcript.\nTick both the htk layer and the segments layer.\nYou will see which lines have been force-aligned, as they have an HTK timestamp, and have the segments layer filled in. If it has missed some lines, this is most likely because there is an unknown word, another speaker speaking at the same time, or possibly HTK simply failed to align the line (there are various reasons this happens, including not enough data for training, noisy recordings, inaccurate transcription, etc.).\n\nThe interactive transcript page doesn’t show you the alignments of the words or phones, but you can see those using Praat. You can open individual utterances in Praat directly from the transcript page, but first, the LaBB-CAT/Praat integration has to be set up; this only has to be done once:\n\nOn the top-right of the page, above the playback controls, there’s a Praat icon  - click it.\nFollow the instructions that appear (these vary depending on what web browser you use).\nYou may be asked whether to allow the “LaBB-CAT Integration Applet” to run. If you tick the “Do not show this again” option, then this message will not appear every time you open a transcript.\nYou may need to grant a browser extension permission to install, and it’s possible you will need a connection to the internet in order to download this extension.\nYou also may be asked where Praat is installed; Navigate to the location where Praat is installed, and double-click the “Praat.exe” file (on some systems the file may simply be called “Praat”). The Praat program may open, and then immediately close, as LaBB-CAT tests it can communicate with Praat.\n\nNow Praat integration has been set up, and you should be able to access Praat options in the transcript page from now on…\n\nClick on a line that has been aligned, and select the Open Text Grid in Praat option on the menu.\nYou may be asked you if want to allow access to the “LaBB-CAT Integration Applet” - if so, tick “Do not show this again”, and click Allow.\n\nPraat should open, and show you a spectrogram of the line’s audio, with a TextGrid below that includes the words and the segments.\n\nIf you click on a word, and hit the tab key, the word’s interval is played. Try out various words, and see what you think about how accurate HTK has been with its alignment.\nTry this out with different lines in the transcript.\nYou will see that in some cases the alignment is pretty good, and in other cases, it’s not so good. In the not-so-good cases, see if you can figure out why HTK got it wrong.\n\nYou may have noticed that, each time you open an utterance in Praat, a button appears in the transcript to the left of the line, labelled Import Changes. This button allows you to save any adjustments you might want to make to the alignments back into the LaBB-CAT database.\n\nIf you feel confident using Praat, open an utterance TextGrid, adjust the alignments of the words an phones so that they’re more accurate, and then click the Import Changes button in the transcript.\n\n\n\n\n\n\n\nWarning\n\n\n\nThese changes are flagged as manual edits, so if forced-alignment is run again, they will not be over-written with new bad alignments. Therefore it’s important that the changes you make are actually improvements, because HTK will never change them again.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThere are some rules about what you can change:\n\nYou’re not allowed to add or delete words (if this is necessary, it should be done by correcting the transcript instead).\nAll the phones must be within the bounds of their own word.\nThe start of the first phone should line up with the start of the word, and the end of the last phone should line up with the end of the word.\nYou should not change the alignment of the utterance itself (which would only be possible if you select the Open Text Grid incl. ± 1 utterance in Praat option).\n\n\n\nIn this exercise, you have seen how HTK can be used to compute word and phone alignments automatically from your data, but that there is a fair amount of careful transcription, tagging, and dictionary filling required. Even after all that work, perfect automatic alignments are not guaranteed, but LaBB-CAT has a mechanism for manually correcting poor alignments.\nAll this manual annotation and correction means a lot of work, but obviously somewhat less work than would be involved in aligning by hand the transcripts from scratch!"
  },
  {
    "objectID": "worksheets/course/2-setting-up.html",
    "href": "worksheets/course/2-setting-up.html",
    "title": "LaBB-CAT Documentation",
    "section": "",
    "text": "In this exercise you will:\n\nDefine corpora\nDefine transcript types\nDefine speaker meta data\n\nAfter this you will have an empty LaBB-CAT database set up ready to upload transcripts into.\nNow that the software is installed, we will set up a basic structure for receiving data:\n\nOpen LaBB-CAT.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you will be running LaBB-CAT on your own computer, you will need to start LaBB-CAT, and this will open your browser on the LaBB-CAT start page.\nIf you are using a LaBB-CAT server that’s already been installed for you elsewhere, you will have been given a link or URL to use; open your browser on that page now.\n\n\n\nThe start page has a link on it called Where do I start? - you may like to click on this link and read the first section, which explains a little about how to navigate around LaBB-CAT and where to find online help and hints. \nClick back on the start page of LaBB-CAT (the page with the Where do I start? link).\n\nNow we will set up some corpus names…\n\nOn the menu at the top, click the corpora link.\nThis page shows a list of current corpora, which only contains one corpus, called corpus.\nAbove the corpus corpus, there’s a form that you can fill in to add a new corpus. Fill in the following information:\n\nName : QB\nLanguage : English\nDescription : Quakebox recordings\n\nClick the New button to add the “QB” corpus.\nYou should see a message at the top of the page saying Record created and now the QB corpus is in the list, under the “corpus” corpus.\nAdd another corpus called UC with the description Campus recordings.\nWe won’t actually be using the corpus called corpus, so we want to delete it. To do this, click the Delete button to the right of the corpus corpus in the list.\nYou will be asked Are you sure you want to delete corpus? You are sure, so click OK.\nThe row will be deleted from the list.\n\nNow you have some corpora set up with the names you’ve provided.\nThe data we are using is a collection of stories about peoples’ experiences during the devastating earthquakes that hit the Canterbury region of New Zealand in 2010 and 2011. Some recordings are interviews, where an interviewer asks the participant questions, and others are monologues. Now we’re going to set up these two transcript types…\n\nClick on the transcript types menu option.\nYou will see a list of transcript types, although there’s currently only one type in the list, called interview.\nAbove this, fill in the empty Type box with the word: monologue\nClick the New button.\nYou will notice that now the list has two transcript types, interview and monologue. A Save button has appeared, because your changes aren’t yet saved to LaBB-CAT.\nClick the Save button.\nYou will see a message at the top saying Layer saved: transcript_type.\n\n\nNow that we have both corpora and transcript types, we’re going to set up meta-data options for the participants in our corpus…\n\nClick on the participant attributes menu option.\nYou will see a list of fields or attributes that participants (or speakers) can have. There are currently only three attributes:\n\nGender - i.e. whether the participant is female or male or something else \nBirth Year - i.e. the year the participant was born \nNotes - general arbitrary notes\n\n\nFor our corpus data, we don’t have the exact year of birth. Instead we have the age of the participant, defined by various age group categories.\n\nAs with previous pages, the headings at the top are also a form you can fill in to add a new row, which we will now fill in with the following information:\n\nAttribute ID : age_category\nType : Select (this is because we want to be able to select from a list of possible values) \nLayer Label : Age\nAccess : Public\nSearchability : Searchable (so that it appears on the search page)\nCategory : General\nDescription : Age Category\n\nPress the New button to add the attribute.\nYou will see a message saying Layer added: participant_age_category and the new attribute will now appear at the bottom of the list. Now we need to define the options for it…\n\n\n\n\n\n\n\nTip\n\n\n\n If you want more information about what each of these are for, check the online help for this page.\n\n\n\nTo the right of the Age attribute’s Category there’s an icon like a tag 🏷.\nHover your mouse over this icon to see what it does.\nPress the Valid labels icon.\nThis shows a (currently empty) list of options for the participant_age_category attribute.\nIn the blank Label box, enter: 18-25\nPress the New button to add the option.\nYou will see that “18-25” appears twice on the row that’s added:\n\nOn the left is the value that is saved in LaBB-CAT’s database.\nOn the right is an editable description that is displayed in various places in LaBB-CAT, which can be used to provide a little further explanation about the value.\n\nChange the Description to 18-25 years.\nIn the Label row at the top, enter: 26-35\nPress the New button to add the option.\nChange the Description to 26-35 years\nSimilarly, add the following age categories:\n\n36-45\n46-55\n56-65\n66-75\n76-85\n\nAdd a final option:\n\nLabel : 85+\nDescription : 85 years or more\n\nLastly, press the New button without filling in a Label to add a ‘default’ option for participants with missing data.\nPress the Save to save all your changes to LaBB-CAT.\nYou will see a message saying Layer saved: participant_age_category\nNow select the participant attributes option on the menu to return the list of all attributes.\n\n\nWe are going to add a few more attributes, but they will be ‘free text’ fields without predefined options.\n\nAdd another attribute, called ethnicity. For ‘Type’ select String, and make it Public and Searchable.\nSimilarly, add the following Public Searchable string attribute:\n\nlanguages_spoken - a list of languages they speak\n… and the following Not Searchable attributes:\ngrew_up - what country they grew up in\ngrew_up_region - what region of New Zealand they grew up in\ngrew_up_town - what town or city they grew up in\n\nLastly, as we will not be using it, delete the Birth Year attribute.\n\nNow you have an empty database for which you’ve:\n\ncreated two corpora, QB and UC,\ncreated a new transcript type, so that we can have monologues as well as interviews, and\ncreated some new attributes for participants, so we can record the ages of our speakers, their place of origin, and the languages they speak, in addition to their genders."
  },
  {
    "objectID": "worksheets/course/2-setting-up.html#setting-up",
    "href": "worksheets/course/2-setting-up.html#setting-up",
    "title": "LaBB-CAT Documentation",
    "section": "",
    "text": "In this exercise you will:\n\nDefine corpora\nDefine transcript types\nDefine speaker meta data\n\nAfter this you will have an empty LaBB-CAT database set up ready to upload transcripts into.\nNow that the software is installed, we will set up a basic structure for receiving data:\n\nOpen LaBB-CAT.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you will be running LaBB-CAT on your own computer, you will need to start LaBB-CAT, and this will open your browser on the LaBB-CAT start page.\nIf you are using a LaBB-CAT server that’s already been installed for you elsewhere, you will have been given a link or URL to use; open your browser on that page now.\n\n\n\nThe start page has a link on it called Where do I start? - you may like to click on this link and read the first section, which explains a little about how to navigate around LaBB-CAT and where to find online help and hints. \nClick back on the start page of LaBB-CAT (the page with the Where do I start? link).\n\nNow we will set up some corpus names…\n\nOn the menu at the top, click the corpora link.\nThis page shows a list of current corpora, which only contains one corpus, called corpus.\nAbove the corpus corpus, there’s a form that you can fill in to add a new corpus. Fill in the following information:\n\nName : QB\nLanguage : English\nDescription : Quakebox recordings\n\nClick the New button to add the “QB” corpus.\nYou should see a message at the top of the page saying Record created and now the QB corpus is in the list, under the “corpus” corpus.\nAdd another corpus called UC with the description Campus recordings.\nWe won’t actually be using the corpus called corpus, so we want to delete it. To do this, click the Delete button to the right of the corpus corpus in the list.\nYou will be asked Are you sure you want to delete corpus? You are sure, so click OK.\nThe row will be deleted from the list.\n\nNow you have some corpora set up with the names you’ve provided.\nThe data we are using is a collection of stories about peoples’ experiences during the devastating earthquakes that hit the Canterbury region of New Zealand in 2010 and 2011. Some recordings are interviews, where an interviewer asks the participant questions, and others are monologues. Now we’re going to set up these two transcript types…\n\nClick on the transcript types menu option.\nYou will see a list of transcript types, although there’s currently only one type in the list, called interview.\nAbove this, fill in the empty Type box with the word: monologue\nClick the New button.\nYou will notice that now the list has two transcript types, interview and monologue. A Save button has appeared, because your changes aren’t yet saved to LaBB-CAT.\nClick the Save button.\nYou will see a message at the top saying Layer saved: transcript_type.\n\n\nNow that we have both corpora and transcript types, we’re going to set up meta-data options for the participants in our corpus…\n\nClick on the participant attributes menu option.\nYou will see a list of fields or attributes that participants (or speakers) can have. There are currently only three attributes:\n\nGender - i.e. whether the participant is female or male or something else \nBirth Year - i.e. the year the participant was born \nNotes - general arbitrary notes\n\n\nFor our corpus data, we don’t have the exact year of birth. Instead we have the age of the participant, defined by various age group categories.\n\nAs with previous pages, the headings at the top are also a form you can fill in to add a new row, which we will now fill in with the following information:\n\nAttribute ID : age_category\nType : Select (this is because we want to be able to select from a list of possible values) \nLayer Label : Age\nAccess : Public\nSearchability : Searchable (so that it appears on the search page)\nCategory : General\nDescription : Age Category\n\nPress the New button to add the attribute.\nYou will see a message saying Layer added: participant_age_category and the new attribute will now appear at the bottom of the list. Now we need to define the options for it…\n\n\n\n\n\n\n\nTip\n\n\n\n If you want more information about what each of these are for, check the online help for this page.\n\n\n\nTo the right of the Age attribute’s Category there’s an icon like a tag 🏷.\nHover your mouse over this icon to see what it does.\nPress the Valid labels icon.\nThis shows a (currently empty) list of options for the participant_age_category attribute.\nIn the blank Label box, enter: 18-25\nPress the New button to add the option.\nYou will see that “18-25” appears twice on the row that’s added:\n\nOn the left is the value that is saved in LaBB-CAT’s database.\nOn the right is an editable description that is displayed in various places in LaBB-CAT, which can be used to provide a little further explanation about the value.\n\nChange the Description to 18-25 years.\nIn the Label row at the top, enter: 26-35\nPress the New button to add the option.\nChange the Description to 26-35 years\nSimilarly, add the following age categories:\n\n36-45\n46-55\n56-65\n66-75\n76-85\n\nAdd a final option:\n\nLabel : 85+\nDescription : 85 years or more\n\nLastly, press the New button without filling in a Label to add a ‘default’ option for participants with missing data.\nPress the Save to save all your changes to LaBB-CAT.\nYou will see a message saying Layer saved: participant_age_category\nNow select the participant attributes option on the menu to return the list of all attributes.\n\n\nWe are going to add a few more attributes, but they will be ‘free text’ fields without predefined options.\n\nAdd another attribute, called ethnicity. For ‘Type’ select String, and make it Public and Searchable.\nSimilarly, add the following Public Searchable string attribute:\n\nlanguages_spoken - a list of languages they speak\n… and the following Not Searchable attributes:\ngrew_up - what country they grew up in\ngrew_up_region - what region of New Zealand they grew up in\ngrew_up_town - what town or city they grew up in\n\nLastly, as we will not be using it, delete the Birth Year attribute.\n\nNow you have an empty database for which you’ve:\n\ncreated two corpora, QB and UC,\ncreated a new transcript type, so that we can have monologues as well as interviews, and\ncreated some new attributes for participants, so we can record the ages of our speakers, their place of origin, and the languages they speak, in addition to their genders."
  },
  {
    "objectID": "worksheets/index.html",
    "href": "worksheets/index.html",
    "title": "Worksheets",
    "section": "",
    "text": "Worksheets\nHere you will find various learning resources for LaBB-CAT, including:\n\na course for learning how to use LaBB-CAT, from scratch (3x2-hour sessions)\na series of worksheets for exploring the capabilities of LaBB-CAT using a demo corpus (one 2-hour session)\na brief hands-on introduction setting up a LaBB-CAT corpus from scratch, then annotating, and exploring it (one 1-hour session)\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023-2024 NZILBB"
  },
  {
    "objectID": "worksheets/express-tutorial/4-cmudict-and-cross-layer-search.html",
    "href": "worksheets/express-tutorial/4-cmudict-and-cross-layer-search.html",
    "title": "4 - The CMU Dictionary and Cross Layer Searching",
    "section": "",
    "text": "LaBB-CAT can be integrated with the CMU Pronouncing Dictionary, which is a free pronounciation dictionary of English maintained by the Speech Group in the School of Computer Science at Carnegie Mellon University. The pronunciations are based on American English, so are suitable for American English recordings.\nIt can also serve as a free alternative to the CELEX lexicon (which is based on British English), for those that have not purchased CELEX, although is less ideal for ‘non-rhotic’ varieties of English.\nIn this exercise you will:\n\ninstall the CMU Pronouncing Dictionary layer manager,\nuse it to create new annotations for word pronunciations, and \nincorporate the new layers in more sophisticated searches.\n\n\n\nThe first thing we’re going to to is install the CMU Pronouncing Dictionary layer manager…\n\nSelect the layer managers menu option.\nYou will see a list of pre-installed layer managers, which are modules that can perform automatic annotation tasks. The CMU Pronouncing Dictionary layer manager isn’t pre-installed, because it is language-specific.\nFollow the List of layer managers that are not yet installed link near the bottom.\nFind CMU Pronouncing Dictionary in the list, and press its Install button.\nPress Install on the resulting information page.\nThis displays some further information about the layer manager, allowing you to upload an alternative version of the dictionary file.\nWe be using the standard file that is included with the layer manager.\nPress Configure.\nYou will see a progress bar while the layer manager loads the data from the dictionary file into the LaBB-CAT database. This will take a minute or so.\nOnce it’s finished, you will see a new window open with information about the CMU Pronouncing Dictionary layer manager.\n\n\n\n\nNow that we’ve installed the layer manager, we’ll create a layer that contains word pronunciations.\n\nSelect the word layers option on the menu.\nYou will see a list of existing word layers, including the orthography layer, the lexical layer, etc.\nThe column headings are also a form for defining a new word layer. Fill in the following details in this form:\n\nLayer ID: phonemes\nType: Phonological\nAlignment: None\nManager: CMU Pronouncing Dictionary\nDescription: All possible phonemic transcriptions for each word.\n\nPress New to add the layer.\nYou will see the layer configuration form.\nSet the Encoding field to CELEX DISC, and the default values for everything else.\n\n\n\n\n\n\n\nTip\n\n\n\n If you’re curious about what the configuration options do, hover your mouse over each one to see further information about what the setting does.\n\n\n\nPress Set Parameters.\nYou will see a message asking you if you want (re)generate the layer data now.\nPress Regenerate.\nYou will see a progress bar moving across the page while the annotations are being generated. When it is finished, you will see a message saying Layer complete.\nOnce the layer has finished generating, click the transcripts menu option, and open the first transcript in the list.\nTick your new phonemes layer.\nYou will see that each word is tagged with a phonemic transcription. You will notice that the annotations are displayed using IPA symbols. However, the layer manager doesn’t use IPA symbols directly, it actually uses the ‘DISC’ encoding for phonemes, which uses ordinary ‘typewriter’ characters (ASCII), and uses exactly one character per phoneme.\nThe IPA symbols are being displayed by LaBB-CAT to provide a linguist-friendly representation of the phonemic transcription. But you can see the underlying DISC characters by selecting the ASCII option on the layer in the transcript.\nSelect ASCII on the phonemes layer, to see what the layer manager is actually producing.\nYou may find that this is somewhat harder to read. It’s similar to the ‘SAMPA’ system for encoding phonemes, but diphthongs are generally represented by digits, and various other characters are used to represent affricates, etc.\nSelect IPA on the phonemes layer, to return to the IPA view of the layer.\n\n\n\n\n\nIt’s nice to display the IPA symbols, but it’s important to understand the DISC symbols (shown in the table below), because they are what we have to use when searching on the phonemes layer, which we are going to try now.\nThere is another possible representation of the pronunciations, called ARPABET; this is what is used in the original dictionary file published by CMU, and uses up to three uppercase characters per phoneme. While we’re not using ARPABET in this exercise, you can use it if you like, and the ARPABET symbols are included in the table. In the table, there are gaps where no ARPABET version of the phoneme is shown; this means that the CMU Pronouncing Dictionary contains no entries that include that phoneme.\n\n\n\nIPA\nDISC\nARPABET\n \n \nIPA\nDISC\nARPABET\n \n\n\np\np\nP\npat\n \nɪ\nI\nIH\nKIT\n\n\nb\nb\nB\nbad\n \nε\nE\nEH\nDRESS\n\n\nt\nt\nT\ntack\n \næ\n{\nAE\nTRAP\n\n\nd\nd\nD\ndad\n \nʌ\nV\nAH\nSTRUT\n\n\nk\nk\nK\ncad\n \nɒ\nQ\nAH\nLOT\n\n\ng\ng\nG\ngame\n \nʊ\nU\nUH\nFOOT\n\n\nŋ\nN\nNG\nbang\n \nə\n@\n[vowel ending in 0]\nanother\n\n\nm\nm\nM\nmat\n \ni:\ni\nIY\nFLEECE\n\n\nn\nn\nN\nnat\n \nα: \n#\nAA\nfather\n\n\nl\nl\nL\nlad\n \nɔ:\n$\nAO\nTHOUGHT\n\n\nr\nr\nR\nrat\n \nu:\nu\nUW\nGOOSE\n\n\nf\nf\nF\nfat\n \nɜ:\n3\nER\nNURSE\n\n\nv\nv\nV\nvat\n \neɪ\n1\nEY\nFACE\n\n\nθ\nT\nTH\nthin\n \nαɪ\n2\nAY\nPRICE\n\n\nð\nD\nDH\nthen\n \nɔɪ\n4\nOY\nCHOICE\n\n\ns\ns\nS\nsap\n \nəʊ\n5\nOW\nGOAT\n\n\nz\nz\nZ\nzap\n \nαʊ\n6\nAW\nMOUTH\n\n\n∫\nS\nSH\nsheep\n \nɪə\n7\n \nNEAR\n\n\nʒ\nZ\nZH\nmeasure\n \nεə\n8\n \nSQUARE\n\n\nj\nj\nY\nyank\n \nʊə\n9\n \nCURE\n\n\nx\nx\n \nloch\n \næ\nc\n \ntimbre\n\n\nh\nh\nHH\nhad\n \nɑ̃ː\nq\n \ndétente\n\n\nw\nw\nW\nwet\n \næ̃ː\n0\n \nlingerie\n\n\nʧ\nJ\nCH\ncheap\n \nɒ̃ː\n~\n \nbouillon\n\n\nʤ\n_\nJH\njeep\n \n \n \n \n \n\n\nŋ̩\nC\n \nbacon\n \n \n \n \n \n\n\nm̩\nF\n \nidealism\n \n \n \n \n \n\n\nn̩\nH\n \nburden\n \n \n \n \n \n\n\nl̩\nP\n \n dangle\n \n \n \n \n \n\n\n\n\nSelect the search option on the menu, which allows you to search all participants by default.\nIf it’s not already ticked, tick the new phonemes layer.\nNow you will see that our search matrix is two layers high by one word wide.\n\nSearch your new phonemes layer for words that start with h by entering the appropriate regular expression in the phonemes box.\nYou will see that the results contain words that you might not expect, like “where”, “which” and “when”.\nClick one of these unexpected results, to open the transcript.\nYou will see that, in the transcript, the pronunciation appears to start with /w/, not with /h/.\nClick on the word and select the bottom Edit option on the menu that appears.\nThis opens a small window that displays all annotations on that word token.\nNow look for the phonemes layer. You will see that, in addition to the pronunciation that starts with /w/, there’s another annotation that starts with /h/, which is invisible on the transcript.\nThese are all the possible phonemic transcriptions for the word, ordered most-frequent first. Only the first one is displayed in the transcript, but when you do searches, all of them are searched. This can result in unexpected matches like this, but it can be useful, as it ensures that when you search for a particular phonemic pattern, all possible tokens are returned, not just those that match on the most ‘normal’ transcription.\n\n\nNow that we have phonemic transcripts, we can do a better job of the search we tried in the earlier exercise - “the” followed by a word starting with a vowel…\n\nGo to the search page.\nCreate a search matrix that’s two words wide, and includes the orthography and phonemes layers.\nType the in the first orthography box.\nClick the second box on the phonemes layer, but don’t enter anything in the box yet.\nThe box has a little symbol « to the right of it.\nHover the mouse over it to see what it says, and then click it.\nYou will see that a section opens with a bunch of phoneme symbols on it; clicking on a phoneme adds its DISC representation to the search box.\nYou could use the square-brackets [ at the start of your pattern, and click all vowel symbols to add all possible vowels - Note that the vowels in the DISC representation extend beyond a, e, i, o, and u - you should add in all the vowels you see in the list that appears when you expand the IPA helper, including all the diphthongs.\n\nAlternatively, you can simply click the VOWEL link in the ‘phoneme symbol selector’, which will add all the DISC vowels for you, already enclosed in square-brackets.\nBe sure to append the ‘any vowel’ regular expression with .* to ensure the search matches words that have phonemes after the initial vowel\nRun the search and check that it’s giving you what you expect. Notice that now there are no ‘false positives’ like “the one” that we were getting when searching by orthography alone.\n\nNow that you’ve generated an annotation layer, and have seen how the search matrix works, you might want to try out some of the following searches, or invent some others:\n\nWords which have the DRESS vowel as the second phoneme\nWords ending with a front vowel, followed by words beginning with /p/ or /b/\nWords that begin with “k” in their spelling, but begin with the phoneme /n/\nWords that begin with “k” in their spelling, but do not begin with the phoneme /n/"
  },
  {
    "objectID": "worksheets/express-tutorial/4-cmudict-and-cross-layer-search.html#install-the-cmu-dictionary",
    "href": "worksheets/express-tutorial/4-cmudict-and-cross-layer-search.html#install-the-cmu-dictionary",
    "title": "4 - The CMU Dictionary and Cross Layer Searching",
    "section": "",
    "text": "The first thing we’re going to to is install the CMU Pronouncing Dictionary layer manager…\n\nSelect the layer managers menu option.\nYou will see a list of pre-installed layer managers, which are modules that can perform automatic annotation tasks. The CMU Pronouncing Dictionary layer manager isn’t pre-installed, because it is language-specific.\nFollow the List of layer managers that are not yet installed link near the bottom.\nFind CMU Pronouncing Dictionary in the list, and press its Install button.\nPress Install on the resulting information page.\nThis displays some further information about the layer manager, allowing you to upload an alternative version of the dictionary file.\nWe be using the standard file that is included with the layer manager.\nPress Configure.\nYou will see a progress bar while the layer manager loads the data from the dictionary file into the LaBB-CAT database. This will take a minute or so.\nOnce it’s finished, you will see a new window open with information about the CMU Pronouncing Dictionary layer manager."
  },
  {
    "objectID": "worksheets/express-tutorial/4-cmudict-and-cross-layer-search.html#annotate-words-with-pronunciations",
    "href": "worksheets/express-tutorial/4-cmudict-and-cross-layer-search.html#annotate-words-with-pronunciations",
    "title": "4 - The CMU Dictionary and Cross Layer Searching",
    "section": "",
    "text": "Now that we’ve installed the layer manager, we’ll create a layer that contains word pronunciations.\n\nSelect the word layers option on the menu.\nYou will see a list of existing word layers, including the orthography layer, the lexical layer, etc.\nThe column headings are also a form for defining a new word layer. Fill in the following details in this form:\n\nLayer ID: phonemes\nType: Phonological\nAlignment: None\nManager: CMU Pronouncing Dictionary\nDescription: All possible phonemic transcriptions for each word.\n\nPress New to add the layer.\nYou will see the layer configuration form.\nSet the Encoding field to CELEX DISC, and the default values for everything else.\n\n\n\n\n\n\n\nTip\n\n\n\n If you’re curious about what the configuration options do, hover your mouse over each one to see further information about what the setting does.\n\n\n\nPress Set Parameters.\nYou will see a message asking you if you want (re)generate the layer data now.\nPress Regenerate.\nYou will see a progress bar moving across the page while the annotations are being generated. When it is finished, you will see a message saying Layer complete.\nOnce the layer has finished generating, click the transcripts menu option, and open the first transcript in the list.\nTick your new phonemes layer.\nYou will see that each word is tagged with a phonemic transcription. You will notice that the annotations are displayed using IPA symbols. However, the layer manager doesn’t use IPA symbols directly, it actually uses the ‘DISC’ encoding for phonemes, which uses ordinary ‘typewriter’ characters (ASCII), and uses exactly one character per phoneme.\nThe IPA symbols are being displayed by LaBB-CAT to provide a linguist-friendly representation of the phonemic transcription. But you can see the underlying DISC characters by selecting the ASCII option on the layer in the transcript.\nSelect ASCII on the phonemes layer, to see what the layer manager is actually producing.\nYou may find that this is somewhat harder to read. It’s similar to the ‘SAMPA’ system for encoding phonemes, but diphthongs are generally represented by digits, and various other characters are used to represent affricates, etc.\nSelect IPA on the phonemes layer, to return to the IPA view of the layer."
  },
  {
    "objectID": "worksheets/express-tutorial/4-cmudict-and-cross-layer-search.html#search-across-layers",
    "href": "worksheets/express-tutorial/4-cmudict-and-cross-layer-search.html#search-across-layers",
    "title": "4 - The CMU Dictionary and Cross Layer Searching",
    "section": "",
    "text": "It’s nice to display the IPA symbols, but it’s important to understand the DISC symbols (shown in the table below), because they are what we have to use when searching on the phonemes layer, which we are going to try now.\nThere is another possible representation of the pronunciations, called ARPABET; this is what is used in the original dictionary file published by CMU, and uses up to three uppercase characters per phoneme. While we’re not using ARPABET in this exercise, you can use it if you like, and the ARPABET symbols are included in the table. In the table, there are gaps where no ARPABET version of the phoneme is shown; this means that the CMU Pronouncing Dictionary contains no entries that include that phoneme.\n\n\n\nIPA\nDISC\nARPABET\n \n \nIPA\nDISC\nARPABET\n \n\n\np\np\nP\npat\n \nɪ\nI\nIH\nKIT\n\n\nb\nb\nB\nbad\n \nε\nE\nEH\nDRESS\n\n\nt\nt\nT\ntack\n \næ\n{\nAE\nTRAP\n\n\nd\nd\nD\ndad\n \nʌ\nV\nAH\nSTRUT\n\n\nk\nk\nK\ncad\n \nɒ\nQ\nAH\nLOT\n\n\ng\ng\nG\ngame\n \nʊ\nU\nUH\nFOOT\n\n\nŋ\nN\nNG\nbang\n \nə\n@\n[vowel ending in 0]\nanother\n\n\nm\nm\nM\nmat\n \ni:\ni\nIY\nFLEECE\n\n\nn\nn\nN\nnat\n \nα: \n#\nAA\nfather\n\n\nl\nl\nL\nlad\n \nɔ:\n$\nAO\nTHOUGHT\n\n\nr\nr\nR\nrat\n \nu:\nu\nUW\nGOOSE\n\n\nf\nf\nF\nfat\n \nɜ:\n3\nER\nNURSE\n\n\nv\nv\nV\nvat\n \neɪ\n1\nEY\nFACE\n\n\nθ\nT\nTH\nthin\n \nαɪ\n2\nAY\nPRICE\n\n\nð\nD\nDH\nthen\n \nɔɪ\n4\nOY\nCHOICE\n\n\ns\ns\nS\nsap\n \nəʊ\n5\nOW\nGOAT\n\n\nz\nz\nZ\nzap\n \nαʊ\n6\nAW\nMOUTH\n\n\n∫\nS\nSH\nsheep\n \nɪə\n7\n \nNEAR\n\n\nʒ\nZ\nZH\nmeasure\n \nεə\n8\n \nSQUARE\n\n\nj\nj\nY\nyank\n \nʊə\n9\n \nCURE\n\n\nx\nx\n \nloch\n \næ\nc\n \ntimbre\n\n\nh\nh\nHH\nhad\n \nɑ̃ː\nq\n \ndétente\n\n\nw\nw\nW\nwet\n \næ̃ː\n0\n \nlingerie\n\n\nʧ\nJ\nCH\ncheap\n \nɒ̃ː\n~\n \nbouillon\n\n\nʤ\n_\nJH\njeep\n \n \n \n \n \n\n\nŋ̩\nC\n \nbacon\n \n \n \n \n \n\n\nm̩\nF\n \nidealism\n \n \n \n \n \n\n\nn̩\nH\n \nburden\n \n \n \n \n \n\n\nl̩\nP\n \n dangle\n \n \n \n \n \n\n\n\n\nSelect the search option on the menu, which allows you to search all participants by default.\nIf it’s not already ticked, tick the new phonemes layer.\nNow you will see that our search matrix is two layers high by one word wide.\n\nSearch your new phonemes layer for words that start with h by entering the appropriate regular expression in the phonemes box.\nYou will see that the results contain words that you might not expect, like “where”, “which” and “when”.\nClick one of these unexpected results, to open the transcript.\nYou will see that, in the transcript, the pronunciation appears to start with /w/, not with /h/.\nClick on the word and select the bottom Edit option on the menu that appears.\nThis opens a small window that displays all annotations on that word token.\nNow look for the phonemes layer. You will see that, in addition to the pronunciation that starts with /w/, there’s another annotation that starts with /h/, which is invisible on the transcript.\nThese are all the possible phonemic transcriptions for the word, ordered most-frequent first. Only the first one is displayed in the transcript, but when you do searches, all of them are searched. This can result in unexpected matches like this, but it can be useful, as it ensures that when you search for a particular phonemic pattern, all possible tokens are returned, not just those that match on the most ‘normal’ transcription.\n\n\nNow that we have phonemic transcripts, we can do a better job of the search we tried in the earlier exercise - “the” followed by a word starting with a vowel…\n\nGo to the search page.\nCreate a search matrix that’s two words wide, and includes the orthography and phonemes layers.\nType the in the first orthography box.\nClick the second box on the phonemes layer, but don’t enter anything in the box yet.\nThe box has a little symbol « to the right of it.\nHover the mouse over it to see what it says, and then click it.\nYou will see that a section opens with a bunch of phoneme symbols on it; clicking on a phoneme adds its DISC representation to the search box.\nYou could use the square-brackets [ at the start of your pattern, and click all vowel symbols to add all possible vowels - Note that the vowels in the DISC representation extend beyond a, e, i, o, and u - you should add in all the vowels you see in the list that appears when you expand the IPA helper, including all the diphthongs.\n\nAlternatively, you can simply click the VOWEL link in the ‘phoneme symbol selector’, which will add all the DISC vowels for you, already enclosed in square-brackets.\nBe sure to append the ‘any vowel’ regular expression with .* to ensure the search matches words that have phonemes after the initial vowel\nRun the search and check that it’s giving you what you expect. Notice that now there are no ‘false positives’ like “the one” that we were getting when searching by orthography alone.\n\nNow that you’ve generated an annotation layer, and have seen how the search matrix works, you might want to try out some of the following searches, or invent some others:\n\nWords which have the DRESS vowel as the second phoneme\nWords ending with a front vowel, followed by words beginning with /p/ or /b/\nWords that begin with “k” in their spelling, but begin with the phoneme /n/\nWords that begin with “k” in their spelling, but do not begin with the phoneme /n/"
  },
  {
    "objectID": "worksheets/express-tutorial/1-install-and-configure-labb-cat.html",
    "href": "worksheets/express-tutorial/1-install-and-configure-labb-cat.html",
    "title": "1 - Install and Configure LaBB-CAT",
    "section": "",
    "text": "In this exercise you will:\n\nInstall the LaBB-CAT software\nDefine corpora\nDefine transcript types\n\nAfter this you will have an empty LaBB-CAT database set up ready to upload transcripts into.\n\n\nYou should only follow these steps if you will be running LaBB-CAT on your own computer.\nIf you are using a LaBB-CAT server that’s already been installed for you elsewhere, you can skip to Setup below.\n\nYou have a file called install-labbcat.jar - double click this file to start the installer.\n\n\n\n\n\n\n\nIf you are using OS X\n\n\n\n\n\nOn Mac OS X you may see a message that the file can’t be opened: \nIf this happens:\n\nClick the Apple icon in the top left corner of the screen.\nSelect System Preferences\nClick Security & Privacy\nNear the bottom it says “install-labbcat.jar” was blocked from opening because it is not from an identified developer.\n\nClick Open Anyway\nYou may see another warning about the program being downloaded from the internet\n\nClick Open\n \n\n\n\n\n\nClick Start\nYou will see the progress bar move as files are installed. Once this is finished, you’ll see a message saying “Installation complete.”\n\n\nClick Finished to close the installer\n\nThe software is now installed. LaBB-CAT is a browser-based system, which means that it works as a mini web server on your computer, and you need to access it using your web browser.\nEach time you want to use LaBB-CAT, you must start it up, and which you’ve finished, you close it down again.\nTo start LaBB-CAT, click the LaBB-CAT icon in your applications area.\n\nOn Windows, open the Start menu and type LaBB-CAT.\nOn OS X you will find LaBB-CAT in your Applications folder.\n\nA window called “LaBB-CAT Server” will open, and after a short delay, your default web browser will open on a page called “LaBB-CAT”.\n\n\n\nNow we will set up a basic structure for receiving data:\n\nOpen your web browser on LaBB-CAT’s start page.\nThe very first time the page opens, you will see the software’s licence. Press I Agree to access the start page.\nThe start page has a link on it called “Where do I start?” - you may like to click on this link and read the first section, which explains a little about how to navigate around LaBB-CAT and where to find online help and hints.\nClick back on the start page of LaBB-CAT (the page with the “Where do I start?” link).\n\nNow we will set up some corpus names…\n\nOn the menu at the top, select the corpora link.\nThis page shows a list of current corpora, which only contains one corpus, called corpus.\nThe column headings at the top, Name, Language, and Description, also make a form you can fill in to create a new corpus. Fill in the following information:\n\nName: QB\nLanguage: English\nDescription: Quakebox recordings\n\nPress the New button on the right to add the QB corpus.\nYou should see a message at the top of the page saying “Record created” and now the QB corpus is in the list, under the corpus corpus.\nAdd another corpus called UC with the description Campus recordings\nWe won’t actually be using the corpus called corpus, so we want to delete it. To do this, press the Delete button to the right of the ‘corpus’ corpus in the list.\nYou will be asked “Are you sure you want to delete this record?”\nYou are sure, so press OK\nThe row will be deleted from the list.\n\nNow you have some corpora set up with the names you’ve provided.\nThe data we are using is a collection of stories about peoples’ experiences during the devastating earthquakes that hit the Canterbury region of New Zealand in 2010 and 2011. Some recordings are interviews, where an interviewer asks the participant questions, and others are monologues. Now we’re going to set up these two transcript types …\n\nSelect on the transcript types menu option.\nYou will see a list of transcript types, although there’s currently only one type in the list, called ‘interview’.\nAbove this, fill in the empty Type box with the word: monologue\nPress the New button on the right.\nYou will notice that now the list has two transcript types, interview and monologue.\nPress the Save button that has appeared below the other buttons to confirm this change.\nYou will see a message at the top saying “Updated transcript types”.\n\n\nNow you have an empty database for which you’ve:\n\ncreated two corpora, QB and UC, and\ncreated a new transcript type, so that we can have monologues as well as interviews."
  },
  {
    "objectID": "worksheets/express-tutorial/1-install-and-configure-labb-cat.html#installation",
    "href": "worksheets/express-tutorial/1-install-and-configure-labb-cat.html#installation",
    "title": "1 - Install and Configure LaBB-CAT",
    "section": "",
    "text": "You should only follow these steps if you will be running LaBB-CAT on your own computer.\nIf you are using a LaBB-CAT server that’s already been installed for you elsewhere, you can skip to Setup below.\n\nYou have a file called install-labbcat.jar - double click this file to start the installer.\n\n\n\n\n\n\n\nIf you are using OS X\n\n\n\n\n\nOn Mac OS X you may see a message that the file can’t be opened: \nIf this happens:\n\nClick the Apple icon in the top left corner of the screen.\nSelect System Preferences\nClick Security & Privacy\nNear the bottom it says “install-labbcat.jar” was blocked from opening because it is not from an identified developer.\n\nClick Open Anyway\nYou may see another warning about the program being downloaded from the internet\n\nClick Open\n \n\n\n\n\n\nClick Start\nYou will see the progress bar move as files are installed. Once this is finished, you’ll see a message saying “Installation complete.”\n\n\nClick Finished to close the installer\n\nThe software is now installed. LaBB-CAT is a browser-based system, which means that it works as a mini web server on your computer, and you need to access it using your web browser.\nEach time you want to use LaBB-CAT, you must start it up, and which you’ve finished, you close it down again.\nTo start LaBB-CAT, click the LaBB-CAT icon in your applications area.\n\nOn Windows, open the Start menu and type LaBB-CAT.\nOn OS X you will find LaBB-CAT in your Applications folder.\n\nA window called “LaBB-CAT Server” will open, and after a short delay, your default web browser will open on a page called “LaBB-CAT”."
  },
  {
    "objectID": "worksheets/express-tutorial/1-install-and-configure-labb-cat.html#sec-setup",
    "href": "worksheets/express-tutorial/1-install-and-configure-labb-cat.html#sec-setup",
    "title": "1 - Install and Configure LaBB-CAT",
    "section": "",
    "text": "Now we will set up a basic structure for receiving data:\n\nOpen your web browser on LaBB-CAT’s start page.\nThe very first time the page opens, you will see the software’s licence. Press I Agree to access the start page.\nThe start page has a link on it called “Where do I start?” - you may like to click on this link and read the first section, which explains a little about how to navigate around LaBB-CAT and where to find online help and hints.\nClick back on the start page of LaBB-CAT (the page with the “Where do I start?” link).\n\nNow we will set up some corpus names…\n\nOn the menu at the top, select the corpora link.\nThis page shows a list of current corpora, which only contains one corpus, called corpus.\nThe column headings at the top, Name, Language, and Description, also make a form you can fill in to create a new corpus. Fill in the following information:\n\nName: QB\nLanguage: English\nDescription: Quakebox recordings\n\nPress the New button on the right to add the QB corpus.\nYou should see a message at the top of the page saying “Record created” and now the QB corpus is in the list, under the corpus corpus.\nAdd another corpus called UC with the description Campus recordings\nWe won’t actually be using the corpus called corpus, so we want to delete it. To do this, press the Delete button to the right of the ‘corpus’ corpus in the list.\nYou will be asked “Are you sure you want to delete this record?”\nYou are sure, so press OK\nThe row will be deleted from the list.\n\nNow you have some corpora set up with the names you’ve provided.\nThe data we are using is a collection of stories about peoples’ experiences during the devastating earthquakes that hit the Canterbury region of New Zealand in 2010 and 2011. Some recordings are interviews, where an interviewer asks the participant questions, and others are monologues. Now we’re going to set up these two transcript types …\n\nSelect on the transcript types menu option.\nYou will see a list of transcript types, although there’s currently only one type in the list, called ‘interview’.\nAbove this, fill in the empty Type box with the word: monologue\nPress the New button on the right.\nYou will notice that now the list has two transcript types, interview and monologue.\nPress the Save button that has appeared below the other buttons to confirm this change.\nYou will see a message at the top saying “Updated transcript types”.\n\n\nNow you have an empty database for which you’ve:\n\ncreated two corpora, QB and UC, and\ncreated a new transcript type, so that we can have monologues as well as interviews."
  },
  {
    "objectID": "worksheets/express-tutorial/3-basic-searching.html",
    "href": "worksheets/express-tutorial/3-basic-searching.html",
    "title": "3 - Basic Searching",
    "section": "",
    "text": "3 - Basic Searching\nNow that you have some transcripts in your database, we’ll try out LaBB-CAT’s search functions a little.\nSearching broadly involves the following steps:\n\nselecting participants whose utterances you want to search, \nspecifying one or more patterns to search for, and\nexploring or extracting the search results.\n\n\nWe’ll start with a very simple search - all the instances of the word “the” uttered by monolingual English-speaking males.\n\nIn LaBB-CAT, select the participants link on the menu.\nThis takes you to the Participants page you have already seen in a previous exercise, where you can list participants and filter them by their attributes. You can see various participant attributes listed across top of the page.\n\nWe’re interested in male participants, so under the word Gender, select M.\nThe page will then display a list of all the male participants in the database.\nWe want the participants who speak only English, so enter the following regular expression under Languages:\n^English$\nThis pattern, starting with “^” and ending with “$”, means “match only values that start and end with the word ‘English’” - i.e. English is the only word mentioned.\nThe page will then display a list of male participants who list only English as their language.\n\nPress Layered Search at the top.\nYou will see the participants you selected listed at the top, followed by a list of layers (which we’ll ignore for now). Below that, there’s a heading Search Matrix with various controls. This is the ‘search matrix’, although it doesn’t look much like a matrix yet, because it’s only one layer high and one word wide…\nIn the box labelled ‘regular expression’ under the word orthography type the word the\n\nNow press the Search button at the bottom.\nA progress bar will appear, and then shortly after that, a new window will open, which has a list of search results in it.\n\n\n\n\n\n\n\nNote\n\n\n\nYour browser’s popup-blocker might prevent the results page from opening – you can fix that either by allowing the popups in your browser, or by clicking the Display results link that appears after the search finishes.\n\n\n\nEach match is highlighted and shown within a few words context. Click on the first match.\nYou will see that the interactive transcript page opens in a new browser tab, with the match at the top, and highlighted. You will also see that all the other matches from the same transcript are also highlighted.\nWe’ve already seen what can be done in the interactive transcript page, so close the tab to return to the results page.\nEach result line has a ticked checkbox next to it. Scroll to the bottom of the list.\nYou’ll see that there are several buttons at the bottom, which perform operations on the ticked results CSV Export, Utterance Export, and Audio Export (among others).\nUntick the Select all results checkbox, and then tick a handful of results in the list.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can select a group of matches by ticking the first one, and then holding down the Shift key while ticking the last one.\n\n\n\nHover the mouse pointer over the Prefix Names checkbox to see what this option does, and then tick it.\nClick the Audio Export button.\nSave and open the resulting zip file. \nYou’ll see that the files are systematically named to include:\n\nthe result number,\nthe name of the transcript, and\nthe start and end time of the extracted utterance.\n\nNow go back to the results page and click the Utterance Export button.\nSave and open the resulting zip file.\nYou’ll see that the TextGrid names match the audio file names in the previous zip file.\nOpen one of the TextGrids in Praat.\nYou’ll see that, in addition to the utterance and word tiers, there’s also a target tier which marks the words that matched the search.\nBack on the results page, click the CSV Export button.\nSave the resulting file, and open it.\n\nYou may have to specify some import options, in which case it may be handy to know that the field separator is comma, and the fields are quoted by speech marks.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re using Microsoft Excel and you find it doesn’t open all the columns correctly: 1. Create a new workbook in Excel. 2. Click the Data tab. 3. On the Get External Data ribbon click From Text. 4. Select the CSV file you downloaded. 5. Select Delimited and click Next. 6. Ensure Comma is the only delimiter ticked and click Next. 7. Click Finish and then OK.\n\n\nYou will see a spreadsheet with one line per selected result, and various columns containing information about the speaker, the corpus, the match line and word, and a URL to the interactive transcript for the match.\nWith this spreadsheet, you can work ‘offline’ with the results, tagging them, computing statistics in Excel, R, or any other program that can work with CSV files. There are a few more uses for the CSV results files, which are dealt with in a separate tutorial…\n\nClose the CSV file, and the results page, and go back to the search matrix page.\n\nWe’ve seen that you can search for exact word matches, but you can also search for patterns, using ‘regular expressions’. Now we’re going to search for words beginning with “the…”\n\nChange the orthography search text to the.*\n(i.e. after the word “the”, append a full-stop and an asterisk.\n \nThe full-stop means “any character at all”, and the asterisk means “zero or more of the previous thing”, so *.** means “zero or more characters”.\nPress Search.\nYou will see that now the search results include the word “the” and also words like “then”, “there”, “they”, etc.\nNow go back to the search page, and change the asterisk to a plus-sign, which means “one or more of the previous thing”\n\nClick Search\nYou will see that now the search results exclude the word “the”, only including words where the initial “the…” is followed by at least one character.\nNow change your search by replacing the e in “the” with [aeiou] - so your search pattern will be th[aeiou].+\nThe square-brackets mean “any one of the things inside the brackets”, so [aeiou] means “any vowel”.\nClick Search\nYou will now see that the results include words like “think”, “that”, “thought”, etc.\n\n\n\n\n\n\n\nTip\n\n\n\n You can get more information about regular expressions by using the online help on the search page, and also by clicking the the regular expressions link above the search matrix.\n\n\nUp until now, we’ve only been matching against one word at a time. Now we’re going to include patterns for a chain of words.\n\nOn the search page, to the right of the search matrix, there’s a + button. Press it.\n![A frame labelled ‘orthography’ with ’matches thaeiou’’ inside, followed by a dropdown box with ‘followed immediately bo’ selected, followed by another frame labelled ‘orthography’ containing a ‘matches’ dropdown box and an empty ‘regular expression’ box\nNow you will see that our search matrix is one layer high by two words wide.\nChange the entries on the orthography layer so that it will match the word “the” followed immediately by a word that starts with a vowel, and click Search.\nCheck the search results are giving you what you expected.\nNow search for “the” followed, within two words, by a word that starts with a vowel.\nDream up some other searches that interest you, and try out other options on the search page.\n\n\n\n\n\n\n\nTip\n\n\n\n If in doubt about a search option, try the online help page.\n\n\nBecause we’re searching by word orthography, you will have noticed that your searches for words starting with a vowel return words where the spelling starts with a vowel, but the pronunciation doesn’t, e.g. “one”, “once”, etc. In order to search by pronunciation, we need to add a layer of pronunciation annotations.  We’ll do that in the next exercise…\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023-2024 NZILBB"
  },
  {
    "objectID": "worksheets/express-tutorial/index.html",
    "href": "worksheets/express-tutorial/index.html",
    "title": "LaBB-CAT Express Tutorial",
    "section": "",
    "text": "LaBB-CAT Express Tutorial\nThis tutorial is a very brief introduction to the LaBB-CAT corpus analysis tool. There are four exercises, in which you:\n\ninstall and configure LaBB-CAT,\nupload a small corpus of recordings into your database and explore the transcript page,\nsearch for some tokens using regular expressions and,\ninstall and configure a module for automatically annotating tokens with their phonemic transcriptions, and search for tokens based on pronunciation instead of spelling.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023-2024 NZILBB"
  },
  {
    "objectID": "worksheets/express-tutorial/2-upload-data.html",
    "href": "worksheets/express-tutorial/2-upload-data.html",
    "title": "2 - Upload Data",
    "section": "",
    "text": "Transcripts can be uploaded manually, one at a time, using the upload transcripts option in the upload menu.\nHowever, if you already have a collection of transcripts and media files (which we have for these exercises - download QuakeStories.zip to get the workshop data), and they are systematically organised (which they are), you may be able to save some manual uploading work by uploading them using the ‘batch upload’ utility.\n\nIn LaBB-CAT, select the upload option on the menu.\nSelect the upload transcript batch link.\nThis shows a window with a large blank area in the middle with various buttons above it.\nOpen Windows Explorer or Finder, and navigate to the LaBB-CAT Workshop data folder.\nDrag the folder called “QuakeStories”, and drop it on to LaBB-CAT, on to the blank area below the buttons.\nThe previously blank area will contain a list of transcripts. Each transcript should have a value filled in for each column - Transcript, Media, Corpus, and Episode.\nMost of the transcripts are monologues, so set Type to monologue on the top left.\nClick the Upload button above the list.\nYou will see that in the Status column, the text changes to “Uploading…” for the first transcript. The progress bar progresses, and once it’s complete, the next transcript changes to “Transferring”, and so on.\nWhile the files are uploading, click  the online help link next to the upload transcript batch link you clicked above and read the conditions that must be met in order to use the batch uploader.\nOnce the uploader is finished, you can verify that all the transcripts are there by selecting the transcripts option on the menu in LaBB-CAT.\nYou should see a list of twenty transcripts.\nUse the “Transcript” box to find UC013AM_Dom.eaf\n(You can type just part of the name if you like)\nSelect the Attributes icon for UC013AM_Dom.eaf\n(the one with the spanner/wrench icon 🔧 on the right).\nChange type to interview and press Save.\nSimilarly, the following transcripts are interviews, so change their type accordingly\n\nUC215YW_DanielaMaoate-Cox.eaf\nUC226AD.eaf\n\nThe heading at the top of the transcript attributes page, which is the name of the transcript, is a link. Click the link.\n\nYou will now see LaBB-CAT’s ‘interactive transcript’ page for the transcript.\nAt the top there is a heading, a list of speakers, and then below this, the lines from the transcript, their speakers in the margin. This includes the words the participants utter, and also any noises, comments, and other annotations that were put in the transcript in ELAN.\n\nIn the top right corner are some playback controls; click the play button. You will see a shaded rectangle following the participant’s speech.\nTry the other controls to see what they do.\nNow click on any word in the transcript.\nYou will see a menu appear, with options for the ‘Utterance’ (the line), and the word.\nClick the play option in the menu to see what it does.\nClick on the formats link under the title.\nYou will see a menu, which includes various formats for exporting the transcript.\nSelect Plain Text Document\nSave the resulting file on your desktop, and then open it.\nYou will see the transcript in plain-text form.\nClick the formats link, and select the Praat Text Grid option.\nSave the resulting file on your desktop, and then open it with Praat.\n\nYou will see that the TextGrid has various tiers, two for utterances (one for each speaker), and two for individual words (one for each speaker).\n(You will see that each individual word has a ‘default’ alignment - i.e. the words are evenly spread out during the duration of the line they’re in. It is possible to make these word alignments actually line up with the words in the audio signal, using forced aligment, which is the subject of another tutorial.)\n\n\nYou can also open individual utterances in Praat directly from the transcript page, if you have Praat installed. But first, the LaBB-CAT/Praat integration has to be set up; this only has to be done once:\n\nOn the top-right of the page, above the playback controls, there’s a Praat icon - click it.\nFollow the instructions that appear (these vary depending on what web browser you use).\n\n\n\n\n\n\n\nTip\n\n\n\nYou may need to grant a browser extension permission to install, and it’s possible you will need a connection to the internet in order to download this extension.\nYou also may be asked where Praat is installed; Navigate to the location where Praat is installed, and double-click the “Praat.exe” file (on some systems the file may simply be called \"Praat\"). The Praat program may open, and then immediately close, as LaBB-CAT tests it can communicate with Praat.\n There are illustrated instructions for setting up Praat integration for each web browser in the online help for the transcript page; check there if you run into problems.\n\n\n\nNow Praat integration has been set up, and you should be able to access Praat options in the transcript page from now on…\n\nClick on a line in the transcript, and select the Open Text Grid in Praat option on the menu.\n\nPraat should open, and show you a spectrogram of the line’s audio, with a TextGrid below that includes a tier for the utterance, and another tier for individual word alignments. You could manually align them here, but it’s much more efficient to use HTK to force-align the utterances. Forced alignment is the subject of another tutorial…\n\n\n\nThe transcripts are now in the database, but the meta-data for the participants hasn’t been set yet (because it’s not contained in the ELAN files). We could manually add this for each speaker, but fortunately we have it stored in a spreadsheet (actually, a CSV text file) that we can upload in one go.\n\nIn LaBB-CAT, select the upload option on the menu.\nSelect the upload participant data option.\nClick Choose File, and select the file in the LaBB-CAT Exercises data folder called “participants.csv”.\nClick Upload\nYou will now see a list of the columns from the spreadsheet.\nFirstly, ensure that the Participant identity column is set to name. This ensures that the “name” column in the spreadsheet will be used to match names of participants in the LaBB-CAT database.\nBelow that is listed each column from the spreadsheet, with an arrow pointing to a drop-down box. The box contains various options, including each of the participant attributes set up in LaBB-CAT, an ignore option, and create a new attribute option.\nSelect the options as follows as follows:\n\n\nThe CSV column name: → ignore because it’s the Participant Identity Column identified above\nThe CSV column gender: → the Gender LaBB-CAT attribute\nThe CSV column age_category: → the create a new attribute called option, and set the Label to “Age”\nThe CSV column ethnicity: → the create a new attribute called option, and set the Label to “Ethnicity”\nThe CSV column grew_up: → the create a new attribute called option, and set the Label to “Country”\nThe CSV column grew_up_region: → the create a new attribute called option, and set the Label to “Region”\nThe CSV column grew_up_town: → the create a new attribute called option, and set the Label to “Town”\nThe CSV column languages_spoken: → the create a new attribute called option, and set the Label to “Languages”\n\nPress import.\nYou should see a page with information about the import, including the columns that were ignored, and the number of participants that were added.\n\nTo check the participant attributes really are now set:\n\nSelect the participants option on the menu.\nYou will see a list of speakers, and page links at the bottom.\nThe page also includes participant attribute values where they are known.\nPick a speaker (e.g. QB702_AnnaSoboleva) and click their name.\nYou will see the participant attributes page with their details filled in (e.g. QB702_AnnaSoboleva is a female English/Russian speaker between 18 and 25 years old).\n\nBy default, the new attributes are not flagged as searchable, so we will make a few of them searchable now.\n\n\nClick the participant attributes link on the menu.\nThis will display a list of the participant meta-data fields.\nEnsure that Searchability is set to Searchable for the following attributes:\n\ngender\nage_category\nlanguages_spoken\n\nPress the Save button at the bottom of the list.\nSelect participants on the top menu.\nYou will see that the searchable attributes are now listed with the participants.\n\nYou can filter the list using the attribute headers at the top of the list.\nUnder Gender select F\nNow the list only shows female participants.\n\n\nYou now have a small database with a number of speakers in it, so we can start doing some searches and creating some annotations."
  },
  {
    "objectID": "worksheets/express-tutorial/2-upload-data.html#uploading-transcripts-and-recordings",
    "href": "worksheets/express-tutorial/2-upload-data.html#uploading-transcripts-and-recordings",
    "title": "2 - Upload Data",
    "section": "",
    "text": "Transcripts can be uploaded manually, one at a time, using the upload transcripts option in the upload menu.\nHowever, if you already have a collection of transcripts and media files (which we have for these exercises - download QuakeStories.zip to get the workshop data), and they are systematically organised (which they are), you may be able to save some manual uploading work by uploading them using the ‘batch upload’ utility.\n\nIn LaBB-CAT, select the upload option on the menu.\nSelect the upload transcript batch link.\nThis shows a window with a large blank area in the middle with various buttons above it.\nOpen Windows Explorer or Finder, and navigate to the LaBB-CAT Workshop data folder.\nDrag the folder called “QuakeStories”, and drop it on to LaBB-CAT, on to the blank area below the buttons.\nThe previously blank area will contain a list of transcripts. Each transcript should have a value filled in for each column - Transcript, Media, Corpus, and Episode.\nMost of the transcripts are monologues, so set Type to monologue on the top left.\nClick the Upload button above the list.\nYou will see that in the Status column, the text changes to “Uploading…” for the first transcript. The progress bar progresses, and once it’s complete, the next transcript changes to “Transferring”, and so on.\nWhile the files are uploading, click  the online help link next to the upload transcript batch link you clicked above and read the conditions that must be met in order to use the batch uploader.\nOnce the uploader is finished, you can verify that all the transcripts are there by selecting the transcripts option on the menu in LaBB-CAT.\nYou should see a list of twenty transcripts.\nUse the “Transcript” box to find UC013AM_Dom.eaf\n(You can type just part of the name if you like)\nSelect the Attributes icon for UC013AM_Dom.eaf\n(the one with the spanner/wrench icon 🔧 on the right).\nChange type to interview and press Save.\nSimilarly, the following transcripts are interviews, so change their type accordingly\n\nUC215YW_DanielaMaoate-Cox.eaf\nUC226AD.eaf\n\nThe heading at the top of the transcript attributes page, which is the name of the transcript, is a link. Click the link.\n\nYou will now see LaBB-CAT’s ‘interactive transcript’ page for the transcript.\nAt the top there is a heading, a list of speakers, and then below this, the lines from the transcript, their speakers in the margin. This includes the words the participants utter, and also any noises, comments, and other annotations that were put in the transcript in ELAN.\n\nIn the top right corner are some playback controls; click the play button. You will see a shaded rectangle following the participant’s speech.\nTry the other controls to see what they do.\nNow click on any word in the transcript.\nYou will see a menu appear, with options for the ‘Utterance’ (the line), and the word.\nClick the play option in the menu to see what it does.\nClick on the formats link under the title.\nYou will see a menu, which includes various formats for exporting the transcript.\nSelect Plain Text Document\nSave the resulting file on your desktop, and then open it.\nYou will see the transcript in plain-text form.\nClick the formats link, and select the Praat Text Grid option.\nSave the resulting file on your desktop, and then open it with Praat.\n\nYou will see that the TextGrid has various tiers, two for utterances (one for each speaker), and two for individual words (one for each speaker).\n(You will see that each individual word has a ‘default’ alignment - i.e. the words are evenly spread out during the duration of the line they’re in. It is possible to make these word alignments actually line up with the words in the audio signal, using forced aligment, which is the subject of another tutorial.)\n\n\nYou can also open individual utterances in Praat directly from the transcript page, if you have Praat installed. But first, the LaBB-CAT/Praat integration has to be set up; this only has to be done once:\n\nOn the top-right of the page, above the playback controls, there’s a Praat icon - click it.\nFollow the instructions that appear (these vary depending on what web browser you use).\n\n\n\n\n\n\n\nTip\n\n\n\nYou may need to grant a browser extension permission to install, and it’s possible you will need a connection to the internet in order to download this extension.\nYou also may be asked where Praat is installed; Navigate to the location where Praat is installed, and double-click the “Praat.exe” file (on some systems the file may simply be called \"Praat\"). The Praat program may open, and then immediately close, as LaBB-CAT tests it can communicate with Praat.\n There are illustrated instructions for setting up Praat integration for each web browser in the online help for the transcript page; check there if you run into problems.\n\n\n\nNow Praat integration has been set up, and you should be able to access Praat options in the transcript page from now on…\n\nClick on a line in the transcript, and select the Open Text Grid in Praat option on the menu.\n\nPraat should open, and show you a spectrogram of the line’s audio, with a TextGrid below that includes a tier for the utterance, and another tier for individual word alignments. You could manually align them here, but it’s much more efficient to use HTK to force-align the utterances. Forced alignment is the subject of another tutorial…"
  },
  {
    "objectID": "worksheets/express-tutorial/2-upload-data.html#participant-data-import",
    "href": "worksheets/express-tutorial/2-upload-data.html#participant-data-import",
    "title": "2 - Upload Data",
    "section": "",
    "text": "The transcripts are now in the database, but the meta-data for the participants hasn’t been set yet (because it’s not contained in the ELAN files). We could manually add this for each speaker, but fortunately we have it stored in a spreadsheet (actually, a CSV text file) that we can upload in one go.\n\nIn LaBB-CAT, select the upload option on the menu.\nSelect the upload participant data option.\nClick Choose File, and select the file in the LaBB-CAT Exercises data folder called “participants.csv”.\nClick Upload\nYou will now see a list of the columns from the spreadsheet.\nFirstly, ensure that the Participant identity column is set to name. This ensures that the “name” column in the spreadsheet will be used to match names of participants in the LaBB-CAT database.\nBelow that is listed each column from the spreadsheet, with an arrow pointing to a drop-down box. The box contains various options, including each of the participant attributes set up in LaBB-CAT, an ignore option, and create a new attribute option.\nSelect the options as follows as follows:\n\n\nThe CSV column name: → ignore because it’s the Participant Identity Column identified above\nThe CSV column gender: → the Gender LaBB-CAT attribute\nThe CSV column age_category: → the create a new attribute called option, and set the Label to “Age”\nThe CSV column ethnicity: → the create a new attribute called option, and set the Label to “Ethnicity”\nThe CSV column grew_up: → the create a new attribute called option, and set the Label to “Country”\nThe CSV column grew_up_region: → the create a new attribute called option, and set the Label to “Region”\nThe CSV column grew_up_town: → the create a new attribute called option, and set the Label to “Town”\nThe CSV column languages_spoken: → the create a new attribute called option, and set the Label to “Languages”\n\nPress import.\nYou should see a page with information about the import, including the columns that were ignored, and the number of participants that were added.\n\nTo check the participant attributes really are now set:\n\nSelect the participants option on the menu.\nYou will see a list of speakers, and page links at the bottom.\nThe page also includes participant attribute values where they are known.\nPick a speaker (e.g. QB702_AnnaSoboleva) and click their name.\nYou will see the participant attributes page with their details filled in (e.g. QB702_AnnaSoboleva is a female English/Russian speaker between 18 and 25 years old).\n\nBy default, the new attributes are not flagged as searchable, so we will make a few of them searchable now.\n\n\nClick the participant attributes link on the menu.\nThis will display a list of the participant meta-data fields.\nEnsure that Searchability is set to Searchable for the following attributes:\n\ngender\nage_category\nlanguages_spoken\n\nPress the Save button at the bottom of the list.\nSelect participants on the top menu.\nYou will see that the searchable attributes are now listed with the participants.\n\nYou can filter the list using the attribute headers at the top of the list.\nUnder Gender select F\nNow the list only shows female participants.\n\n\nYou now have a small database with a number of speakers in it, so we can start doing some searches and creating some annotations."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LaBB-CAT Documentation",
    "section": "",
    "text": "LaBB-CAT Documentation\nThis is the documentation for the LaBB-CAT corpus management system.\nLaBB-CAT is a browser-based linguistics research tool that stores audio or video recordings, text transcripts, and other annotations.\nAnnotations of various types can be automatically generated or manually added.\nThe transcripts and annotations can be searched for particular text or regular expressions. The search results, or entire transcripts, can be viewed or saved in a variety of formats, and the related parts of the recordings can be played or opened in acoustic analysis software, all directly through the web-browser.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023-2024 NZILBB"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023-2024 NZILBB"
  },
  {
    "objectID": "howto/transcription/elan.html",
    "href": "howto/transcription/elan.html",
    "title": "Transcribing with ELAN",
    "section": "",
    "text": "ELAN (EUDICO Linguistic Annotator - http://www.lat-mpi.eu/tools/elan/) is a tier-based media annotation tool developed by the Max Planck Institute for Psycholinguistics, which can be used both for orthographic transcription, and also extensive annotation on different tiers. It can be used to annotate multiple video files, and/or an audio file.\nLaBB-CAT supports using ELAN files for uploading, as long as the ELAN file contains one tier per speaker that includes orthographic transcription of their speech, like this:\n\n\n\nELAN file with orthographic transcription, one tier per speaker\n\n\nLaBB-CAT needs a mechanism for uniquely identifying the speaker of each utterance. In ELAN, the best way to achieve that is to ensure that the Participant attribute of each tier is set with the name (or ID) of the speaker:\n\n\n\nThe Participant tier attribute is set as the speaker ID\n\n\nIf the transcript is part of multilingual corpus, you should also set the Content Language attribute of each tier. For more information, see Specifying Language in ELAN transcripts.\n\n\nTo upload ELAN files, click the upload menu option, and then the upload transcripts option.\nPress Choose File on the left and select the transcript .eaf file. On the right the text “ELAN EAF Transcript” should appear. Press the Choose File to the right of this to select the media file(s) that correspond to the transcript. Press Upload.\nNext you will be asked to specify which ELAN tiers (listed on the left) correspond to which LaBB-CAT layers (listed on the right). Tiers that contain orthographic transcript text (as illustrated above) should be mapped to LaBB-CAT’s “utterance” layer. You may also have other annotations in the transcript (e.g. a tier for noise annotations). These can be mapped to LaBB-CAT layers if a corresponding layer has already been set up in LaBB-CAT. Generally speaking, for these you’ll want to create a “span” layer for time intervals, and it’s best to create a LaBB-CAT layer with the “name” set to the same name as the tier, so that it’s selected by default. LaBB-CAT already has a “noise” layer and a “comment” layer, so these don’t need to be set up in advance.\n\n\n\nELAN tiers listed on the left are mapped to LaBB-CAT layers on the right\n\n\nIn the illustration, the “Interverwee” and “Interviewer” ELAN tiers are mapped to the “utterances” LaBB-CAT phrase layer, and the “Noise” ELAN tier is mapped to the “noise” LaBB-CAT span layer.\nWhen you click Set Mappings, the transcript will be imported into LaBB-CAT using the specified tier/layer correspondences.\n\n\n\nELAN has no direct mechanism for marking non-speech annotations in their position within the transcript text. However, LaBB-CAT supports the use of textual conventions in various ways to make certain annotations:\n\nTo tag a word with its pronunciation, enter the pronunciation in square brackets, directly following the word (i.e. with no intervening space), e.g.:\n…this was at Wingatui[wIN@tui]…\nTo tag a word with its full orthography (if the transcript doesn’t include it), enter the orthography in round parentheses, directly following the word (i.e. with no intervening space), e.g.:\n…I can't remem~(remember)…\nTo tag a word as being in a different language, enter the code CS: (for ‘code switch’) followed by the the ISO 639 3-letter code for the language, in square brackets, directly following the word (i.e. with no intervening space), e.g.:\n…me mudé de New[CS:eng] Zealand[CS:eng] en 2004…\nFor longer phrases, the code-switch tag can be placed immediately before the first word and immediately after the last word, to mark those and all intervening words as being in a different language. e.g.:\n…has a certain [CS:fre]je ne sais quoi[CS:fre] I think…\nTo insert a noise annotation within the text, enclose it in square brackets (surrounded by spaces so it’s not taken as a pronunciation annotation), e.g.:\n…sometimes me [laughs] not always but sometimes…\nTo insert a comment annotation within the text, enclose it in curly braces (surrounded by spaces), e.g.:\n…beautifully warm {softly} but its…\n\nDuring upload, these annotations will be extracted from the transcript text and inserted into corresponding LaBB-CAT layers."
  },
  {
    "objectID": "howto/transcription/elan.html#uploading-elan-files",
    "href": "howto/transcription/elan.html#uploading-elan-files",
    "title": "Transcribing with ELAN",
    "section": "",
    "text": "To upload ELAN files, click the upload menu option, and then the upload transcripts option.\nPress Choose File on the left and select the transcript .eaf file. On the right the text “ELAN EAF Transcript” should appear. Press the Choose File to the right of this to select the media file(s) that correspond to the transcript. Press Upload.\nNext you will be asked to specify which ELAN tiers (listed on the left) correspond to which LaBB-CAT layers (listed on the right). Tiers that contain orthographic transcript text (as illustrated above) should be mapped to LaBB-CAT’s “utterance” layer. You may also have other annotations in the transcript (e.g. a tier for noise annotations). These can be mapped to LaBB-CAT layers if a corresponding layer has already been set up in LaBB-CAT. Generally speaking, for these you’ll want to create a “span” layer for time intervals, and it’s best to create a LaBB-CAT layer with the “name” set to the same name as the tier, so that it’s selected by default. LaBB-CAT already has a “noise” layer and a “comment” layer, so these don’t need to be set up in advance.\n\n\n\nELAN tiers listed on the left are mapped to LaBB-CAT layers on the right\n\n\nIn the illustration, the “Interverwee” and “Interviewer” ELAN tiers are mapped to the “utterances” LaBB-CAT phrase layer, and the “Noise” ELAN tier is mapped to the “noise” LaBB-CAT span layer.\nWhen you click Set Mappings, the transcript will be imported into LaBB-CAT using the specified tier/layer correspondences."
  },
  {
    "objectID": "howto/transcription/elan.html#conventions-for-non-speech-annotations-within-the-transcript",
    "href": "howto/transcription/elan.html#conventions-for-non-speech-annotations-within-the-transcript",
    "title": "Transcribing with ELAN",
    "section": "",
    "text": "ELAN has no direct mechanism for marking non-speech annotations in their position within the transcript text. However, LaBB-CAT supports the use of textual conventions in various ways to make certain annotations:\n\nTo tag a word with its pronunciation, enter the pronunciation in square brackets, directly following the word (i.e. with no intervening space), e.g.:\n…this was at Wingatui[wIN@tui]…\nTo tag a word with its full orthography (if the transcript doesn’t include it), enter the orthography in round parentheses, directly following the word (i.e. with no intervening space), e.g.:\n…I can't remem~(remember)…\nTo tag a word as being in a different language, enter the code CS: (for ‘code switch’) followed by the the ISO 639 3-letter code for the language, in square brackets, directly following the word (i.e. with no intervening space), e.g.:\n…me mudé de New[CS:eng] Zealand[CS:eng] en 2004…\nFor longer phrases, the code-switch tag can be placed immediately before the first word and immediately after the last word, to mark those and all intervening words as being in a different language. e.g.:\n…has a certain [CS:fre]je ne sais quoi[CS:fre] I think…\nTo insert a noise annotation within the text, enclose it in square brackets (surrounded by spaces so it’s not taken as a pronunciation annotation), e.g.:\n…sometimes me [laughs] not always but sometimes…\nTo insert a comment annotation within the text, enclose it in curly braces (surrounded by spaces), e.g.:\n…beautifully warm {softly} but its…\n\nDuring upload, these annotations will be extracted from the transcript text and inserted into corresponding LaBB-CAT layers."
  },
  {
    "objectID": "howto/transcription/index.html",
    "href": "howto/transcription/index.html",
    "title": "Transcription Guidelines",
    "section": "",
    "text": "There are various tools available for transcribing recordings, and LaBB-CAT supports the transcription file formats used by the most commonly used tools. Each of these tools has its own capabilities for specifying speakers and meta data, and adding annotations.\nBeyond the specific tool and file format used for transcription, there are some general principles that can facilitate subsequent processing of speech data in LaBB-CAT.\n\n\nMany automatic annotation tasks involve looking up standard dictionaries, and words that are not found are not annotated, so it’s important to use standard spelling where possible.\n\nUse conventional spelling, and if you are unsure of how to spell something, look it up in a dictionary, or on a map.\nWrite all numbers out in full, with spaces instead of hyphens - e.g.\n\n\\(\\times\\) “1984”\n\\(\\times\\) “nineteen-eighty-four”\n\\(\\checkmark\\) “nineteen eighty four”\n\nWhen abbreviations are used, use capital letters with spaces in between each letter if each letter is said separately, otherwise use capitals with no spaces - e.g.\n\n\\(\\times\\) “NSA”\n\\(\\times\\) “N A S A”\n\\(\\checkmark\\) “N S A”\n\\(\\checkmark\\) “NASA”\n\nAll words should be spelt out in full, like “and” and “suppose”. Final gs and ds should not be dropped from words even if that’s what the speaker says - e.g.\n\n\\(\\times\\) “skippin’ an’ jumpin’ an ol’ rope”\n\\(\\checkmark\\) “skipping and jumping an old rope”\n\nA single word should always be spelt as an entire word, even if there is a pause between syllables.\nDon’t tidy up the speech. Leave in the repetitions, fillers and errors.\n\nThere may be a limited set of shortened words and contractions defined, which is fine as long as they’re consistently used - e.g. if you use “cos” as a shortened version of “because”, always spell it “cos”, and never “cause”, nor “’cause”, nor “coz”. For example:\n\ngonna\nsorta\ncos\nkinda\ngotta\ndunno\nwanna\nyip\nyeah\nokay\nuh huh\ngee\njeez\n\n\n\n\nIt’s important to be consistent with the spelling of filled pauses:\n\nah\ner\num\nmmm\n\nThe spelling of the last of these, with three m’s, is recommended because if it’s spelled with one m - “m” - this can match the name of the letter “M” in the dictionary, so the pronunciation can be tagged as /εm/, and if it’s spelled with two m’s - “mm” - this sometimes matches an alternative spelling of the word “millimeter”, so the pronunciation can be tagged /'mɪ-lɪ-\"mi-tə/.\nUnfilled pauses can be transcribed with a hyphen surrounded by whitespace; some modules use such pause information to help with automatic annotation (e.g. force-alginment with HTK benefits with pause annotations like this) - e.g.\n\n\\(\\times\\) “stop-before you begin”\n\\(\\checkmark\\) “stop - before you begin”\n\nIncomplete words should marked with a tilde ~ (not a hyphen which may be interpreted as a pause) at the end of the word e.g.:\n\n\\(\\times\\) “hesi-”\n\\(\\checkmark\\) “hesi~”\n\nFor very short hesitations - “b~ bu~ but” - some pronunciation module can infer a pronunciation for such words, without the need for a manual pronunciation tag.\n\n\n\nSome transcription tools allow tagging individual words with extra information, but others do not. For these, the only way to, for example, tag a word with its pronunciation, is to use transcription conventions.\nLaBB-CAT optionally supports the following transcription conventions, if you are using ELAN transcripts, Praat TextGrids, or plain text files for transcripts:\n\nThe pronunciation of an invented word or a hesitation can be marked by using square brackets immediately after the word (i.e. with no space between the word and the annotation) - e.g.\n“stut~[stVt]”\nThe full form standard form of a hesitation (or other word with non-standard spelling) can be marked by using parentheses immediately after the word (i.e. with no space between the word and the annotation) - e.g.\n“stut~[stVt](stutter)”\nNoises can be annotated using square brackets surrounded by white space, e.g.\n“now [tongue click] where were we”\nComments can be added by using curly braces surrounded by white space, e.g.\n“It hit me about here {points to temple}”\n\n\n\n\nSome processes, like forced alignment, involve processing individual utterances in the recording, which correspond to lines of text in many transcription systems. Very long or very short utterances can be difficult to process.\nIdeally, each line in a transcript should be 5 to 15 words long, and line breaks should be made where there are pauses in speech.\nSome annotation tools allow for marking periods of simultaneous speech - i.e. periods during which there’s more than one person speaking. These periods should be aligned as accurately as possible, because some automatic processing (e.g. forced alignment) ignore simultaneous speech; short simultaneous-speech utterances ensure that as little speech as possible is ignored."
  },
  {
    "objectID": "howto/transcription/index.html#spelling",
    "href": "howto/transcription/index.html#spelling",
    "title": "Transcription Guidelines",
    "section": "",
    "text": "Many automatic annotation tasks involve looking up standard dictionaries, and words that are not found are not annotated, so it’s important to use standard spelling where possible.\n\nUse conventional spelling, and if you are unsure of how to spell something, look it up in a dictionary, or on a map.\nWrite all numbers out in full, with spaces instead of hyphens - e.g.\n\n\\(\\times\\) “1984”\n\\(\\times\\) “nineteen-eighty-four”\n\\(\\checkmark\\) “nineteen eighty four”\n\nWhen abbreviations are used, use capital letters with spaces in between each letter if each letter is said separately, otherwise use capitals with no spaces - e.g.\n\n\\(\\times\\) “NSA”\n\\(\\times\\) “N A S A”\n\\(\\checkmark\\) “N S A”\n\\(\\checkmark\\) “NASA”\n\nAll words should be spelt out in full, like “and” and “suppose”. Final gs and ds should not be dropped from words even if that’s what the speaker says - e.g.\n\n\\(\\times\\) “skippin’ an’ jumpin’ an ol’ rope”\n\\(\\checkmark\\) “skipping and jumping an old rope”\n\nA single word should always be spelt as an entire word, even if there is a pause between syllables.\nDon’t tidy up the speech. Leave in the repetitions, fillers and errors.\n\nThere may be a limited set of shortened words and contractions defined, which is fine as long as they’re consistently used - e.g. if you use “cos” as a shortened version of “because”, always spell it “cos”, and never “cause”, nor “’cause”, nor “coz”. For example:\n\ngonna\nsorta\ncos\nkinda\ngotta\ndunno\nwanna\nyip\nyeah\nokay\nuh huh\ngee\njeez"
  },
  {
    "objectID": "howto/transcription/index.html#disfluencies",
    "href": "howto/transcription/index.html#disfluencies",
    "title": "Transcription Guidelines",
    "section": "",
    "text": "It’s important to be consistent with the spelling of filled pauses:\n\nah\ner\num\nmmm\n\nThe spelling of the last of these, with three m’s, is recommended because if it’s spelled with one m - “m” - this can match the name of the letter “M” in the dictionary, so the pronunciation can be tagged as /εm/, and if it’s spelled with two m’s - “mm” - this sometimes matches an alternative spelling of the word “millimeter”, so the pronunciation can be tagged /'mɪ-lɪ-\"mi-tə/.\nUnfilled pauses can be transcribed with a hyphen surrounded by whitespace; some modules use such pause information to help with automatic annotation (e.g. force-alginment with HTK benefits with pause annotations like this) - e.g.\n\n\\(\\times\\) “stop-before you begin”\n\\(\\checkmark\\) “stop - before you begin”\n\nIncomplete words should marked with a tilde ~ (not a hyphen which may be interpreted as a pause) at the end of the word e.g.:\n\n\\(\\times\\) “hesi-”\n\\(\\checkmark\\) “hesi~”\n\nFor very short hesitations - “b~ bu~ but” - some pronunciation module can infer a pronunciation for such words, without the need for a manual pronunciation tag."
  },
  {
    "objectID": "howto/transcription/index.html#word-tags-and-other-in-situ-annotations",
    "href": "howto/transcription/index.html#word-tags-and-other-in-situ-annotations",
    "title": "Transcription Guidelines",
    "section": "",
    "text": "Some transcription tools allow tagging individual words with extra information, but others do not. For these, the only way to, for example, tag a word with its pronunciation, is to use transcription conventions.\nLaBB-CAT optionally supports the following transcription conventions, if you are using ELAN transcripts, Praat TextGrids, or plain text files for transcripts:\n\nThe pronunciation of an invented word or a hesitation can be marked by using square brackets immediately after the word (i.e. with no space between the word and the annotation) - e.g.\n“stut~[stVt]”\nThe full form standard form of a hesitation (or other word with non-standard spelling) can be marked by using parentheses immediately after the word (i.e. with no space between the word and the annotation) - e.g.\n“stut~[stVt](stutter)”\nNoises can be annotated using square brackets surrounded by white space, e.g.\n“now [tongue click] where were we”\nComments can be added by using curly braces surrounded by white space, e.g.\n“It hit me about here {points to temple}”"
  },
  {
    "objectID": "howto/transcription/index.html#utteranceslines",
    "href": "howto/transcription/index.html#utteranceslines",
    "title": "Transcription Guidelines",
    "section": "",
    "text": "Some processes, like forced alignment, involve processing individual utterances in the recording, which correspond to lines of text in many transcription systems. Very long or very short utterances can be difficult to process.\nIdeally, each line in a transcript should be 5 to 15 words long, and line breaks should be made where there are pauses in speech.\nSome annotation tools allow for marking periods of simultaneous speech - i.e. periods during which there’s more than one person speaking. These periods should be aligned as accurately as possible, because some automatic processing (e.g. forced alignment) ignore simultaneous speech; short simultaneous-speech utterances ensure that as little speech as possible is ignored."
  },
  {
    "objectID": "howto/transcription/elan-language.html",
    "href": "howto/transcription/elan-language.html",
    "title": "Specifying Language in ELAN transcripts",
    "section": "",
    "text": "For multilingual corpora, it’s important that each transcript specifies the language of the speech that has been transcribed.\nIn ELAN, each tier can have a language specified by setting the Content Language attributed of the tier. This is set using a dropdown box, which unfortunately is often populated with a single item: English (eng).\nIf the speech is in a language other than English, you will first have to add the desired language to the list of options.\n\n\n\nClick the Edit menu and select the Edit List of Languages… option.\n\nClick the lower dropdown box (directly above the buttons) and select the language you want.\n\nClick Add.\nThe selected language should appear in the upper dropdown box.\n\nClick Close\n\n\n\n\nYour transcript tiers now need to have this language selected.\n\nIn the transcript, right-click the name of a transcript tier on the left and select the Change Attributes Of… option.\n\nChange the Content Language option by selecting the language from the dropdown list.\n\nClick Change.\nThe tier attributes window will close.\nRepeat the previous three steps for each transcript tier.\n\n\n\n\nSometime the speaker may include words or phrases in a different language from the rest of the transcript. It may be important to tag these words with the language they’re in (e.g. to ensure that after upload to LaBB-CAT, the pronunciation is correctly inferred).\nLaBB-CAT recognises a transcription convention for tagging ‘code-switches’ (CS) into another language. Immediately following the word (i.e. with no intevening space) in square brackes the code “CS:” is added with the ISO 639 3-letter code for the language. e.g.\n\n…me mudé de Christchurch[CS:eng] en 2004…\n\nFor longer phrases, the code-switch tag can be placed immediately before the first word and immediately after the last word, to mark those and all intervening words as being in a different language. e.g.\n\n…it has a certain [CS:fre]je ne sais quoi[CS:fre] I think…"
  },
  {
    "objectID": "howto/transcription/elan-language.html#adding-a-new-language-option",
    "href": "howto/transcription/elan-language.html#adding-a-new-language-option",
    "title": "Specifying Language in ELAN transcripts",
    "section": "",
    "text": "Click the Edit menu and select the Edit List of Languages… option.\n\nClick the lower dropdown box (directly above the buttons) and select the language you want.\n\nClick Add.\nThe selected language should appear in the upper dropdown box.\n\nClick Close"
  },
  {
    "objectID": "howto/transcription/elan-language.html#defining-the-language-for-the-transcript",
    "href": "howto/transcription/elan-language.html#defining-the-language-for-the-transcript",
    "title": "Specifying Language in ELAN transcripts",
    "section": "",
    "text": "Your transcript tiers now need to have this language selected.\n\nIn the transcript, right-click the name of a transcript tier on the left and select the Change Attributes Of… option.\n\nChange the Content Language option by selecting the language from the dropdown list.\n\nClick Change.\nThe tier attributes window will close.\nRepeat the previous three steps for each transcript tier."
  },
  {
    "objectID": "howto/transcription/elan-language.html#speech-in-a-different-language",
    "href": "howto/transcription/elan-language.html#speech-in-a-different-language",
    "title": "Specifying Language in ELAN transcripts",
    "section": "",
    "text": "Sometime the speaker may include words or phrases in a different language from the rest of the transcript. It may be important to tag these words with the language they’re in (e.g. to ensure that after upload to LaBB-CAT, the pronunciation is correctly inferred).\nLaBB-CAT recognises a transcription convention for tagging ‘code-switches’ (CS) into another language. Immediately following the word (i.e. with no intevening space) in square brackes the code “CS:” is added with the ISO 639 3-letter code for the language. e.g.\n\n…me mudé de Christchurch[CS:eng] en 2004…\n\nFor longer phrases, the code-switch tag can be placed immediately before the first word and immediately after the last word, to mark those and all intervening words as being in a different language. e.g.\n\n…it has a certain [CS:fre]je ne sais quoi[CS:fre] I think…"
  },
  {
    "objectID": "howto/corpus-management/change-corpus.html",
    "href": "howto/corpus-management/change-corpus.html",
    "title": "Changing Corpus",
    "section": "",
    "text": "The ‘corpus’ and the ‘episode’ of a transcript belongs to is specified when it is uploaded; every transcript belongs to an ‘episode’ (which groups together transcripts from the same recording session), and each episode belongs to a ‘corpus’\nWhat if you choose the wrong corpus, or change your mind about episode name later? You can use the episode organiser option on the menu to make such changes after uploading transcripts.\nThe organiser appears as a list of collapsable ‘folders’ that represent existing corpora, containing episodes:\n\n\n\nTo move an episode from one corpus to another, simply drag the episode with your mouse and drop it on to the desired corpus (if you want a new corpus, you must add it first using the corpora option on the menu).\n\n\n\nTo rename an episode, right-click on the episode and select the Rename option."
  },
  {
    "objectID": "howto/corpus-management/change-corpus.html#changing-corpus-1",
    "href": "howto/corpus-management/change-corpus.html#changing-corpus-1",
    "title": "Changing Corpus",
    "section": "",
    "text": "To move an episode from one corpus to another, simply drag the episode with your mouse and drop it on to the desired corpus (if you want a new corpus, you must add it first using the corpora option on the menu)."
  },
  {
    "objectID": "howto/corpus-management/change-corpus.html#renaming-an-episode",
    "href": "howto/corpus-management/change-corpus.html#renaming-an-episode",
    "title": "Changing Corpus",
    "section": "",
    "text": "To rename an episode, right-click on the episode and select the Rename option."
  },
  {
    "objectID": "howto/corpus-management/participant-deduplication.html",
    "href": "howto/corpus-management/participant-deduplication.html",
    "title": "Participant Deduplication",
    "section": "",
    "text": "Participant Deduplication\nDuring transcript upload, LaBB-CAT looks up the participants named in the transcript, and if it doesn’t find a matching participant record, it creates a new one.\nBut sometimes, names of participants are not totally identical across all of their transcripts, leading to multiple records for the same person.\nYou can merge participant records together to fix this problem:\n\nIn LaBB-CAT, select the participants option on the menu\nIn the box under Participant, type part of the participant name that will match all the records you want to merge, and hit Enter:\n\nTick the records you want to merge together.\nPress Merge Participants. \nMerging cannot be undone, so double-check that the records are the correct ones, and untick any you don’t want to be merged together.\nChange the name to the final name/ID for the participant.\nIf there are participant attributes with conflicting values, the conflicts are also listed; choose the correct final value for each conflict.\nPress Merge\nYou will see a message:\n“This will merge all selected participants into a single new record. Existing attribute values will be assigned to the new participant. Transcripts for these participants will be assigned to the new participant. Automatic layer annotations will be lost and must be regenerated.\nThis can not be undone. Are you sure?”\nPress OK\nThe new record is created, and the number of corpora, transcripts, turns, and lines affected is shown\nIf you would like to double-check, or tweak, the new participant record, press the Edit … link.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023-2024 NZILBB"
  },
  {
    "objectID": "howto/corpus-management/index.html",
    "href": "howto/corpus-management/index.html",
    "title": "Corpus Management",
    "section": "",
    "text": "Corpus Management\n↙ Please select a Corpus Management option from the menu on the left.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023-2024 NZILBB"
  },
  {
    "objectID": "howto/word-frequency/index.html",
    "href": "howto/word-frequency/index.html",
    "title": "Word Frequency",
    "section": "",
    "text": "LaBB-CAT can generate frequency data about your corpus; i.e. count the number of tokens of each word (type) that appears in you transcripts. LaBB-CAT can both\n\nGenerate a list of word types with the token count of each type, and\nTag each token in the corpus with its frequency (token count)\n\nTo do this:\n\nSelect the word layers menu option.\nYou will see a list of word layers\nAdd a new layer, with the following settings:\n\nLayer ID: frequency\nType: Number\nAlignment: None\nManager: Frequency Layer Manager\nGenerate: Always\nDescription: Count of tokens of the same type across all corpora.\n\nPress the New button\nYou will see the layer configuration form. Fill it in with the following details:\n\nSummary: Raw Count\nLayer to summarize: orthography\nScope of Summary: Database\nMain participants only: ticked\nFilter Layer: unticked\nWord pairs: unticked\nTranscript types: If you have a word-list or reading trascript type, un-tick it to ensure that your readings don’t make certain words over-represented in the frequency counts.\nAnnotate tokens: ticked\nIf you want more information about what these options mean, check the online help page by clicking the question-mark icon at the top right of the page. This will provide information about how to break down counts by corpus, by speaker, etc.\n\nPress Save\nPress Regenerate\nYou will see a progress bar moving across the page while the counts are being generated. When it is finished, you will see a message saying Layer complete.\n\nNow each word in each transcript is annotated with the count of the number of instances of that word with the corpus of the transcript. To see what that looks like:\n\nSelect the transcripts menu option.\nClick the name of the first transcript in the list.\nTick the frequency layer.\n\nWhen the transcript reloads, you will see that above each word is a number. That number is the number of times that word appears in the transcript’s corpus. e.g. if the word “and” has 1743 above it, that means that the word “and” appears in the corpus 1743 times.\n\n\n\nLines of text from transcript, where each word has a number above it representing its frequency\n\n\nThe new word tags are searchable and exportable into CSV results files, just like any other annotations; If you do any search, the CSV Export options dropdown now contains a frequency checkbox that allows you to include the word frequency of the matches as a column in the CSV file.\nIf you have already exported the CSV results previously, and want to insert frequencies into the existing CSV file, you can do this by using the uploads → insert data option.\nThe Frequency Layer Manager also keeps a word-list with token counts for each corpus:\n\nSelect the layer managers menu option.\nOn the Frequency Layer Manager row, press the Extensions button.\n(If you have multiple Frequency Layer Manager layers, you will have to select the layer you’re after from the list, and then press Select. If you have only one Frequency Layer Manager layer, this step is not necessary.)\nPress Export\nSave and open the resulting CSV file.\n\nYou will see an alphabetical list of all the distinct word types in your corpus, and next to each, a count of the number of tokens of that type.\nThis page can also be used to search for target words and list their frequencies directly on the page.\n\n\n\nA search for the pattern\n\n\nThe steps above will give you basic word-form counts across all your data. The Frequency Layer Manager can be used to calculate other frequencies too:\n\nStem/lemma frequencies can be computed if you have tagged each word token with its stem (e.g. using the Porter Stemmer Layer Manager or the CELEX Layer Manager); to do this, specify the stem or lemma layer generated by the other layer manager as the Layer to Summarize for the Frequency Layer Manager.\nIf you have several sub-corpora in your database, you can get frequencies by corpus, by selecting “Corpus” as the Scope of Summary.\nSimilarly you can get frequencies by speaker, to get information of each speaker’s vocabulary use, by selecting “Speaker” as the Scope of Summary.\nYou can compare frequencies in your corpus against a reference corpus, in order to identify unusually frequent or infrequent words, by selecting Keyness as the Summary option, and then selecting a reference corpus.\nFor this option to work, you must have frequencies from a reference corpus loaded as a dictionary into LaBB-CAT; e.g. if you have installed the CELEX Layer Manager, this includes word-form and lemma frequencies from reference corpora.\n\n\n\n\nKeyness ratings, compared to the Cobuild corpus, listing showing\n\n\n\n\nIf you want word tokens tagged with their individual frequencies, and also want a word-pair frequency list, simply tick the Word Pairs option when you create the word layer to get an extra word-pair list.\nThis does not tag word tokens with bigram frequencies, so you can’t see the word-pair frequencies in transcript, nor extract them as part of search results files. But it does keep a list of frequencies that can be downloaded separately.\nTo access the word pair frequency list:\n\nSelect the layer managers menu option.\nOn the Frequency Layer ManagerI row, press the Extensions button.\n(If you have multiple Frequency Layer Manager layers, you will have to select the layer you’re after from the list, and then click Select. If you have only one Frequency Layer Manager layer, this step is not necessary.)\nSelect the Token Pair Counts option:\n\nPress Export\nSave and open the resulting CSV file; you will see there are three columns:\n\nWord1 - the first word in the pair\nWord2 - the second word in the pair\n0 words between - the number of times Word1 is immediately followed by Word2\n\n\nNote that when configuring the layer, next to the Word Pairs checkbox, there’s a dropdown box with ‘adjacent’ selected by default. This option lets you get frequencies for word pairs that are further apart. e.g. if you select the ‘within 1 word’ option, then in addition to the Word1, Word2, and 0 words between columns in the CSV file, you’ll also get a 1 words between column, containing the number of times Word1 is followed by Word2 with one intervening word.\n\n\n\nIf you not only want bigram frequencies in a list, but you also want to token pairs themselves annotated with that bigram’s frequency, or you’re interested in frequencies of trigrams or larger word clusters, you can use the Frequency Layer Manager to tag multiple words with their n-gram frequencies.\nFor example, to tag bigrams with their frequencies:\n\nSelect the phrase layers option on the menu (because instead of tagging individual word tokens, we’re going to create annotations that cover multiple words within the same speaker turn).\nCreate a layer with the following characteristics\n\nLayer ID: bigram\nType: Number\nAlignment: Intervals\nManager: Frequency Layer Manager\nGenerate: Always\n\nOn the configuration page, use the following settings:\n\nSummary: Raw Count\nLayer to Summarize: orthrogphy (unless you’re interested in combinations of stems/lemmas, in which case select your stem/lemma layer)\nScope of Summary: Database\nMain participants only: ticked (unless you want to include interviewer speech or other incidental speakers)\nFilter Layer: unticked\nN-gram: ticked, and leave the bigram option selected\nUnder Transcript Types you may want to un-tick ‘word list’ or ‘reading’ transcript types, if you have them, in order to only include spontaneous speech\nAnnotate Tokens: ticked\n\nPress Save\nPress Regenerate\n\nOnce the annotation layer has been generated, if you open a transcript and tick the bigram layer you just created, you’ll see that each pair of words has been annotated with a number; the frequency of that bigram:\n\n\n\nBigram frequency annotation\n\n\nFor example, you can see that the bigram “first one” appears 7 times in this corpus.\nFor example, you can see that the bigram “first one” appears 7 times in this corpus.\nYou can see that each word token is covered by two annotations; for when it’s the first word in the bigram, and for when it’s the second word in the bigram. For example, the token “one” above is at the beginning of the “one being” bigram (labelled “1”), and at the end of the “first one” bigram (labelled “7”)."
  },
  {
    "objectID": "howto/word-frequency/index.html#word-pair-frequency-list",
    "href": "howto/word-frequency/index.html#word-pair-frequency-list",
    "title": "Word Frequency",
    "section": "",
    "text": "If you want word tokens tagged with their individual frequencies, and also want a word-pair frequency list, simply tick the Word Pairs option when you create the word layer to get an extra word-pair list.\nThis does not tag word tokens with bigram frequencies, so you can’t see the word-pair frequencies in transcript, nor extract them as part of search results files. But it does keep a list of frequencies that can be downloaded separately.\nTo access the word pair frequency list:\n\nSelect the layer managers menu option.\nOn the Frequency Layer ManagerI row, press the Extensions button.\n(If you have multiple Frequency Layer Manager layers, you will have to select the layer you’re after from the list, and then click Select. If you have only one Frequency Layer Manager layer, this step is not necessary.)\nSelect the Token Pair Counts option:\n\nPress Export\nSave and open the resulting CSV file; you will see there are three columns:\n\nWord1 - the first word in the pair\nWord2 - the second word in the pair\n0 words between - the number of times Word1 is immediately followed by Word2\n\n\nNote that when configuring the layer, next to the Word Pairs checkbox, there’s a dropdown box with ‘adjacent’ selected by default. This option lets you get frequencies for word pairs that are further apart. e.g. if you select the ‘within 1 word’ option, then in addition to the Word1, Word2, and 0 words between columns in the CSV file, you’ll also get a 1 words between column, containing the number of times Word1 is followed by Word2 with one intervening word."
  },
  {
    "objectID": "howto/word-frequency/index.html#n-gram-annotations",
    "href": "howto/word-frequency/index.html#n-gram-annotations",
    "title": "Word Frequency",
    "section": "",
    "text": "If you not only want bigram frequencies in a list, but you also want to token pairs themselves annotated with that bigram’s frequency, or you’re interested in frequencies of trigrams or larger word clusters, you can use the Frequency Layer Manager to tag multiple words with their n-gram frequencies.\nFor example, to tag bigrams with their frequencies:\n\nSelect the phrase layers option on the menu (because instead of tagging individual word tokens, we’re going to create annotations that cover multiple words within the same speaker turn).\nCreate a layer with the following characteristics\n\nLayer ID: bigram\nType: Number\nAlignment: Intervals\nManager: Frequency Layer Manager\nGenerate: Always\n\nOn the configuration page, use the following settings:\n\nSummary: Raw Count\nLayer to Summarize: orthrogphy (unless you’re interested in combinations of stems/lemmas, in which case select your stem/lemma layer)\nScope of Summary: Database\nMain participants only: ticked (unless you want to include interviewer speech or other incidental speakers)\nFilter Layer: unticked\nN-gram: ticked, and leave the bigram option selected\nUnder Transcript Types you may want to un-tick ‘word list’ or ‘reading’ transcript types, if you have them, in order to only include spontaneous speech\nAnnotate Tokens: ticked\n\nPress Save\nPress Regenerate\n\nOnce the annotation layer has been generated, if you open a transcript and tick the bigram layer you just created, you’ll see that each pair of words has been annotated with a number; the frequency of that bigram:\n\n\n\nBigram frequency annotation\n\n\nFor example, you can see that the bigram “first one” appears 7 times in this corpus.\nFor example, you can see that the bigram “first one” appears 7 times in this corpus.\nYou can see that each word token is covered by two annotations; for when it’s the first word in the bigram, and for when it’s the second word in the bigram. For example, the token “one” above is at the beginning of the “one being” bigram (labelled “1”), and at the end of the “first one” bigram (labelled “7”)."
  },
  {
    "objectID": "howto/index.html",
    "href": "howto/index.html",
    "title": "How-to",
    "section": "",
    "text": "How-to\nThese pages describe how to achieve various common tasks in LaBB-CAT\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023-2024 NZILBB"
  },
  {
    "objectID": "howto/phonemic-tagging/character-mapper.html",
    "href": "howto/phonemic-tagging/character-mapper.html",
    "title": "Phonemic Tagging using the Character Mapper",
    "section": "",
    "text": "The orthography of some languages is closely related to its phonology.\nFor example, in the case of Te Reo Māori, it’s possible to devise a relatively simple mapping from spelling to phonemes; most letters can represent themselves, with some letter clusters mapping to specific phonemes (e.g. “ng” → /N/, “wh” → /f/, etc.).\nLaBB-CAT’s Character Mapper layer manager can be used to define the details of such a mapping, in order to generate the phonemic transcription annotation layer.\n\n\nThe Character Mapper maps characters strings in the source layer to new characters strings to generate in the target layer. e.g. for mapping spelling to phonemic transcription.\nTo create a new layer with annotations from your dictionary:\n\nSelect the word layers option from the menu - this will display a list of all the word layers you already have in the database.\nAt the top of the list, there’s a blank form for creating a new layer - fill this form in:\n\nLayer ID - enter a one- or two-word description - e.g. phonemes\nType - select Phonological\nManager - select Character Mapper\nAlignment - select None (as these are simply tags on the orthographic words)\nGenerate - select Always\n\nPress the New button to create the layer\nYou will see the layer configuration page. Check the online help for explanations of all options.\n\nSource Layer - the layer from which annotations will have their characters matched.\nLanguage - a regular expression identifying the ISO 639 code of the language that this configuration should annotate, e.g. en-NZ for New Zealand English, de.* for any variety of German, or blank for any language at all.\nMappings - a list of mappings from source characters to destination characters. If a character (or sequence of characters) in the left column is found in the source layer, then the corresponding character(s) in the right column is saved in the destination layer. Mapping rows are processed in order, and once a match is found, the rest of the rows are ignored for that character.\nRows can be added using the “add” button on the right, or removed by selecting a mapping (by clicking in the source or destination box) and using the “remove” button.\nThe order of mappings can be changed by selecting a mapping and using the “up” button to move it up, or the “down” button to move it down.\nFor characters in the source layer that don’t match any character in the left column, you can either Copy them into the destination layer, or Ignore them (so they are not copied).\n\n\nPress Save\nPress Regenerate.\nYou will see a progress bar while the layer manager annotates all the transcripts that have already been uploaded.\n\nLaBB-CAT will then generate annotations for all the transcripts you already have in your database. If you have a lot of data, this may take a while.\nFrom now on, when you upload a new transcript, annotations will automatically be generated by using the mapping rules you specified."
  },
  {
    "objectID": "howto/phonemic-tagging/character-mapper.html#creating-a-phonemes-layer",
    "href": "howto/phonemic-tagging/character-mapper.html#creating-a-phonemes-layer",
    "title": "Phonemic Tagging using the Character Mapper",
    "section": "",
    "text": "The Character Mapper maps characters strings in the source layer to new characters strings to generate in the target layer. e.g. for mapping spelling to phonemic transcription.\nTo create a new layer with annotations from your dictionary:\n\nSelect the word layers option from the menu - this will display a list of all the word layers you already have in the database.\nAt the top of the list, there’s a blank form for creating a new layer - fill this form in:\n\nLayer ID - enter a one- or two-word description - e.g. phonemes\nType - select Phonological\nManager - select Character Mapper\nAlignment - select None (as these are simply tags on the orthographic words)\nGenerate - select Always\n\nPress the New button to create the layer\nYou will see the layer configuration page. Check the online help for explanations of all options.\n\nSource Layer - the layer from which annotations will have their characters matched.\nLanguage - a regular expression identifying the ISO 639 code of the language that this configuration should annotate, e.g. en-NZ for New Zealand English, de.* for any variety of German, or blank for any language at all.\nMappings - a list of mappings from source characters to destination characters. If a character (or sequence of characters) in the left column is found in the source layer, then the corresponding character(s) in the right column is saved in the destination layer. Mapping rows are processed in order, and once a match is found, the rest of the rows are ignored for that character.\nRows can be added using the “add” button on the right, or removed by selecting a mapping (by clicking in the source or destination box) and using the “remove” button.\nThe order of mappings can be changed by selecting a mapping and using the “up” button to move it up, or the “down” button to move it down.\nFor characters in the source layer that don’t match any character in the left column, you can either Copy them into the destination layer, or Ignore them (so they are not copied).\n\n\nPress Save\nPress Regenerate.\nYou will see a progress bar while the layer manager annotates all the transcripts that have already been uploaded.\n\nLaBB-CAT will then generate annotations for all the transcripts you already have in your database. If you have a lot of data, this may take a while.\nFrom now on, when you upload a new transcript, annotations will automatically be generated by using the mapping rules you specified."
  },
  {
    "objectID": "howto/phonemic-tagging/celex.html",
    "href": "howto/phonemic-tagging/celex.html",
    "title": "Phonemic Tagging with CELEX",
    "section": "",
    "text": "If you have access to the CELEX lexical database, you can integrate LaBB-CAT with it, allowing you to annotate words in your transcripts with data from CELEX - that can include:\n\nstandard phonemic transcriptions - e.g. “difference” → “dɪfrəns” or “dɪfərəns”\nmorphological information - e.g. “difference” → “different+ence”\npossible syntactic category - e.g. “difference” → “N”\nfrequency data\nlemma\nsyllable count\n\nIf you want to do force-aligment (to determine the start and end times of each phone within each word), you will need to start with standard phonemic transcriptions, which CELEX can provide.\nCELEX is essentially a lexical database, available in English, German, and Dutch (although the Dutch database is not yet supported by LaBB-CAT). You can purchase the database from the LDC, who provide a number of text files for each language. Once you’ve got these data files, you need to install LaBB-CAT’s CELEX layer manager, which loads the data from the text files into the LaBB-CAT database, and provides mechanisms for generating annotation layers from it.\nThe basic steps are:\n\nBuy CELEX from the LDC.\nSave the data files on the same computer than LaBB-CAT is installed on. If you have zip files, these must be unzipped.\nInstall the CELEX English layer manager (or the German one if your data is in German), providing the location of the files you saved in the previous step\nCreate a new word layer, managed by the CELEX English (or German) layer manager, to generate the annotations you want.\n\n\n\nThe CELEX lexical databases are available from the Linguistic Data Consortium (LDC), and are available for English, German, and Dutch. Currently integration with only the English and German databases is supported by LaBB-CAT.\nYou can buy the databases online from the LDC catalogue.\n\n\n\nFor each language, the database you receive from the LDC consists of a collection of plain text files, arranged in a set of folders. For the language you are going to install, these files, in their original folders, must be saved on to the same computer that the LaBB-CAT server is running on.\nFor example, if you’re going to install the English CELEX data, then you need to end up with a folder on your LaBB-CAT server called ENGLISH, which contains the folders called:\n\nECT\nEFL\nEFS\nEFW\nEML\nEMW\nEOL\nEOW\nEPL\nEPW\nESL\n\nEach of these subfolders will contain a file named after the subdirectory (e.g. in the ECT folder there’s a file called ECT.CD) and a file called README.\nIt doesn’t matter where the top level ENGLISH folder is saved, except that:\n\nit must be accessible to the LaBB-CAT application (so don’t save it in a private or read-protected location)\nyou have to know the path to the folder - e.g. if you’re using a Windows computer, and you save the ENGLISH folder on the C: drive inside a folder called Temp, then the path would be C:\\Temp\\ENGLISH\n\n\n\n\nThe CELEX English (or CELEX German or CELEX Dutch) layer manager is a LaBB-CAT module that handles the integration with the CELEX database. It does two tasks:\n\nWhen you install the layer manager, it reads all of the data from the CELEX files, and loads it into a relational database that is part of LaBB-CAT (So once you’ve installed the layer manager successfully, you can delete the original CELEX files if you want to, as LaBB-CAT doesn’t need them any more).\nAfter installation, the layer manager handles looking up relevant data from its database, and using it to generate annotations for words.\n\nTo install the layer manager:\n\nIn LaBB-CAT, select on the layer managers link on the menu.\nAt the bottom of the page, follow the List of layer managers that are not yet installed link.\nLook for CELEX English (or CELEX Dutch or CELEX German, depending on your needs) and press its Install button.\nYou will see a form that asks for some information. Mostly the values are already filled in, and you can leave the default values as they are. The one important field you must specify is the ENGLISH data folder (or GERMAN data folder). You must fill in the path to the folder here. For example if you’re using a Windows computer, and you saved the ENGLISH folder on the C: drive inside a folder called Temp, then the path you enter here should be C:\\Temp\\ENGLISH\nClick Install Layer Manager.\nDuring the installation, you will see a progress bar, and information about the files currently being loaded from the CELEX folders. This may take a few minutes. Once it’s finished, the CELEX Layer Manager help page will appear, telling you what to do next (or you can navigate back to this page, and follow the instructions below).\n\n\n\n\nThe CELEX layer manager can be configured to annotate word tokens in your transcripts with data found in the CELEX database. As these annotations are about individual words, the layer manager can be used for ‘word layers’ (only).\nTo create a new layer with CELEX annotations:\n\nSelect on the word layers menu option - this will display a list of all the word layers you already have in the database.\nAt the top of the list, there’s a blank form for creating a new layer - fill this form in:\n\n\nID - enter a one- or two-word description - e.g. phonemes\nlayer type - select the option that suits what you want the layer to contain. e.g. if it’s a layer of phonemic transcriptions, select Phonological, if it will be frequency data, select Number, and otherwise, select Text\nlayer manager - select CELEX English (or CELEX German or CELEX Dutch)\nalignment - select None (as these are simply tags on the orthographic words)\ngenerate - select Always\n\n\nPress the New button to create the layer.\nYou will see a form that allows you to specify what the layer should generate.\n\nSelect the Phonology option on the left to generate phonemic transcriptions.\nTick the Pronounce Event Override option; this means that if a particular token is annotated on the pronounce layer, that annotation will take precedence over any pronunciation that might be found in CELEX.\n\nPress Save\nPress Renegerate\n\nLaBB-CAT will then generate annotations for all the transcripts you already have in your database. If you have a lot of data, this may take a while.\nFrom now on, when you upload a new transcript, the CELEX annotations will automatically be generated for it.\n\n\n\nSometimes speakers start saying a word but don’t finish it. The CELEX layer manager includes some special handling for these case, so that it’s possible to tag them with a pronunciation even though the complete word is not uttered.\n\nFor a hesitation for which multiple syllables are uttered, these can be transcribed up to the point the speaker stops, and then a tilde ~ is used to indicate the word was cut off. Then the pronunciation of the word can be manually entered on the pronounce layer, and it will be copied to the CELEX layer as long as the Pronounce Event Override option is ticked.\ne.g. if you are using ELAN or Praat for transcripts, you can provide pronounce tags in square brackets, directly after the word with no intervening whitespace, directly in the transcript:\nhesi~[hEz@].\nIf you are using Transcriber for transcripts, Transcriber has a mechanism for adding pronunciations to word tokens; just use that.\nFor very short hesitations, where only one or teo sounds are uttered, simply transcribing a couple of letters followed by a ~ is often sufficient; for such very short hesitations, the CELEX Layer Manager will provide a likely pronunciation. e.g.\nhe~"
  },
  {
    "objectID": "howto/phonemic-tagging/celex.html#getting-celex",
    "href": "howto/phonemic-tagging/celex.html#getting-celex",
    "title": "Phonemic Tagging with CELEX",
    "section": "",
    "text": "The CELEX lexical databases are available from the Linguistic Data Consortium (LDC), and are available for English, German, and Dutch. Currently integration with only the English and German databases is supported by LaBB-CAT.\nYou can buy the databases online from the LDC catalogue."
  },
  {
    "objectID": "howto/phonemic-tagging/celex.html#saving-the-data-files",
    "href": "howto/phonemic-tagging/celex.html#saving-the-data-files",
    "title": "Phonemic Tagging with CELEX",
    "section": "",
    "text": "For each language, the database you receive from the LDC consists of a collection of plain text files, arranged in a set of folders. For the language you are going to install, these files, in their original folders, must be saved on to the same computer that the LaBB-CAT server is running on.\nFor example, if you’re going to install the English CELEX data, then you need to end up with a folder on your LaBB-CAT server called ENGLISH, which contains the folders called:\n\nECT\nEFL\nEFS\nEFW\nEML\nEMW\nEOL\nEOW\nEPL\nEPW\nESL\n\nEach of these subfolders will contain a file named after the subdirectory (e.g. in the ECT folder there’s a file called ECT.CD) and a file called README.\nIt doesn’t matter where the top level ENGLISH folder is saved, except that:\n\nit must be accessible to the LaBB-CAT application (so don’t save it in a private or read-protected location)\nyou have to know the path to the folder - e.g. if you’re using a Windows computer, and you save the ENGLISH folder on the C: drive inside a folder called Temp, then the path would be C:\\Temp\\ENGLISH"
  },
  {
    "objectID": "howto/phonemic-tagging/celex.html#installing-the-layer-manager",
    "href": "howto/phonemic-tagging/celex.html#installing-the-layer-manager",
    "title": "Phonemic Tagging with CELEX",
    "section": "",
    "text": "The CELEX English (or CELEX German or CELEX Dutch) layer manager is a LaBB-CAT module that handles the integration with the CELEX database. It does two tasks:\n\nWhen you install the layer manager, it reads all of the data from the CELEX files, and loads it into a relational database that is part of LaBB-CAT (So once you’ve installed the layer manager successfully, you can delete the original CELEX files if you want to, as LaBB-CAT doesn’t need them any more).\nAfter installation, the layer manager handles looking up relevant data from its database, and using it to generate annotations for words.\n\nTo install the layer manager:\n\nIn LaBB-CAT, select on the layer managers link on the menu.\nAt the bottom of the page, follow the List of layer managers that are not yet installed link.\nLook for CELEX English (or CELEX Dutch or CELEX German, depending on your needs) and press its Install button.\nYou will see a form that asks for some information. Mostly the values are already filled in, and you can leave the default values as they are. The one important field you must specify is the ENGLISH data folder (or GERMAN data folder). You must fill in the path to the folder here. For example if you’re using a Windows computer, and you saved the ENGLISH folder on the C: drive inside a folder called Temp, then the path you enter here should be C:\\Temp\\ENGLISH\nClick Install Layer Manager.\nDuring the installation, you will see a progress bar, and information about the files currently being loaded from the CELEX folders. This may take a few minutes. Once it’s finished, the CELEX Layer Manager help page will appear, telling you what to do next (or you can navigate back to this page, and follow the instructions below)."
  },
  {
    "objectID": "howto/phonemic-tagging/celex.html#generating-annotations",
    "href": "howto/phonemic-tagging/celex.html#generating-annotations",
    "title": "Phonemic Tagging with CELEX",
    "section": "",
    "text": "The CELEX layer manager can be configured to annotate word tokens in your transcripts with data found in the CELEX database. As these annotations are about individual words, the layer manager can be used for ‘word layers’ (only).\nTo create a new layer with CELEX annotations:\n\nSelect on the word layers menu option - this will display a list of all the word layers you already have in the database.\nAt the top of the list, there’s a blank form for creating a new layer - fill this form in:\n\n\nID - enter a one- or two-word description - e.g. phonemes\nlayer type - select the option that suits what you want the layer to contain. e.g. if it’s a layer of phonemic transcriptions, select Phonological, if it will be frequency data, select Number, and otherwise, select Text\nlayer manager - select CELEX English (or CELEX German or CELEX Dutch)\nalignment - select None (as these are simply tags on the orthographic words)\ngenerate - select Always\n\n\nPress the New button to create the layer.\nYou will see a form that allows you to specify what the layer should generate.\n\nSelect the Phonology option on the left to generate phonemic transcriptions.\nTick the Pronounce Event Override option; this means that if a particular token is annotated on the pronounce layer, that annotation will take precedence over any pronunciation that might be found in CELEX.\n\nPress Save\nPress Renegerate\n\nLaBB-CAT will then generate annotations for all the transcripts you already have in your database. If you have a lot of data, this may take a while.\nFrom now on, when you upload a new transcript, the CELEX annotations will automatically be generated for it."
  },
  {
    "objectID": "howto/phonemic-tagging/celex.html#incomplete-words-and-hesitations",
    "href": "howto/phonemic-tagging/celex.html#incomplete-words-and-hesitations",
    "title": "Phonemic Tagging with CELEX",
    "section": "",
    "text": "Sometimes speakers start saying a word but don’t finish it. The CELEX layer manager includes some special handling for these case, so that it’s possible to tag them with a pronunciation even though the complete word is not uttered.\n\nFor a hesitation for which multiple syllables are uttered, these can be transcribed up to the point the speaker stops, and then a tilde ~ is used to indicate the word was cut off. Then the pronunciation of the word can be manually entered on the pronounce layer, and it will be copied to the CELEX layer as long as the Pronounce Event Override option is ticked.\ne.g. if you are using ELAN or Praat for transcripts, you can provide pronounce tags in square brackets, directly after the word with no intervening whitespace, directly in the transcript:\nhesi~[hEz@].\nIf you are using Transcriber for transcripts, Transcriber has a mechanism for adding pronunciations to word tokens; just use that.\nFor very short hesitations, where only one or teo sounds are uttered, simply transcribing a couple of letters followed by a ~ is often sufficient; for such very short hesitations, the CELEX Layer Manager will provide a likely pronunciation. e.g.\nhe~"
  },
  {
    "objectID": "howto/phonemic-tagging/g2p.html",
    "href": "howto/phonemic-tagging/g2p.html",
    "title": "Phonemic Tagging with the G2P BAS Web Service",
    "section": "",
    "text": "The Bavarian Archive for Speech Signals (BAS), has kindly published a set of speech processing web services including one for phonemic transcription called G2P. You can use this service yourself directly, using your web browser, but LaBB-CAT also has a module for using it automatically, called the BAS Services Manager.\nNB: In order to function, your LaBB-CAT server must be able to connect to the internet.\nNB: Using G2P for phonological tagging requires LaBB-CAT to send your orthographic transcripts over the internet to a third party. Although point 3 of the BAS Web Services Terms of Service makes clear that uploaded data is deleted after 24 hours, using the service is only suitable in situations in which you have consent from participants to do so.\nYou can use G2P for forced alignment if your speech is in any of the following languages:\n\nAlbanian\nAustralian Aboriginal Languages\nAfrikaans\nAlbanian\nBasque\nCatalan\nDutch\nEnglish\nEstonian\nFinnish\nFrench\nGeorgian\nGerman\nHungarian\nItalian\nJapanese\nKunwinjku\nLuxembourgish\nMaltese\nNorwegian\nPolish\nRomanian\nRussian\nSpanish\nSwedish\nYolŋu Matha\n\nLaBB-CAT must be able to identify which language each transcript is in, so you must ensure the language is set either\n\nin the transcript’s Language transcript attribute, or\non the corpora page (where you can define the language for all transcripts each corpus).\n\nThe available language options can be set in LaBB-CAT by going to the transcript attributes page and clicking the Options button of the “language” attribute. The value must be a two-letter ISO639-1 code optionally appended with a two-letter country code - e.g. “en” or “en-NZ”.\n\n\n\nSelect the word layers option on the menu - this will display a list of all the word layers you already have in the database.\nAt the bottom of the list, there’s a blank form for creating a new layer - fill this form in:\n\nLayer ID - enter something like phonemes\nType - select Phonological (Or Text if you don’t want to use “DISC” encoding; see below)\nManager - select BAS Web Services Manager\nAlignment - select None (as these are simply tags on the orthographic words)\nGenerate - select Always\n\nPress the New button to create the layer\nYou will see a form that allows you to configure the layer; check the online help for that page to guide you.\nOptions are:\n\nPhoneme Encoding - the encoding of the phonemes, which includes all of the options supported by G2P, plus “DISC” which, if selected, invokes G2P with “sampa” as the encoding option, and then converts the result to CELEX’s DISC encoding, which uses exactly one character per phoneme. The “DISC” option is recommended if the layer has its type set to “phonological”.\nWord Stress - prefix stressed vowels with a stress marker\nSyllabification - include syllable boundary markers in the transcriptions.\n\n\nPress Save\nPress Regenerate\n\nLaBB-CAT will then generate annotations for all the transcripts you already have in your database. If you have a lot of data, this may take a while.\nFrom now on, when you upload a new transcript, the G2P annotations will automatically be generated for it."
  },
  {
    "objectID": "howto/phonemic-tagging/g2p.html#using-g2p-for-phonemic-transcription",
    "href": "howto/phonemic-tagging/g2p.html#using-g2p-for-phonemic-transcription",
    "title": "Phonemic Tagging with the G2P BAS Web Service",
    "section": "",
    "text": "Select the word layers option on the menu - this will display a list of all the word layers you already have in the database.\nAt the bottom of the list, there’s a blank form for creating a new layer - fill this form in:\n\nLayer ID - enter something like phonemes\nType - select Phonological (Or Text if you don’t want to use “DISC” encoding; see below)\nManager - select BAS Web Services Manager\nAlignment - select None (as these are simply tags on the orthographic words)\nGenerate - select Always\n\nPress the New button to create the layer\nYou will see a form that allows you to configure the layer; check the online help for that page to guide you.\nOptions are:\n\nPhoneme Encoding - the encoding of the phonemes, which includes all of the options supported by G2P, plus “DISC” which, if selected, invokes G2P with “sampa” as the encoding option, and then converts the result to CELEX’s DISC encoding, which uses exactly one character per phoneme. The “DISC” option is recommended if the layer has its type set to “phonological”.\nWord Stress - prefix stressed vowels with a stress marker\nSyllabification - include syllable boundary markers in the transcriptions.\n\n\nPress Save\nPress Regenerate\n\nLaBB-CAT will then generate annotations for all the transcripts you already have in your database. If you have a lot of data, this may take a while.\nFrom now on, when you upload a new transcript, the G2P annotations will automatically be generated for it."
  },
  {
    "objectID": "howto/phonemic-tagging/index.html",
    "href": "howto/phonemic-tagging/index.html",
    "title": "Phonemic Tagging",
    "section": "",
    "text": "Phonemic Tagging\nDepending on your speech data, there are several ways to obtain phonemic transcriptions for words:\n\nLexical tagging\n\nCELEX - for British English, German, Dutch, using one of the CELEX layer managers.\nCMU Pronouncing Dictionary - for US English, using th CMU Pronouncing Dictionary layer manager.\nUnisyn - for various English varieties, using the Unisyn layer manager.\nDefine your own lexicon, and use the Flat File Dictionary layer manager to integrate it into LaBB-CAT.\n\nInferring pronunciation from orthography\n\nSpanish, using the Spanish Phonological Transcriber layer manager\nBas Web Service: G2P - for various languages.\nDefine your own simple mapping rules from orthography to phonology, using the Character Mapper layer manager.\n\n\nIf the speech corpus includes data in more than one language, it is possible to ensure that the utterances are phonemically tagged in a way that’s sensitive to the language of the specific utterance, using the language layers and attributes, and auxiliary layer managers.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023-2024 NZILBB"
  },
  {
    "objectID": "howto/phonemic-tagging/flat-file.html",
    "href": "howto/phonemic-tagging/flat-file.html",
    "title": "Phonemic Tagging using a Plain Text File",
    "section": "",
    "text": "The Flat Lexicon layer manager annotates words with data from a dictionary loaded from a plain text file (e.g. a CSV file).\nThe dictionary file you supply may contain multiple fields, and multiple entries per word. It might include:\n\nword orthography\nlemma\npart-of-speech\npronunciation\nfrequency\n\n…or any other “type” data you like.\n\n\nWhat dictionary file you want depends on what you want to annotate. For pronunciations, you might download some standard dictionary for your target language, such as Unisyn, the CMU Pronouncing dictionary, CELEX, etc. (although there are also specialised layer managers for these particular lexicons). Frequency lists include CELEX, SubtlexUS, and Adam Kilgarriff’s BNC Frequency Lists.\nAlternatively, you might have, or prepare, your own dictionary containing pronunciations, lemmata, etc. All you need is a CSV file with a column that includes the word orthography, and other columns that include the pronunciation and any other information you may have.\n\n\n\nExample of a custom pronunciation dictionary\n\n\nNB LaBB-CAT assumes that the text file uses ASCII or UTF-8 character encoding. If your dictionary file uses another encoding (e.g. “Western” or ISO-8859, you will need to re-save the file using UTF-8 (in many text editors, the character encoding is an option available when you select “Save As…” from the “File” menu).\n\n\n\nOnce you have a CSV or other text file, you need to upload it into LaBB-CAT:\n\nSelect the layer managers option in the LaBB-CAT menu.\nFind “Flat Lexicon Tagger” in the list and press its Extensions button.\nPress Choose File and select your dictionary file.\nYou many decide to change the default “Name” that the lexicon will have.\nThe default file structure options will probably be correct, but you may change them if you need to - see the page’s online help for details.\nPress Load.\n\nYou can upload as many dictionaries as you like. Once you have at least one dictionary, you can configure a word layer to lookup the resulting lexicons, by selecting “Flat Lexicon Tagger” as the layer’s layer manager.\n\n\n\nTo create a new layer with annotations from your dictionary:\n\nSelect the word layers option on the menu - this will display a list of all the word layers you already have in the database.\nAt the top of the list, there’s a blank form for creating a new layer - fill this form in:\n\nLayer ID - enter a one- or two-word description - e.g. phonemes\nType - If your dictionary uses CELEX DISC symbols that are not space delimited, select Phonological, otherwise (e.g. space-delimited IPA or ARPABET pronunciations) select Text\nAlignment - select None (as these are simply tags on the orthographic words)\nManager - select Flat Lexicon Tagger\nGenerate - select Always\nDescription - enter a description of the layer - e.g. Pronunciation (text-file)\n\nPress the New button to create the layer.\nYou will see the layer configuration page. Check the online help for explanations of all options, but at least:\nEnsure the Source Layer is orthography\nSelect the desired Lexicon from the list (these relate to the file or files you uploaded above).\n\nPress Save\nPress Regenerate.\nYou will see a progress bar while the layer manager annotates all the transcripts that have already been uploaded.\n\nLaBB-CAT will then generate annotations for all the transcripts you already have in your database. If you have a lot of data, this may take a while.\nOnce this is finished, be sure to open a transcript and tick the new phonemic tagging layer you just added, and make sure that each word is tagged with a corresponding pronunciation.\nFrom now on, when you upload a new transcript, annotations will automatically be generated by lookup up your lexicon."
  },
  {
    "objectID": "howto/phonemic-tagging/flat-file.html#getting-a-dictionary-file",
    "href": "howto/phonemic-tagging/flat-file.html#getting-a-dictionary-file",
    "title": "Phonemic Tagging using a Plain Text File",
    "section": "",
    "text": "What dictionary file you want depends on what you want to annotate. For pronunciations, you might download some standard dictionary for your target language, such as Unisyn, the CMU Pronouncing dictionary, CELEX, etc. (although there are also specialised layer managers for these particular lexicons). Frequency lists include CELEX, SubtlexUS, and Adam Kilgarriff’s BNC Frequency Lists.\nAlternatively, you might have, or prepare, your own dictionary containing pronunciations, lemmata, etc. All you need is a CSV file with a column that includes the word orthography, and other columns that include the pronunciation and any other information you may have.\n\n\n\nExample of a custom pronunciation dictionary\n\n\nNB LaBB-CAT assumes that the text file uses ASCII or UTF-8 character encoding. If your dictionary file uses another encoding (e.g. “Western” or ISO-8859, you will need to re-save the file using UTF-8 (in many text editors, the character encoding is an option available when you select “Save As…” from the “File” menu)."
  },
  {
    "objectID": "howto/phonemic-tagging/flat-file.html#installing-a-dictionary-file",
    "href": "howto/phonemic-tagging/flat-file.html#installing-a-dictionary-file",
    "title": "Phonemic Tagging using a Plain Text File",
    "section": "",
    "text": "Once you have a CSV or other text file, you need to upload it into LaBB-CAT:\n\nSelect the layer managers option in the LaBB-CAT menu.\nFind “Flat Lexicon Tagger” in the list and press its Extensions button.\nPress Choose File and select your dictionary file.\nYou many decide to change the default “Name” that the lexicon will have.\nThe default file structure options will probably be correct, but you may change them if you need to - see the page’s online help for details.\nPress Load.\n\nYou can upload as many dictionaries as you like. Once you have at least one dictionary, you can configure a word layer to lookup the resulting lexicons, by selecting “Flat Lexicon Tagger” as the layer’s layer manager."
  },
  {
    "objectID": "howto/phonemic-tagging/flat-file.html#creating-a-phonemes-layer",
    "href": "howto/phonemic-tagging/flat-file.html#creating-a-phonemes-layer",
    "title": "Phonemic Tagging using a Plain Text File",
    "section": "",
    "text": "To create a new layer with annotations from your dictionary:\n\nSelect the word layers option on the menu - this will display a list of all the word layers you already have in the database.\nAt the top of the list, there’s a blank form for creating a new layer - fill this form in:\n\nLayer ID - enter a one- or two-word description - e.g. phonemes\nType - If your dictionary uses CELEX DISC symbols that are not space delimited, select Phonological, otherwise (e.g. space-delimited IPA or ARPABET pronunciations) select Text\nAlignment - select None (as these are simply tags on the orthographic words)\nManager - select Flat Lexicon Tagger\nGenerate - select Always\nDescription - enter a description of the layer - e.g. Pronunciation (text-file)\n\nPress the New button to create the layer.\nYou will see the layer configuration page. Check the online help for explanations of all options, but at least:\nEnsure the Source Layer is orthography\nSelect the desired Lexicon from the list (these relate to the file or files you uploaded above).\n\nPress Save\nPress Regenerate.\nYou will see a progress bar while the layer manager annotates all the transcripts that have already been uploaded.\n\nLaBB-CAT will then generate annotations for all the transcripts you already have in your database. If you have a lot of data, this may take a while.\nOnce this is finished, be sure to open a transcript and tick the new phonemic tagging layer you just added, and make sure that each word is tagged with a corresponding pronunciation.\nFrom now on, when you upload a new transcript, annotations will automatically be generated by lookup up your lexicon."
  },
  {
    "objectID": "howto/phonemic-tagging/multilingual-corpora.html",
    "href": "howto/phonemic-tagging/multilingual-corpora.html",
    "title": "Phonemic Tagging: Multilingual Corpora",
    "section": "",
    "text": "Phonemic Tagging: Multilingual Corpora\nIf the speech corpus includes data in more than one language, it is possible to ensure that the utterances are phonemically tagged in a way that’s sensitive to the language of the specific utterance.\nThe layer manager modules that phonemically transcribe the data can be configured to annotate only words that are in the language targeted for that specific module, using the language code (e.g. “mi” for Te Reo Māori, “en” for English, “en-NZ” for New Zealand English, etc.).\nEach annotation layer is usually managed by a single layer manager, but it’s possible to have extra ‘Auxiliary Layer Managers’ configured for each layer. So you can have a single phonemes layer that contains all phonemic transcriptions, regardless of the language of the data; e.g. you might have the layer with\n\nthe CELEX English layer manager as the primary layer manager, targeting only English utterances, plus\nthe CELEX German layer manager as an auxiliary, targeting only German utterances, and\nthe Character Mapperg layer manager as an auxiliary, configured to target only utterances in Te Reo Māori with orthography-to-phonology mappings.\n\nLaBB-CAT has three mechanisms for determining the language of each word token in the corpus:\n\nIf the word is enclosed in an annotation on the language phrase layer, then the annotation’s label determines the language of that token. The language meta layer is a time-span layer that allows spans of words to be marked as being in a specific language.\nOtherwise, the transcript’s language transcript attribute is used to determine the language.\nIf the language transcript attribute is unset, then the language of the corpus the transcript is in is used to determine the language.\n\nUsing these mechanisms, it’s possible to ensure that each token is labelled with the correct phonemic transcription, even if the corpus contains multiple languages, and even if there are multiple languages within the same transcript.\n\n\n\nA transcript in Te Reo Māori, with English words annotated on the language phrase layer\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023-2024 NZILBB"
  },
  {
    "objectID": "howto/phonemic-tagging/cmudict.html",
    "href": "howto/phonemic-tagging/cmudict.html",
    "title": "Phonemic Tagging with CMU Pronouncing Dictionary",
    "section": "",
    "text": "Phonemic Tagging with CMU Pronouncing Dictionary\nLaBB-CAT can be integrated with the CMU Pronouncing Dictionary, which is a free pronouncing dictionary of English maintained by the Speech Group in the School of Computer Science at Carnegie Mellon University. The pronunciations are based on American English, so are suitable for American English recordings.\nIntegrating this lexicon with LaBB-CAT is achieved with the “CMU Dictionary Layer Manager”. As CMU has kindly granted permission to freely distribute the dictionary file, you don’t need to download the file from the CMU site; it’s included in the layer manager that you will install.\nTo install this layer manager:\n\nSelect the layer managers menu option.\nFollow the List of layer managers that are not yet installed link near the bottom.\nFind “CMU Pronouncing Dictionary” in the list, and press its Install button.\nYou will see a progress bar while the layer manager loads the data from the dictionary file into the LaBB-CAT database. This will take a minute or so.\nOnce it’s finished, you will see a new window open with information about the CMU Pronouncing Dictionary layer manager. Reading this information page, you will see some instructions on how to create a pronunciation annotation layer.\n\nTo create a new layer with CMUdict annotations:\n\nSelect on the word layers menu option - this will display a list of all the word layers you already have in the database.\nAt the top of the list, there’s a blank form for creating a new layer - fill this form in:\n\nID - enter a one- or two-word description - e.g. phonemes\nlayer type - select Phonological\nlayer manager - select CMU Pronouncing Dictionary\nalignment - select None (as these are simply tags on the orthographic words)\ngenerate - select Always\n\nPress the New button to create the layer\nYou will see a form that allows you to specify various options. You can use the online help for that page to guide you, but the default options are probably fine to go ahead with.\nPress Save\nPress Regenerate.\nYou will see a progress bar while the layer manager annotates all the transcripts that have already been uploaded.\n\nLaBB-CAT will then generate annotations for all the transcripts you already have in your database. If you have a lot of data, this may take a while.\nFrom now on, when you upload a new transcript, the CMUdict annotations will automatically be generated for it.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023-2024 NZILBB"
  },
  {
    "objectID": "howto/phonemic-tagging/spanish.html",
    "href": "howto/phonemic-tagging/spanish.html",
    "title": "Phonemic Tagging with the Spanish Phonology Transcriber",
    "section": "",
    "text": "This layer manager annotates Spanish words their phonemic transcription, based on rules that map orthography to phonology.\nRule-based conversion is based on spanish-pronunciation-rules-php, an open-source PHP function that converts a Spanish word into IPA phonetic transcription symbols, written by Timur Baytukalov, http://easypronunciation.com/en/\n\n\nTo create a new layer with annotations from your dictionary:\n\nSelect the word layers option on the menu - this will display a list of all the word layers you already have in the database.\nAt the top of the list, there’s a blank form for creating a new layer - fill this form in:\n\nLayer ID - enter a one- or two-word description - e.g. phonemes\nType - select Phonological\nManager - select Spanish phonological transcriber\nAlignment - select None (as these are simply tags on the orthographic words)\nGenerate - select Always\n\nPress the New button to create the layer. You will see the layer configuration page. Check the online help for explanations of all options, but at least:\nEnsure the Source Layer is orthography\nSelect the desired Locale from the list.\n\nPress Save\nPress Regenerate.\nYou will see a progress bar while the layer manager annotates all the transcripts that have already been uploaded.\n\nLaBB-CAT will then generate annotations for all the transcripts you already have in your database. If you have a lot of data, this may take a while.\nFrom now on, when you upload a new transcript, annotations will automatically be generated by using the mapping rules for the selected locale."
  },
  {
    "objectID": "howto/phonemic-tagging/spanish.html#creating-a-phonemes-layer",
    "href": "howto/phonemic-tagging/spanish.html#creating-a-phonemes-layer",
    "title": "Phonemic Tagging with the Spanish Phonology Transcriber",
    "section": "",
    "text": "To create a new layer with annotations from your dictionary:\n\nSelect the word layers option on the menu - this will display a list of all the word layers you already have in the database.\nAt the top of the list, there’s a blank form for creating a new layer - fill this form in:\n\nLayer ID - enter a one- or two-word description - e.g. phonemes\nType - select Phonological\nManager - select Spanish phonological transcriber\nAlignment - select None (as these are simply tags on the orthographic words)\nGenerate - select Always\n\nPress the New button to create the layer. You will see the layer configuration page. Check the online help for explanations of all options, but at least:\nEnsure the Source Layer is orthography\nSelect the desired Locale from the list.\n\nPress Save\nPress Regenerate.\nYou will see a progress bar while the layer manager annotates all the transcripts that have already been uploaded.\n\nLaBB-CAT will then generate annotations for all the transcripts you already have in your database. If you have a lot of data, this may take a while.\nFrom now on, when you upload a new transcript, annotations will automatically be generated by using the mapping rules for the selected locale."
  },
  {
    "objectID": "howto/phonemic-tagging/unisyn.html",
    "href": "howto/phonemic-tagging/unisyn.html",
    "title": "Phonemic Tagging with the Unisyn lexicon",
    "section": "",
    "text": "LaBB-CAT includes the Unisyn layer manager, which is designed for ingesting Unisyn accent-specific lexicons. Unisyn must be downloaded separately, and the included scripts executed to produce a lexicon for the desired variety. The resulting file can be added to LaBB-CAT, and then the layer manager can be configured to use it for tagging word tokens with their phonemic transcriptions.\nUnisyn is a ‘master lexicon’ of English, which contains:\n\northography\npart-of-speech\npronunciation, in an ‘accent neutral’ form\n‘enriched orthography’ showing morphological information\nfrequency, as derived from various sources, including the British National Corpus, Time articles, Gutenberg, etc.\n\nThe pronunciations in the lexicon can be converted into an accent-specific form using perl scripts that are included with the lexicon.\n\n\nUnisyn is available under a non-commercial license, and must be acquired seperately from this layer manager. To acquire Unisyn, you must first register on the the Unisyn website and accept the terms of their license. The Unisyn website is here:\nhttp://www.cstr.ed.ac.uk/projects/unisyn/\n(This layer manager has been tested with version 1.3 of Unisyn)\n\n\n\nOnce you’ve got Unisyn, you can use it to produce accent-specific lexicons, and provide these lexicons to the layer manager, which then uses them to annotate words in LaBB-CAT.\nFor example, if you want to annotate your transcripts with ‘General American English’ pronunciations:\n\nGenerate the General American English (gam) lexicon by running the following Unisyn commands:\n\nget-exceptions.pl -a gam -f unilex &gt; gam.1\npost-lex-rules.pl -a gam -f gam.1 &gt; gam.2\nmap-unique.pl -a gam -f gam.2 &gt; gam.unisyn. This gives you the file gam.unisyn, which is the lexicon file you need for the next step.\n\nUpload the accent-specific lexicon into LaBB-CAT:\n\nselect layer managers on the menu\nfind the Unisyn layer manager in the list, and click the extensions button (the second-to-last button on the right)\npress Choose File and select the gam.unisyn file you generated above.\npress Upload.\n\nCreate the layer for your pronunciation annotations:\n\nselect word layers on the menu\nadd a new layer called something like “GAM Pronunciations”, selecting Unisyn as the layer manager, and Phonological as the layer type.\ncheck the settings, but most likely the default options are correct, so press Save\n\n\n\n\n\nLaBB-CAT’s processing of phonological layers assumes that the annotations use the DISC phoneme set designed for the CELEX phonemic transcriptions. This set is used because each phoneme is expressed by precisely one ASCII character, including phonemes usually expressed using a digraph - e.g. affricates like /tʃ/ (which is /J/ in DISC) and diphthongs like /aɪ/ (which is /2/ in DISC)\nUnisyn transcriptions use a set of phones that is greater that the set of phones available in DISC, and the transcriptions are designed to be broadly phonetic, not phonemic.\nThis means that using the DISC representation of the transcripts is imperfect, as there is a certain amount of loss of information when mapping Unisyn phones to DISC phonemes. The mapping that is used is shown below.\n\n\n\nUnisyn\n\nDISC\nIPA\nLexical set e.g.\n\n\n\n\nAh\n→\n#\nⱭ:\nBATH\n\n\naa\n→\nQ\nɒ\nPALM → LOT\n\n\nar\n→\nQ\nɒ\nstart → PALM → LOT\n\n\noa\n→\n{\næ\nBANANA → TRAP\n\n\nao\n→\n#\nɑ:\nMAZDA → BATH\n\n\ne\n→\nE\nε\nDRESS\n\n\ner\n→\nE\nε\nr-coloured DRESS in scots en\n\n\na\n→\n{\næ\nTRAP\n\n\neh\n→\n{\næ\nann use TRAP\n\n\nou\n→\n5\nəʊ\nGOAT - but a monophthong for in some varieties\n\n\noul\n→\n5\nəʊ\ngoal - post vocalic GOAT\n\n\nouw\n→\n5\nəʊ\nKNOW → GOAT (except for Abergave)\n\n\no\n→\nQ\nɒ\nLOT\n\n\noou\n→\nQ\nɒ\nadios → LOT\n\n\nau\n→\nQ\nɒ\nCLOTH → LOT (but a diphthong in some en-US)\n\n\noo\n→\n$\nɔ:\nTHOUGHT (but a diphthong in some varieties)\n\n\nor\n→\n$\nɔ:\nr-coloured THOUGHT\n\n\nii\n→\ni\ni:\nFLEECE\n\n\niy\n→\ni\ni:\nHAPPY - I for some varieties\n\n\nie\n→\ni\ni:\nHARRIET - Leeds only\n\n\nii;\n→\ni\ni:\nAGREED → FLEECE\n\n\nir\n→\ni\ni:\nNEARING - r-coloured NEAR → FLEECE\n\n\nir;\n→\ni\ni:\nnear - scots-long NEAR → FLEECE\n\n\ni\n→\nI\nɪ\nKIT\n\n\n@\n→\n@\nə\nschwa\n\n\n@r\n→\n@\nə\nr-coloured schwa\n\n\nuh\n→\nV\nʌ\nSTRUT\n\n\nu\n→\nU\nʊ\nFOOT\n\n\nuu\n→\nu\nu:\nGOOSE\n\n\niu\n→\nu\nu:\nBLEW → GOOSE\n\n\nuu;\n→\nu\nu:\nbrewed → GOOSE\n\n\nuw\n→\nu\nu:\nlouise → GOOSE\n\n\nuul\n→\nu\nu:\ngoul - post-vocalic GOOSE\n\n\nei\n→\n1\neɪ\nFACE\n\n\nee\n→\n1\neɪ\nWASTE → FACE (except for abercrave)\n\n\nai\n→\n2\naɪ\nPRICE\n\n\nae\n→\n2\naɪ\nTIED → PRICE (except Edi and Aberdeen)\n\n\nae\n→\n2\naɪ\nTIED → PRICE (except Edi and Aberdeen)\n\n\naer\n→\n2\naɪ\nFIRE - r-coloured PRICE\n\n\naai\n→\n2\naɪ\nTIME → PRICE (except S. Carolina)\n\n\noir\n→\n2\naɪ\nCOIR - r-coloured PRICE\n\n\n@@r\n→\n3\nɜ:\nNURSE\n\n\noi\n→\n4\nɔɪ\nCHOICE\n\n\now\n→\n6\naʊ\nMOUTH\n\n\nowr\n→\n6\naʊ\nHOUR - r-coloured MOUTH\n\n\noow\n→\n6\naʊ\nHOUR → MOUTH (exception S. Carolina)\n\n\ni@\n→\n7\nɪə\nNEAR\n\n\niir\n→\n7\nɪə\nbeard → NEAR (except en-AU)\n\n\neir\n→\n8\nεə\nSQUARING (actually a monophthong in many varieties)\n\n\nur\n→\n9\nʊə\nJURY\n\n\nur;\n→\n9\nʊə\nCURE - scots-long JURY\n\n\niur\n→\n9\nʊə\ncurious - JURY exception in Cardiff & Abercrave\n\n\np\n→\np\np\n\n\n\nt\n→\nt\nt\n\n\n\n?\n→\n?\nʔ\n(glottal stop)\n\n\nt^\n→\nL\nɾ\nbutter/merry flap\n\n\nk\n→\nk\nk\n\n\n\nx\n→\nx\nx\nloch\n\n\nb\n→\nb\nb\n\n\n\nd\n→\nd\nd\n\n\n\ng\n→\ng\ng\n\n\n\nch\n→\nJ\nʧ\n\n\n\njh\n→\n_\nʤ\n\n\n\ns\n→\ns\ns\n\n\n\nz\n→\nz\nz\n\n\n\nsh\n→\nS\nʃ\n\n\n\nzh\n→\nZ\nʒ\n\n\n\nf\n→\nf\nf\n\n\n\nv\n→\nv\nv\n\n\n\nth\n→\nT\nθ\n\n\n\ndh\n→\nD\nð\n\n\n\nh\n→\nm\nm\n\n\n\nm\n→\nm\nm\n\n\n\nm!\n→\nF\nm̩\nchasm\n\n\nn\n→\nn\nn\n\n\n\nn!\n→\nH\nn̩\nmission\n\n\nng\n→\nN\nŋ\n\n\n\nl\n→\nl\nl\n\n\n\nll\n→\nl\nl\nllandudno (for Cardiff and Abercrave, this is different)\n\n\nlw\n→\nl\nl\nfeel - dark l\n\n\nr\n→\nr\nr\n\n\n\ny\n→\nj\nj\n\n\n\nw\n→\nw\nw\nwhich\n\n\nhw\n→\nw\nw\nwhich\n\n\n\nIf having the original transcriptions precisely as defined in the Unisyn lexicon is very important, you can instead create a layer that uses the original transcription as contained in the file you uploaded. This has the advantage that the transcriptions are not filtered through the above mapping, and the disadvantage that LaBB-CAT won’t be able to display the transcriptions using IPA symbols, nor help you when creating search patterns for the layer.\nIf you decide to do this, Unisyn offers you two possible representations:\n\nUnisyn transcriptions - e.g. { p r @ . n ~ uh n s $}.&gt; ii . * ei . sh n! &gt; - these are already present in the file that you generated if you followed the instructions above (i.e. gam.unisyn)\nSAM-PA transcriptions - e.g. pr\\@%nVns$i\"e$Sn=$@5 - these can be obtained by running an extra Unisyn command, and uploading the resulting gam.sampa file:\noutput-sam.pl -a gam -f gam.unisyn &gt; gam.sampa\n\n(Unisyn has a third script called output-ipa.pl which produces transcriptions for displaying in HTML - e.g. pɹəˌnʌns.iˈe.ʃn ̩ - which are not suitable for search, analysis, or forced-alignment)\nIn order to prevent the DISC mapping from applying on your layer:\n\nWhen creating the layer, set the layer type to Text rather than Phonological.\nWhen configuring the layer, set the field to Phonemes (original file) rather than Phonemes (DISC).\n\n\n\n\nTo create a new layer with CMUdict annotations:\n\nSelect the word layers option on the menu - this will display a list of all the word layers you already have in the database.\nAt the top of the list, there’s a blank form for creating a new layer - fill this form in:\n\nID - enter a one- or two-word description - e.g. phonemes\nlayer type - select Phonological\nlayer manager - select Unisyn\nalignment - select None (as these are simply tags on the orthographic words)\ngenerate - select Always\n\nPress the New button to create the layer. You will see the layer configuration page. Check the online help for explanations of all options, but at least:\nEnsure the Source Layer is orthography\nSelect the desired Lexicon from the list (these relate to the file or files you generated and uploaded above)\nTick the Strip syllabification/stress if you will use this layer for forced alignment with HTK. \nPress Save\nPress Regenerate. You will see a progress bar while the layer manager annotates all the transcripts that have already been uploaded.\n\nLaBB-CAT will then generate annotations for all the transcripts you already have in your database. If you have a lot of data, this may take a while.\nFrom now on, when you upload a new transcript, the Unisyn annotations will automatically be generated for it."
  },
  {
    "objectID": "howto/phonemic-tagging/unisyn.html#getting-unisyn",
    "href": "howto/phonemic-tagging/unisyn.html#getting-unisyn",
    "title": "Phonemic Tagging with the Unisyn lexicon",
    "section": "",
    "text": "Unisyn is available under a non-commercial license, and must be acquired seperately from this layer manager. To acquire Unisyn, you must first register on the the Unisyn website and accept the terms of their license. The Unisyn website is here:\nhttp://www.cstr.ed.ac.uk/projects/unisyn/\n(This layer manager has been tested with version 1.3 of Unisyn)"
  },
  {
    "objectID": "howto/phonemic-tagging/unisyn.html#using-unisyn-with-this-layer-manager",
    "href": "howto/phonemic-tagging/unisyn.html#using-unisyn-with-this-layer-manager",
    "title": "Phonemic Tagging with the Unisyn lexicon",
    "section": "",
    "text": "Once you’ve got Unisyn, you can use it to produce accent-specific lexicons, and provide these lexicons to the layer manager, which then uses them to annotate words in LaBB-CAT.\nFor example, if you want to annotate your transcripts with ‘General American English’ pronunciations:\n\nGenerate the General American English (gam) lexicon by running the following Unisyn commands:\n\nget-exceptions.pl -a gam -f unilex &gt; gam.1\npost-lex-rules.pl -a gam -f gam.1 &gt; gam.2\nmap-unique.pl -a gam -f gam.2 &gt; gam.unisyn. This gives you the file gam.unisyn, which is the lexicon file you need for the next step.\n\nUpload the accent-specific lexicon into LaBB-CAT:\n\nselect layer managers on the menu\nfind the Unisyn layer manager in the list, and click the extensions button (the second-to-last button on the right)\npress Choose File and select the gam.unisyn file you generated above.\npress Upload.\n\nCreate the layer for your pronunciation annotations:\n\nselect word layers on the menu\nadd a new layer called something like “GAM Pronunciations”, selecting Unisyn as the layer manager, and Phonological as the layer type.\ncheck the settings, but most likely the default options are correct, so press Save"
  },
  {
    "objectID": "howto/phonemic-tagging/unisyn.html#mapping-unisyn-pronunciations-to-the-disc-phoneme-set",
    "href": "howto/phonemic-tagging/unisyn.html#mapping-unisyn-pronunciations-to-the-disc-phoneme-set",
    "title": "Phonemic Tagging with the Unisyn lexicon",
    "section": "",
    "text": "LaBB-CAT’s processing of phonological layers assumes that the annotations use the DISC phoneme set designed for the CELEX phonemic transcriptions. This set is used because each phoneme is expressed by precisely one ASCII character, including phonemes usually expressed using a digraph - e.g. affricates like /tʃ/ (which is /J/ in DISC) and diphthongs like /aɪ/ (which is /2/ in DISC)\nUnisyn transcriptions use a set of phones that is greater that the set of phones available in DISC, and the transcriptions are designed to be broadly phonetic, not phonemic.\nThis means that using the DISC representation of the transcripts is imperfect, as there is a certain amount of loss of information when mapping Unisyn phones to DISC phonemes. The mapping that is used is shown below.\n\n\n\nUnisyn\n\nDISC\nIPA\nLexical set e.g.\n\n\n\n\nAh\n→\n#\nⱭ:\nBATH\n\n\naa\n→\nQ\nɒ\nPALM → LOT\n\n\nar\n→\nQ\nɒ\nstart → PALM → LOT\n\n\noa\n→\n{\næ\nBANANA → TRAP\n\n\nao\n→\n#\nɑ:\nMAZDA → BATH\n\n\ne\n→\nE\nε\nDRESS\n\n\ner\n→\nE\nε\nr-coloured DRESS in scots en\n\n\na\n→\n{\næ\nTRAP\n\n\neh\n→\n{\næ\nann use TRAP\n\n\nou\n→\n5\nəʊ\nGOAT - but a monophthong for in some varieties\n\n\noul\n→\n5\nəʊ\ngoal - post vocalic GOAT\n\n\nouw\n→\n5\nəʊ\nKNOW → GOAT (except for Abergave)\n\n\no\n→\nQ\nɒ\nLOT\n\n\noou\n→\nQ\nɒ\nadios → LOT\n\n\nau\n→\nQ\nɒ\nCLOTH → LOT (but a diphthong in some en-US)\n\n\noo\n→\n$\nɔ:\nTHOUGHT (but a diphthong in some varieties)\n\n\nor\n→\n$\nɔ:\nr-coloured THOUGHT\n\n\nii\n→\ni\ni:\nFLEECE\n\n\niy\n→\ni\ni:\nHAPPY - I for some varieties\n\n\nie\n→\ni\ni:\nHARRIET - Leeds only\n\n\nii;\n→\ni\ni:\nAGREED → FLEECE\n\n\nir\n→\ni\ni:\nNEARING - r-coloured NEAR → FLEECE\n\n\nir;\n→\ni\ni:\nnear - scots-long NEAR → FLEECE\n\n\ni\n→\nI\nɪ\nKIT\n\n\n@\n→\n@\nə\nschwa\n\n\n@r\n→\n@\nə\nr-coloured schwa\n\n\nuh\n→\nV\nʌ\nSTRUT\n\n\nu\n→\nU\nʊ\nFOOT\n\n\nuu\n→\nu\nu:\nGOOSE\n\n\niu\n→\nu\nu:\nBLEW → GOOSE\n\n\nuu;\n→\nu\nu:\nbrewed → GOOSE\n\n\nuw\n→\nu\nu:\nlouise → GOOSE\n\n\nuul\n→\nu\nu:\ngoul - post-vocalic GOOSE\n\n\nei\n→\n1\neɪ\nFACE\n\n\nee\n→\n1\neɪ\nWASTE → FACE (except for abercrave)\n\n\nai\n→\n2\naɪ\nPRICE\n\n\nae\n→\n2\naɪ\nTIED → PRICE (except Edi and Aberdeen)\n\n\nae\n→\n2\naɪ\nTIED → PRICE (except Edi and Aberdeen)\n\n\naer\n→\n2\naɪ\nFIRE - r-coloured PRICE\n\n\naai\n→\n2\naɪ\nTIME → PRICE (except S. Carolina)\n\n\noir\n→\n2\naɪ\nCOIR - r-coloured PRICE\n\n\n@@r\n→\n3\nɜ:\nNURSE\n\n\noi\n→\n4\nɔɪ\nCHOICE\n\n\now\n→\n6\naʊ\nMOUTH\n\n\nowr\n→\n6\naʊ\nHOUR - r-coloured MOUTH\n\n\noow\n→\n6\naʊ\nHOUR → MOUTH (exception S. Carolina)\n\n\ni@\n→\n7\nɪə\nNEAR\n\n\niir\n→\n7\nɪə\nbeard → NEAR (except en-AU)\n\n\neir\n→\n8\nεə\nSQUARING (actually a monophthong in many varieties)\n\n\nur\n→\n9\nʊə\nJURY\n\n\nur;\n→\n9\nʊə\nCURE - scots-long JURY\n\n\niur\n→\n9\nʊə\ncurious - JURY exception in Cardiff & Abercrave\n\n\np\n→\np\np\n\n\n\nt\n→\nt\nt\n\n\n\n?\n→\n?\nʔ\n(glottal stop)\n\n\nt^\n→\nL\nɾ\nbutter/merry flap\n\n\nk\n→\nk\nk\n\n\n\nx\n→\nx\nx\nloch\n\n\nb\n→\nb\nb\n\n\n\nd\n→\nd\nd\n\n\n\ng\n→\ng\ng\n\n\n\nch\n→\nJ\nʧ\n\n\n\njh\n→\n_\nʤ\n\n\n\ns\n→\ns\ns\n\n\n\nz\n→\nz\nz\n\n\n\nsh\n→\nS\nʃ\n\n\n\nzh\n→\nZ\nʒ\n\n\n\nf\n→\nf\nf\n\n\n\nv\n→\nv\nv\n\n\n\nth\n→\nT\nθ\n\n\n\ndh\n→\nD\nð\n\n\n\nh\n→\nm\nm\n\n\n\nm\n→\nm\nm\n\n\n\nm!\n→\nF\nm̩\nchasm\n\n\nn\n→\nn\nn\n\n\n\nn!\n→\nH\nn̩\nmission\n\n\nng\n→\nN\nŋ\n\n\n\nl\n→\nl\nl\n\n\n\nll\n→\nl\nl\nllandudno (for Cardiff and Abercrave, this is different)\n\n\nlw\n→\nl\nl\nfeel - dark l\n\n\nr\n→\nr\nr\n\n\n\ny\n→\nj\nj\n\n\n\nw\n→\nw\nw\nwhich\n\n\nhw\n→\nw\nw\nwhich\n\n\n\nIf having the original transcriptions precisely as defined in the Unisyn lexicon is very important, you can instead create a layer that uses the original transcription as contained in the file you uploaded. This has the advantage that the transcriptions are not filtered through the above mapping, and the disadvantage that LaBB-CAT won’t be able to display the transcriptions using IPA symbols, nor help you when creating search patterns for the layer.\nIf you decide to do this, Unisyn offers you two possible representations:\n\nUnisyn transcriptions - e.g. { p r @ . n ~ uh n s $}.&gt; ii . * ei . sh n! &gt; - these are already present in the file that you generated if you followed the instructions above (i.e. gam.unisyn)\nSAM-PA transcriptions - e.g. pr\\@%nVns$i\"e$Sn=$@5 - these can be obtained by running an extra Unisyn command, and uploading the resulting gam.sampa file:\noutput-sam.pl -a gam -f gam.unisyn &gt; gam.sampa\n\n(Unisyn has a third script called output-ipa.pl which produces transcriptions for displaying in HTML - e.g. pɹəˌnʌns.iˈe.ʃn ̩ - which are not suitable for search, analysis, or forced-alignment)\nIn order to prevent the DISC mapping from applying on your layer:\n\nWhen creating the layer, set the layer type to Text rather than Phonological.\nWhen configuring the layer, set the field to Phonemes (original file) rather than Phonemes (DISC)."
  },
  {
    "objectID": "howto/phonemic-tagging/unisyn.html#creating-a-phonemes-layer",
    "href": "howto/phonemic-tagging/unisyn.html#creating-a-phonemes-layer",
    "title": "Phonemic Tagging with the Unisyn lexicon",
    "section": "",
    "text": "To create a new layer with CMUdict annotations:\n\nSelect the word layers option on the menu - this will display a list of all the word layers you already have in the database.\nAt the top of the list, there’s a blank form for creating a new layer - fill this form in:\n\nID - enter a one- or two-word description - e.g. phonemes\nlayer type - select Phonological\nlayer manager - select Unisyn\nalignment - select None (as these are simply tags on the orthographic words)\ngenerate - select Always\n\nPress the New button to create the layer. You will see the layer configuration page. Check the online help for explanations of all options, but at least:\nEnsure the Source Layer is orthography\nSelect the desired Lexicon from the list (these relate to the file or files you generated and uploaded above)\nTick the Strip syllabification/stress if you will use this layer for forced alignment with HTK. \nPress Save\nPress Regenerate. You will see a progress bar while the layer manager annotates all the transcripts that have already been uploaded.\n\nLaBB-CAT will then generate annotations for all the transcripts you already have in your database. If you have a lot of data, this may take a while.\nFrom now on, when you upload a new transcript, the Unisyn annotations will automatically be generated for it."
  },
  {
    "objectID": "howto/pos-tagging/stanford-pos-tagger.html",
    "href": "howto/pos-tagging/stanford-pos-tagger.html",
    "title": "Stanford POS Tagger",
    "section": "",
    "text": "Depending on the language of your transcripts, you may be able to tag each word with its part of speech (Noun, Verb, Adjective, etc.) using the Stanford POS Tagger.\nThe Stanford POS Tagger has models for:\n\nArabic\nChinese\nEnglish\nFrench\nGerman\nSpanish\n\nThe steps for POS tagging your corpus are:\n\nInstall the Layer Manager\nConfigure a POS layer\n\n\n\n\nIn LaBB-CAT, select the layer managers option on the menu at the top.\nAt the bottom, follow the link labelled: List of layer managers that are not yet installed.\nFind the StanfordPosTagger layer manager in the list, and press its Install button, then Install again.\nYou will see a configuration page with some information about the tagger.\nPress Configure.\nYou will see a progress bar while the layer manager downloads the Stanford POS Tagger files.\n\nOnce it’s finished, you’ll see a further information page.\n\n\n\nNow the layer manager is installed, we need to create a layer that is configured to use it to tag words with their part of speech…\n\nSelect word layers on the menu at the top.\nYou will see a list of word tag layers that have already been configured. The column headings at the top are also a form for creating a new layer, so we’ll fill in that form now.\nFill in the following details on the form at the top:\n\nLayer ID: pos\nType: Text\nAlignment: Intervals\nNB it’s important that this is not set to None because a single word can have multiple POS tags, one after another, which are strung between the start and the end of the word token.\nManager: Stanford POS Tagger\nGenerate: Always\nProject: This can be left as the default value, unless you want to add the layer to a category of your choice.\n\nPress New\nYou will see the layer configuration form. Mostly you can leave the default values as they are.\nSet the Model to use setting to something that makes sense for your transcripts, which depends on their language. This is a setting you may experiment with to get the best results. For English recordings, you may find the english-bidirectional-distsim.tagger is slower but produces better results.\nPress Set Parameters.\nNow press Regenerate to run the POS tagger on your whole corpus.\nYou will see a progress bar while the transcripts are being tagged.\nOnce it’s complete, select the transcripts option on the menu, and click the first transcript in the list.\nTick the new pos layer to display the tags.\nYou will see that each word has one or more tags above it - these identify the parts of speech or syntactic categories of the words.\n\n\nThe tags can be searched, extracted, summarised, etc. just like any other annotations.\n\n\n\nOne possible aggregated analysis might be to compute the distributions of POS tags for each speaker.\nIn order to do that, you would set up the tagger as above to output the POS tags, and then set up a Frequency Layer Manager layer with the POS tags as input. In order to do that:\n\nIn LaBB-CAT, select the word layers menu option.\nAdd a new word layer with the following characteristics:\n\nLayer ID: posFrequency\nType: Number\nAlignment: None\nManager: Frequency Layer Manager\nGenerate: Always\n\nPress New.\nYou will see the Frequency Layer Manager configuration form.\nConfigure the layer as follows:\n\nSummary: Raw Count\nLayer to Summarize: pos (i.e. the POS layer we created earlier)\nScope of Summary: Speaker\nThe rest of the settings can be left with their defaults, except:\nAnnotate Tokens: unticked - we only want the summary information.\n\nPress Save.\nPress Regenerate to analyse your corpus.\nYou’ll see a progress bar while each POS label is counted for each participant.\nOnce the layer manager is finished, select layer managers on the menu.\nFind the Frequency Layer Manager in the list, and press its Extensions button.\nThis will show a page that lets you select from ‘dictionaries’ that are named after layers managed by the Frequency Layer Manager.\nSelect posFrequency and press Select\nA form is displayed that allows you to perform various operations on the frequency lists you have generated. Most likely, you just want to export a list of frequencies for a speaker:\nUnder Scope, select a speaker.\nOr if you want to include all speakers, select the [all scopes] option at the bottom of the dropdown box options.\nPress the Export button at the bottom.\nThis will give you a CSV file. If you open this in Excel (or any other data analysis tool), you’ll see that it contains three columns:\n\nScope - the speaker name\nType - the POS label\nFrequency - the number of times that speaker uttered a word with that POS label\n\n\n\nIf you prefer to have POS counts by transcript instead of by speaker, you can select Transcript as the scope at step 4 above. If you want both of these, create two word layers, one for summarising by participant, and the other by transcript."
  },
  {
    "objectID": "howto/pos-tagging/stanford-pos-tagger.html#install-the-layer-manager",
    "href": "howto/pos-tagging/stanford-pos-tagger.html#install-the-layer-manager",
    "title": "Stanford POS Tagger",
    "section": "",
    "text": "In LaBB-CAT, select the layer managers option on the menu at the top.\nAt the bottom, follow the link labelled: List of layer managers that are not yet installed.\nFind the StanfordPosTagger layer manager in the list, and press its Install button, then Install again.\nYou will see a configuration page with some information about the tagger.\nPress Configure.\nYou will see a progress bar while the layer manager downloads the Stanford POS Tagger files.\n\nOnce it’s finished, you’ll see a further information page."
  },
  {
    "objectID": "howto/pos-tagging/stanford-pos-tagger.html#create-a-pos-layer",
    "href": "howto/pos-tagging/stanford-pos-tagger.html#create-a-pos-layer",
    "title": "Stanford POS Tagger",
    "section": "",
    "text": "Now the layer manager is installed, we need to create a layer that is configured to use it to tag words with their part of speech…\n\nSelect word layers on the menu at the top.\nYou will see a list of word tag layers that have already been configured. The column headings at the top are also a form for creating a new layer, so we’ll fill in that form now.\nFill in the following details on the form at the top:\n\nLayer ID: pos\nType: Text\nAlignment: Intervals\nNB it’s important that this is not set to None because a single word can have multiple POS tags, one after another, which are strung between the start and the end of the word token.\nManager: Stanford POS Tagger\nGenerate: Always\nProject: This can be left as the default value, unless you want to add the layer to a category of your choice.\n\nPress New\nYou will see the layer configuration form. Mostly you can leave the default values as they are.\nSet the Model to use setting to something that makes sense for your transcripts, which depends on their language. This is a setting you may experiment with to get the best results. For English recordings, you may find the english-bidirectional-distsim.tagger is slower but produces better results.\nPress Set Parameters.\nNow press Regenerate to run the POS tagger on your whole corpus.\nYou will see a progress bar while the transcripts are being tagged.\nOnce it’s complete, select the transcripts option on the menu, and click the first transcript in the list.\nTick the new pos layer to display the tags.\nYou will see that each word has one or more tags above it - these identify the parts of speech or syntactic categories of the words.\n\n\nThe tags can be searched, extracted, summarised, etc. just like any other annotations."
  },
  {
    "objectID": "howto/pos-tagging/stanford-pos-tagger.html#summarising-pos-tags",
    "href": "howto/pos-tagging/stanford-pos-tagger.html#summarising-pos-tags",
    "title": "Stanford POS Tagger",
    "section": "",
    "text": "One possible aggregated analysis might be to compute the distributions of POS tags for each speaker.\nIn order to do that, you would set up the tagger as above to output the POS tags, and then set up a Frequency Layer Manager layer with the POS tags as input. In order to do that:\n\nIn LaBB-CAT, select the word layers menu option.\nAdd a new word layer with the following characteristics:\n\nLayer ID: posFrequency\nType: Number\nAlignment: None\nManager: Frequency Layer Manager\nGenerate: Always\n\nPress New.\nYou will see the Frequency Layer Manager configuration form.\nConfigure the layer as follows:\n\nSummary: Raw Count\nLayer to Summarize: pos (i.e. the POS layer we created earlier)\nScope of Summary: Speaker\nThe rest of the settings can be left with their defaults, except:\nAnnotate Tokens: unticked - we only want the summary information.\n\nPress Save.\nPress Regenerate to analyse your corpus.\nYou’ll see a progress bar while each POS label is counted for each participant.\nOnce the layer manager is finished, select layer managers on the menu.\nFind the Frequency Layer Manager in the list, and press its Extensions button.\nThis will show a page that lets you select from ‘dictionaries’ that are named after layers managed by the Frequency Layer Manager.\nSelect posFrequency and press Select\nA form is displayed that allows you to perform various operations on the frequency lists you have generated. Most likely, you just want to export a list of frequencies for a speaker:\nUnder Scope, select a speaker.\nOr if you want to include all speakers, select the [all scopes] option at the bottom of the dropdown box options.\nPress the Export button at the bottom.\nThis will give you a CSV file. If you open this in Excel (or any other data analysis tool), you’ll see that it contains three columns:\n\nScope - the speaker name\nType - the POS label\nFrequency - the number of times that speaker uttered a word with that POS label\n\n\n\nIf you prefer to have POS counts by transcript instead of by speaker, you can select Transcript as the scope at step 4 above. If you want both of these, create two word layers, one for summarising by participant, and the other by transcript."
  },
  {
    "objectID": "howto/pos-tagging/index.html",
    "href": "howto/pos-tagging/index.html",
    "title": "Part of Speech Tagging",
    "section": "",
    "text": "Part of Speech Tagging\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023-2024 NZILBB"
  },
  {
    "objectID": "howto/forced-alignment/mfa-train-align.html",
    "href": "howto/forced-alignment/mfa-train-align.html",
    "title": "MFA: Train-and-Align",
    "section": "",
    "text": "You can use ‘MFA’ to train new speaker-specific acoustic models on your speech data, and then to force align the data on those models. You may decide to do this if:\n\nYou can’t share your data with third parties and so can’t use WebMAUS.\nMFA has no dictionaries and pre-trained acoustic models for the language of your data and so you can’t use MFA and Pretrained Acoustic Models.\nYou have 3-5 hours’ speech.\n\nThe general process is illustrated below:\n\n\n\nPronunciations are generated from transcripts, and then combined with the recordings to train acoustic models, which are then used to compute phone-level alignements, which are saved to LaBB-CAT’s database\n\n\n\n\nIn order to be able to force-align transcripts to the word and/or segment level, you first need the following:\n\nTranscripts that are aligned at the utterance level (i.e. there’s a known time-point every 20 or so words), whch have been uploaded into LaBB-CAT\nA WAV file for each transcript, on the LaBB-CAT server\nA phonemic transcription word layer, that has at least one pronunciation for every word. If there are some lines/utterances that contain words with missing pronunciations, those lines will be ignored by the HTK Layer Manager.\n\nDepending on your speech data, there are several ways to obtain phonemic transcriptions for words:\n\nLexical tagging\n\nCELEX - for British English, German, Dutch, using one of the CELEX layer managers.\nCMU Pronouncing Dictionary - for US English, using th CMU Pronouncing Dictionary layer manager.\nUnisyn - for various English varieties, using the Unisyn layer manager.\nDefine your own lexicon, and use the Flat File Dictionary layer manager to integrate it into LaBB-CAT.\n\nInferring pronunciation from orthography\n\nSpanish, using the Spanish Phonological Transcriber layer manager\nBas Web Service: G2P - for various languages.\nDefine your own simple mapping rules from orthography to phonology, using the Character Mapper layer manager.\n\n\nIf the speech corpus includes data in more than one language, it is possible to ensure that the utterances are phonemically tagged in a way that’s sensitive to the language of the specific utterance, using the language layers and attributes, and auxiliary layer managers.\nWhichever method you choose, you need a phonemes ‘word layer’ on which each word token is tagged with its pronunciation, before you can proceed with the forced-alignment steps below.\n\n\n\nThe broad steps for getting forced-alignments from MFA are:\n\nInstall MFA on the same computer that LaBB-CAT is installed on\nInstall the MFA Layer Manager, which integrates LaBB-CAT with MFA\nCreate and configure a new MFA layer in LaBB-CAT\nPick a speakers/participants in your database and identify their utterances\nFill in the missing pronunciations for those utterances\nRun forced alignment\nRepeat steps 4-6 for all the participants in your database\n\n\n\n\nMFA is a 3rd-party tool that LaBB-CAT integrates with via a Layer Manager module. MFA is not included as part of LaBB-CAT, and so it must be installed on the server you have installed LaBB-CAT on before you can integrate LaBB-CAT with it.\nIf MFA has not been installed already, please follow the following steps, depending on the operatings system of your LaBB-CAT server. This is a one-time process.\n\n\nTo install the Montreal Forced Aligner on Linux systems for all users, so that your web server can access it if required:\n\nDownload Miniconda:\nwget https://repo.anaconda.com/miniconda/Miniconda3-py38\\_4.10.3-Linux-x86\\_64.sh\nStart the installer:\nsudo bash Miniconda3-py38_4.10.3-Linux-x86_64.sh\nWhen asked the location to install Miniconda, use:\n/opt/conda\nWhen asked whether the installer should initialize Miniconda, this is unnecessary so you can respond no\nChange ownership of the conda files:\nsudo chown -R $USERNAME:$USERNAME /opt/conda\nMake conda accessible to all users (so you web server can access MFA):\nchmod -R go-w /opt/conda\nchmod -R go+rX /opt/conda\n\nInstall the Montreal Forced Aligner. sudo /opt/conda/bin/conda create -n aligner -c conda-forge montreal-forced-aligner\n\n\n\n\nTo install the Montreal Forced Aligner on Windows systems for all users, so that your web server can access it if required:\n\nDownload the Miniconda installer:\nhttps://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe\nStart the installer by double-clicking it.\nWhen asked, select the “Install for all users” option. This will install conda somewhere like. C:\\ProgramData\\Miniconda3\nWhen asked, tick the “add to PATH” option.\nInstall the Montreal Forced Aligner by specifying a path to the environment. conda create -c conda-forge -p C:\\ProgramData\\Miniconda3\\envs\\aligner montreal-forced-aligner\n\n\n\n\n\nOnce MFA has been installed, you have to install the MFA Manager, which is the LaBB-CAT module that provides MFA with all the data it needs, and then saves to alignments MFA produces back to your database.\n\nSelect the layer managers menu option.\nFollow the List of layer managers that are not yet installed link.\nFind MFA Manager in the list, and press its Install button and then press Install again.\nAs long as MFA has been installed for all users, you should see a box that’s already filled in with the location that MFA was installed to.\nClick Configure to continue the layer manager installation.\nYou will see a window open with some information about integrating with MFA, including the information you’ve already read above.\n\n\n\n\nThe labels used for phonemes layer (or whichever layer tags each word token with its pronunciation) will use a specific encoding for the phonemes. Encodings include:\n\nCELEX DISC: Exactly one ASCII character per phoneme,\ne.g. there’ll → D8r@l\nUnicode IPA: One or more Unicode character per phoneme, possibly including diacritics, delimited by spaces:\ne.g. there’ll → ð ɛə ɹ l̩\nARPAbet: Phonemes are one or two uppercase ASCII characters, possibly suffixed with a digit indicating stress, delimited by spaces:\ne.g. there’ll → DH EH1 R AX0 L\n\nIf it uses CELEX DISC encoding, the phonemes layer should have its Type set to Phonological on the word layers page. Otherwise its Type should be set to Text.\nIn order to ensure that the labels that the MFA Manager will create on the segment layer use the same encoding, the segment layer must have the same Type as the phonemes layer. In order to ensure that:\n\nSelect the segment layers option on the menu.\nThe segment layer is the first on the list (and may be the only layer there).\nCheck the Type of the segment layer. If it’s not the same as the phonemes layer, change the Type so that it matches, and press the Save button that appears.\n\n\n\n\nOnce you’ve installed MFA and the MFA Layer Manager, you need to create a new layer for triggering and controlling forced alignment. This layer will itself contain a timestamp for each line/utterance it has force-aligned (and so it’s a ‘phrase’ layer), but during that process, the word and phone alignments will also be set on other layers.\n\nSelect the phrase layers option on the menu\nFill in the form at the top of the page (which doubles as column headings) with the following details:\n\nLayer ID: mfa\nType: Text\nAlignment: Intervals\nManager: MFA Manager\nGenerate: never (this is because we will manually select utterance for forced alignment, to ensure there is enough data to train acoustic models)\nDescription: MFA alignment time\n\nWhen you configure the layer, set the following options:\n\nPronunciation Layer: select the phonemes layer (or whichever word layer that tags each word with its pronunciation)\nDictionary Name: [none]\nPretrained Acoustic Models: [none] (this ensures that the train/align procedure is used)\nThe rest of the options can be left as their default values.\nIf you’re curious about what the configuration options do, hover your mouse over each option to see a `tool tip’ that describes what the option is for.\n\nPress Set Parameters\n\n\n\n\nMFA is data-hungry when training acoustic models for forced alignment, and needs 3-5 hours’ speech to produce accurate alignments. This means that, each time you run forced alignment, you should forced align several hours of speech at once. Depending on your data, this might represent a single speaker’s utterances, several speakers, or all speakers in your corpus!\nTo start a forced-alignment process for a batch of selected participants, you need to first select the participants who will be aligned. Then you need to list all their utterances, and MFA will first training acoustic models from scratch from them, and then using those acoustic models, align those same utterances.\n\nIn LaBB-CAT, select the participants link on the menu\nFilter the list to display the desired participants, and tick the checkbox next to each participant you want to include\nPress the All Utterances button above\nPress List to list all of their utterances.\nA progress bar will appear while LaBB-CAT identifies all the selected participant’s utterances. Once this is done, the first twenty utterances will be listed (like search results, the first twenty are listed for convenience, but you can process all matching utterances)\nClick the Mfa button below the list.\nA progress bar will appear while MFA gathers up all the utterance data, trains acoustic models, force-aligns the utterances, and then saves the resulting alignments back to LaBB-CAT. This process may take some time.\n\nOnce the progress bar reaches 100% and the process is complete, the selected utterances will have word start/end times set, and aligned phones will have been added to the segment layer, using the same labels as appear in the phonemes layer"
  },
  {
    "objectID": "howto/forced-alignment/mfa-train-align.html#prerequisites",
    "href": "howto/forced-alignment/mfa-train-align.html#prerequisites",
    "title": "MFA: Train-and-Align",
    "section": "",
    "text": "In order to be able to force-align transcripts to the word and/or segment level, you first need the following:\n\nTranscripts that are aligned at the utterance level (i.e. there’s a known time-point every 20 or so words), whch have been uploaded into LaBB-CAT\nA WAV file for each transcript, on the LaBB-CAT server\nA phonemic transcription word layer, that has at least one pronunciation for every word. If there are some lines/utterances that contain words with missing pronunciations, those lines will be ignored by the HTK Layer Manager.\n\nDepending on your speech data, there are several ways to obtain phonemic transcriptions for words:\n\nLexical tagging\n\nCELEX - for British English, German, Dutch, using one of the CELEX layer managers.\nCMU Pronouncing Dictionary - for US English, using th CMU Pronouncing Dictionary layer manager.\nUnisyn - for various English varieties, using the Unisyn layer manager.\nDefine your own lexicon, and use the Flat File Dictionary layer manager to integrate it into LaBB-CAT.\n\nInferring pronunciation from orthography\n\nSpanish, using the Spanish Phonological Transcriber layer manager\nBas Web Service: G2P - for various languages.\nDefine your own simple mapping rules from orthography to phonology, using the Character Mapper layer manager.\n\n\nIf the speech corpus includes data in more than one language, it is possible to ensure that the utterances are phonemically tagged in a way that’s sensitive to the language of the specific utterance, using the language layers and attributes, and auxiliary layer managers.\nWhichever method you choose, you need a phonemes ‘word layer’ on which each word token is tagged with its pronunciation, before you can proceed with the forced-alignment steps below."
  },
  {
    "objectID": "howto/forced-alignment/mfa-train-align.html#procedure-for-mfa-forced-alignment",
    "href": "howto/forced-alignment/mfa-train-align.html#procedure-for-mfa-forced-alignment",
    "title": "MFA: Train-and-Align",
    "section": "",
    "text": "The broad steps for getting forced-alignments from MFA are:\n\nInstall MFA on the same computer that LaBB-CAT is installed on\nInstall the MFA Layer Manager, which integrates LaBB-CAT with MFA\nCreate and configure a new MFA layer in LaBB-CAT\nPick a speakers/participants in your database and identify their utterances\nFill in the missing pronunciations for those utterances\nRun forced alignment\nRepeat steps 4-6 for all the participants in your database"
  },
  {
    "objectID": "howto/forced-alignment/mfa-train-align.html#mfa-installation",
    "href": "howto/forced-alignment/mfa-train-align.html#mfa-installation",
    "title": "MFA: Train-and-Align",
    "section": "",
    "text": "MFA is a 3rd-party tool that LaBB-CAT integrates with via a Layer Manager module. MFA is not included as part of LaBB-CAT, and so it must be installed on the server you have installed LaBB-CAT on before you can integrate LaBB-CAT with it.\nIf MFA has not been installed already, please follow the following steps, depending on the operatings system of your LaBB-CAT server. This is a one-time process.\n\n\nTo install the Montreal Forced Aligner on Linux systems for all users, so that your web server can access it if required:\n\nDownload Miniconda:\nwget https://repo.anaconda.com/miniconda/Miniconda3-py38\\_4.10.3-Linux-x86\\_64.sh\nStart the installer:\nsudo bash Miniconda3-py38_4.10.3-Linux-x86_64.sh\nWhen asked the location to install Miniconda, use:\n/opt/conda\nWhen asked whether the installer should initialize Miniconda, this is unnecessary so you can respond no\nChange ownership of the conda files:\nsudo chown -R $USERNAME:$USERNAME /opt/conda\nMake conda accessible to all users (so you web server can access MFA):\nchmod -R go-w /opt/conda\nchmod -R go+rX /opt/conda\n\nInstall the Montreal Forced Aligner. sudo /opt/conda/bin/conda create -n aligner -c conda-forge montreal-forced-aligner\n\n\n\n\nTo install the Montreal Forced Aligner on Windows systems for all users, so that your web server can access it if required:\n\nDownload the Miniconda installer:\nhttps://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe\nStart the installer by double-clicking it.\nWhen asked, select the “Install for all users” option. This will install conda somewhere like. C:\\ProgramData\\Miniconda3\nWhen asked, tick the “add to PATH” option.\nInstall the Montreal Forced Aligner by specifying a path to the environment. conda create -c conda-forge -p C:\\ProgramData\\Miniconda3\\envs\\aligner montreal-forced-aligner"
  },
  {
    "objectID": "howto/forced-alignment/mfa-train-align.html#install-the-mfa-layer-manager",
    "href": "howto/forced-alignment/mfa-train-align.html#install-the-mfa-layer-manager",
    "title": "MFA: Train-and-Align",
    "section": "",
    "text": "Once MFA has been installed, you have to install the MFA Manager, which is the LaBB-CAT module that provides MFA with all the data it needs, and then saves to alignments MFA produces back to your database.\n\nSelect the layer managers menu option.\nFollow the List of layer managers that are not yet installed link.\nFind MFA Manager in the list, and press its Install button and then press Install again.\nAs long as MFA has been installed for all users, you should see a box that’s already filled in with the location that MFA was installed to.\nClick Configure to continue the layer manager installation.\nYou will see a window open with some information about integrating with MFA, including the information you’ve already read above."
  },
  {
    "objectID": "howto/forced-alignment/mfa-train-align.html#dictionary-and-segment-layer-labels",
    "href": "howto/forced-alignment/mfa-train-align.html#dictionary-and-segment-layer-labels",
    "title": "MFA: Train-and-Align",
    "section": "",
    "text": "The labels used for phonemes layer (or whichever layer tags each word token with its pronunciation) will use a specific encoding for the phonemes. Encodings include:\n\nCELEX DISC: Exactly one ASCII character per phoneme,\ne.g. there’ll → D8r@l\nUnicode IPA: One or more Unicode character per phoneme, possibly including diacritics, delimited by spaces:\ne.g. there’ll → ð ɛə ɹ l̩\nARPAbet: Phonemes are one or two uppercase ASCII characters, possibly suffixed with a digit indicating stress, delimited by spaces:\ne.g. there’ll → DH EH1 R AX0 L\n\nIf it uses CELEX DISC encoding, the phonemes layer should have its Type set to Phonological on the word layers page. Otherwise its Type should be set to Text.\nIn order to ensure that the labels that the MFA Manager will create on the segment layer use the same encoding, the segment layer must have the same Type as the phonemes layer. In order to ensure that:\n\nSelect the segment layers option on the menu.\nThe segment layer is the first on the list (and may be the only layer there).\nCheck the Type of the segment layer. If it’s not the same as the phonemes layer, change the Type so that it matches, and press the Save button that appears."
  },
  {
    "objectID": "howto/forced-alignment/mfa-train-align.html#create-the-mfa-layer",
    "href": "howto/forced-alignment/mfa-train-align.html#create-the-mfa-layer",
    "title": "MFA: Train-and-Align",
    "section": "",
    "text": "Once you’ve installed MFA and the MFA Layer Manager, you need to create a new layer for triggering and controlling forced alignment. This layer will itself contain a timestamp for each line/utterance it has force-aligned (and so it’s a ‘phrase’ layer), but during that process, the word and phone alignments will also be set on other layers.\n\nSelect the phrase layers option on the menu\nFill in the form at the top of the page (which doubles as column headings) with the following details:\n\nLayer ID: mfa\nType: Text\nAlignment: Intervals\nManager: MFA Manager\nGenerate: never (this is because we will manually select utterance for forced alignment, to ensure there is enough data to train acoustic models)\nDescription: MFA alignment time\n\nWhen you configure the layer, set the following options:\n\nPronunciation Layer: select the phonemes layer (or whichever word layer that tags each word with its pronunciation)\nDictionary Name: [none]\nPretrained Acoustic Models: [none] (this ensures that the train/align procedure is used)\nThe rest of the options can be left as their default values.\nIf you’re curious about what the configuration options do, hover your mouse over each option to see a `tool tip’ that describes what the option is for.\n\nPress Set Parameters"
  },
  {
    "objectID": "howto/forced-alignment/mfa-train-align.html#batch-alignment",
    "href": "howto/forced-alignment/mfa-train-align.html#batch-alignment",
    "title": "MFA: Train-and-Align",
    "section": "",
    "text": "MFA is data-hungry when training acoustic models for forced alignment, and needs 3-5 hours’ speech to produce accurate alignments. This means that, each time you run forced alignment, you should forced align several hours of speech at once. Depending on your data, this might represent a single speaker’s utterances, several speakers, or all speakers in your corpus!\nTo start a forced-alignment process for a batch of selected participants, you need to first select the participants who will be aligned. Then you need to list all their utterances, and MFA will first training acoustic models from scratch from them, and then using those acoustic models, align those same utterances.\n\nIn LaBB-CAT, select the participants link on the menu\nFilter the list to display the desired participants, and tick the checkbox next to each participant you want to include\nPress the All Utterances button above\nPress List to list all of their utterances.\nA progress bar will appear while LaBB-CAT identifies all the selected participant’s utterances. Once this is done, the first twenty utterances will be listed (like search results, the first twenty are listed for convenience, but you can process all matching utterances)\nClick the Mfa button below the list.\nA progress bar will appear while MFA gathers up all the utterance data, trains acoustic models, force-aligns the utterances, and then saves the resulting alignments back to LaBB-CAT. This process may take some time.\n\nOnce the progress bar reaches 100% and the process is complete, the selected utterances will have word start/end times set, and aligned phones will have been added to the segment layer, using the same labels as appear in the phonemes layer"
  },
  {
    "objectID": "howto/forced-alignment/htk-p2fa.html",
    "href": "howto/forced-alignment/htk-p2fa.html",
    "title": "HTK and P2FA",
    "section": "",
    "text": "You can use ‘HTK’ and the ‘P2FA’ pre-trained acoustic models to force align you speech data if it is US English, or sufficiently similar to US English for the phoneme set to produce good alignments.\nThe general process is illustrated below:\n\n\n\nPronunciations are looked up for transcripts, and then combined with recordings and the P2FA models to compute alignments, which are saved in LaBB-CAT\n\n\nThe Hidden Markhov Model Toolkit (HTK) is a speech recognition toolkit developed at Cambridge University. LaBB-CAT can use it for force-aligning words and segments. HTK is not free software in the “GNU” sense - i.e. we can not distribute it with LaBB-CAT, instead you have to download it yourself from the Cambridge University website - however it is free in the “no cost” sense, you just need to register on the HTK website, and you can then download and use HTK free of charge.\nThe Penn Phonetics Lab Forced Aligner (P2FA) is a is an automatic phonetic alignment toolkit based on HTK, and includes acoustic models that were pre-trained on US English. Jiahong Yuan has kindly granted permission for those models to be included in LaBB-CAT, so they can be used directly without needing to download the P2FA toolkit seperately.\n\n\nIn order to be able to force-align transcripts to the word and/or segment level, you first need the following:\n\nTranscripts that are aligned at the utterance level (i.e. there’s a known time-point every 20 or so words), whch have been uploaded into LaBB-CAT\nA WAV file for each transcript, on the LaBB-CAT server\nA phonemic transcription word layer, that has at least one pronunciation for every word. This can be achieved for English data by using the CMU Dictionary Layer Manager (further information below). If there are some lines/utterances that contain words with missing pronunciations, those lines will be ignored by the HTK Layer Manager.\n\n\n\nLaBB-CAT can be integrated with the CMU Pronouncing Dictionary, which is a free pronouncing dictionary of English maintained by the Speech Group in the School of Computer Science at Carnegie Mellon University. The pronunciations are based on American English, so are suitable for American English recordings.\nIntegrating this lexicon with LaBB-CAT is achieved with the “CMU Dictionary Layer Manager”. As CMU has kindly granted permission to freely distribute the dictionary file, you don’t need to download the file from the CMU site; it’s included in the layer manager that you will install.\nTo install the CMU Pronouncing Dictionary and use it to tag words with their pronunciations, follow these instructions\n\n\n\n\nThe broad steps for getting forced-alignments from HTK are:\n\nInstall HTK on the same computer that LaBB-CAT is installed on\nInstall the HTK Layer Manager, which integrates LaBB-CAT with HTK\nCreate and configure a new HTK layer in LaBB-CAT\nPick a speaker/participant in your database\nFill in the missing pronunciations for that participant\nRun forced alignment\nRepeat steps 4-6 for all the participants in your database\n\n\n\nAs mentioned above, HTK is a 3rd-party tool that you must download and install from the Cambridge University website.\n\nRegister at http://htk.eng.cam.ac.uk/register.shtml\nDownload the version of HTK that is appropriate for the computer that LaBB-CAT is install on:\nFor Windows systems, there are pre-compiled .exe files that you can download. For Unix-like systems, you need to download the source code, which you will then install following the provided instructions (you may also need to install the xorg-dev package before it will successfully compile).\nUnzip (for Windows) or compile and install (for Unix-like systems) the downloaded files on the computer that LaBB-CAT is installed on.\n\n\n\n\nThe HTK Layer Manager is a LaBB-CAT module that integrates LaBB-CAT with HTK.\n\nIn LaBB-CAT, select the layer managers option on the menu, which gives you a list of the layer managers already installed.\nAt the bottom of the page, follow the List of layer managers that are not yet installed link.\nLook for HTK in the list, and press it’s Install button.\nYou will see a form with boxes for filling in information.\n\nHTK Path must be set to the location where the HTK files are installed on your system. If this is already filled in, it’s probably correct. If it’s blank, you have to enter the full path for the HTK programs:\n\nOn Windows systems, this is where you unzipped the HTK .exe files - e.g. something like C:\\Downloads\\HTK\nOn Unix-like systems, this is probably /usr/local/bin, but you can verify this by entering which HCopy at a command shell prompt.\n\nHTK Working Folder will already have a default value, which is probably best left as-is\n\nPress Install Layer Manager\n\n\n\n\nOnce you’ve installed HTK and the HTK Layer Manager, you need to create a new layer for triggering and controlling forced alignment. This layer will itself contain a timestamp for each line/utterance it has force-aligned (and so it’s a ‘phrase’ layer), but during that process, the word and phone alignments will also be set on other layers.\n\nIn LaBB-CAT, select the phrase layers option\nAt the top of the page, there’s a blank form for creating a new layer; fill in the following details:\n\nLayer ID: HTK\nType: Text\nManager: HTK Manager\nAlignment: Time Intervals\nGenerate: Always\n\nPress New.\nYou will see the layer configuration page.\nCheck the online help if you want information about all the options, however, most likely the default options are approriate, except:\n\nPronunciaton Layer: this is the layer that provides the phonemic transcriptions for all the words; ensure you select the phonemes layer you created above.\nNB If you have created this layer but it doesn’t appear here as an option, it’s probably because the ‘layer type’ of your pronunciation layer is not set to ‘Phonological’, which will need to be changed in order for it to appear as an option here.\nNB In the list of options there’s also a layer called “pronounce”; this is a system layer for manually-added pronunciations, and you would only select that layer here if you have manually annotated pronunciations against every single word in your transcripts. You probably haven’t done that, so you don’t want to select the “pronounce” layer here.\nUse P2FA models: ensure you tick this option.\n\nPress Save.\nIf you are confident all your transcripts include all pronunciations for all words, you can press Regenerate to force-align your whole corpus now. However, most likely you’ll need to proceed per-speaker, described below, in order to fill in missing pronunciations.\n\n\n\n\nTo start a forced-alignment process per-speaker, you need to first select a speaker who will be aligned. Then you will fill in any missing pronunciations. After that, HTK will automatically force-align their utterances.\n\nIn LaBB-CAT, select the participants optoin on the menu\nTick a speaker, and press the All Utterances button\nClick List\nOnce the paginated list of utterances appears, press the HTK button below.\nBasically you need to fill in the boxes with the pronunciations and click Save Pronunciations.\n\n\n\n\n\n\n\nNote\n\n\n\n\nYou don’t have to fill them all in at once, you can do a few, and click Save, which will save your work and list what’s left.\nYou don’t have to fill them all in, you can leave some empty and continue with the HTK forced-alignment by clicking Start (HTK will ignore any lines where the remaining unknown words appear, but the ones you filled in will be included).\nSome of the boxes will be initially filled in with a suggestion from the lexicon layer manager - these may or may not be correct, and aren’t saved until you save them.\nThe pronunciations have to be in the ‘DISC’ format - i.e. one character per phoneme, with no spaces. There’s a ‘helper’ link on the right of each pronunciation box - if you click it, it expands into a list of clickable phonemes - just the ones that aren’t ordinary letters, and diphthongs etc.\nThe search button lets you look up the lexicon for similar words - this probably won’t help for place names, but for words like “tarseal”, you can click the lookup button, enter “tar seal” in the box as two separate words, and you’ll get back the DISC pronunciation of each word, with clickable buttons to copy the given pronunciation into the box. This is useful for digits and numbers too, which may not be in the lexicon - so for “1”, search for “one” and copy the pronunciation.\nIf you click on the word itself, the transcript for the first instance of that word is opened, in case you want to listen to it, or in case it’s actually just a typo and you want to correct the transcript.\nIf you’re using CELEX, when you specify the pronunciations, it’s recommended to put syllable separators (-) and primary stress markers (’) too - e.g. for “tarseal” you can put t#sil but it would actually be better to put t#-’sil. These markers are entered into the dictionary even though they’re stripped out for HTK, and they may come in handy later (e.g. the syllable separators are used by the CELEX layer manager to count syllables).\n\n\n\nWhen you add pronunciations this way, they’re added to the dictionary and all the instances of those words in LaBB-CAT are updated with the pronunciations - not just the participant you’re looking at, but all participants in the database. So you only have to come up with a pronunciation for each word once.\n\nOnce you’ve filled in all the missing pronunciations, forced alignment will start automatically. If you want to start forced alignment before you’ve entered all pronunciations, click the Start button at the bottom of the page.\n\nYou should see a progress bar while the forced alignment is running. It will take a few minutes to complete.\n\nOnce HTK has produced the word and segment alignments, it:\n\nsets the start/end times of the words on the transcript layer accordingly,\nadds new phone annotations to the segments layer with the alignments of the phones, and\nsaves a timestamp in the htk layer.\n\nWhen the layer manager has finished, you’ll see a message saying “Complete - words and phones from selected utterances are now aligned.”"
  },
  {
    "objectID": "howto/forced-alignment/htk-p2fa.html#prerequisites",
    "href": "howto/forced-alignment/htk-p2fa.html#prerequisites",
    "title": "HTK and P2FA",
    "section": "",
    "text": "In order to be able to force-align transcripts to the word and/or segment level, you first need the following:\n\nTranscripts that are aligned at the utterance level (i.e. there’s a known time-point every 20 or so words), whch have been uploaded into LaBB-CAT\nA WAV file for each transcript, on the LaBB-CAT server\nA phonemic transcription word layer, that has at least one pronunciation for every word. This can be achieved for English data by using the CMU Dictionary Layer Manager (further information below). If there are some lines/utterances that contain words with missing pronunciations, those lines will be ignored by the HTK Layer Manager.\n\n\n\nLaBB-CAT can be integrated with the CMU Pronouncing Dictionary, which is a free pronouncing dictionary of English maintained by the Speech Group in the School of Computer Science at Carnegie Mellon University. The pronunciations are based on American English, so are suitable for American English recordings.\nIntegrating this lexicon with LaBB-CAT is achieved with the “CMU Dictionary Layer Manager”. As CMU has kindly granted permission to freely distribute the dictionary file, you don’t need to download the file from the CMU site; it’s included in the layer manager that you will install.\nTo install the CMU Pronouncing Dictionary and use it to tag words with their pronunciations, follow these instructions"
  },
  {
    "objectID": "howto/forced-alignment/htk-p2fa.html#procedure-for-htk-forced-alignment",
    "href": "howto/forced-alignment/htk-p2fa.html#procedure-for-htk-forced-alignment",
    "title": "HTK and P2FA",
    "section": "",
    "text": "The broad steps for getting forced-alignments from HTK are:\n\nInstall HTK on the same computer that LaBB-CAT is installed on\nInstall the HTK Layer Manager, which integrates LaBB-CAT with HTK\nCreate and configure a new HTK layer in LaBB-CAT\nPick a speaker/participant in your database\nFill in the missing pronunciations for that participant\nRun forced alignment\nRepeat steps 4-6 for all the participants in your database\n\n\n\nAs mentioned above, HTK is a 3rd-party tool that you must download and install from the Cambridge University website.\n\nRegister at http://htk.eng.cam.ac.uk/register.shtml\nDownload the version of HTK that is appropriate for the computer that LaBB-CAT is install on:\nFor Windows systems, there are pre-compiled .exe files that you can download. For Unix-like systems, you need to download the source code, which you will then install following the provided instructions (you may also need to install the xorg-dev package before it will successfully compile).\nUnzip (for Windows) or compile and install (for Unix-like systems) the downloaded files on the computer that LaBB-CAT is installed on.\n\n\n\n\nThe HTK Layer Manager is a LaBB-CAT module that integrates LaBB-CAT with HTK.\n\nIn LaBB-CAT, select the layer managers option on the menu, which gives you a list of the layer managers already installed.\nAt the bottom of the page, follow the List of layer managers that are not yet installed link.\nLook for HTK in the list, and press it’s Install button.\nYou will see a form with boxes for filling in information.\n\nHTK Path must be set to the location where the HTK files are installed on your system. If this is already filled in, it’s probably correct. If it’s blank, you have to enter the full path for the HTK programs:\n\nOn Windows systems, this is where you unzipped the HTK .exe files - e.g. something like C:\\Downloads\\HTK\nOn Unix-like systems, this is probably /usr/local/bin, but you can verify this by entering which HCopy at a command shell prompt.\n\nHTK Working Folder will already have a default value, which is probably best left as-is\n\nPress Install Layer Manager\n\n\n\n\nOnce you’ve installed HTK and the HTK Layer Manager, you need to create a new layer for triggering and controlling forced alignment. This layer will itself contain a timestamp for each line/utterance it has force-aligned (and so it’s a ‘phrase’ layer), but during that process, the word and phone alignments will also be set on other layers.\n\nIn LaBB-CAT, select the phrase layers option\nAt the top of the page, there’s a blank form for creating a new layer; fill in the following details:\n\nLayer ID: HTK\nType: Text\nManager: HTK Manager\nAlignment: Time Intervals\nGenerate: Always\n\nPress New.\nYou will see the layer configuration page.\nCheck the online help if you want information about all the options, however, most likely the default options are approriate, except:\n\nPronunciaton Layer: this is the layer that provides the phonemic transcriptions for all the words; ensure you select the phonemes layer you created above.\nNB If you have created this layer but it doesn’t appear here as an option, it’s probably because the ‘layer type’ of your pronunciation layer is not set to ‘Phonological’, which will need to be changed in order for it to appear as an option here.\nNB In the list of options there’s also a layer called “pronounce”; this is a system layer for manually-added pronunciations, and you would only select that layer here if you have manually annotated pronunciations against every single word in your transcripts. You probably haven’t done that, so you don’t want to select the “pronounce” layer here.\nUse P2FA models: ensure you tick this option.\n\nPress Save.\nIf you are confident all your transcripts include all pronunciations for all words, you can press Regenerate to force-align your whole corpus now. However, most likely you’ll need to proceed per-speaker, described below, in order to fill in missing pronunciations.\n\n\n\n\nTo start a forced-alignment process per-speaker, you need to first select a speaker who will be aligned. Then you will fill in any missing pronunciations. After that, HTK will automatically force-align their utterances.\n\nIn LaBB-CAT, select the participants optoin on the menu\nTick a speaker, and press the All Utterances button\nClick List\nOnce the paginated list of utterances appears, press the HTK button below.\nBasically you need to fill in the boxes with the pronunciations and click Save Pronunciations.\n\n\n\n\n\n\n\nNote\n\n\n\n\nYou don’t have to fill them all in at once, you can do a few, and click Save, which will save your work and list what’s left.\nYou don’t have to fill them all in, you can leave some empty and continue with the HTK forced-alignment by clicking Start (HTK will ignore any lines where the remaining unknown words appear, but the ones you filled in will be included).\nSome of the boxes will be initially filled in with a suggestion from the lexicon layer manager - these may or may not be correct, and aren’t saved until you save them.\nThe pronunciations have to be in the ‘DISC’ format - i.e. one character per phoneme, with no spaces. There’s a ‘helper’ link on the right of each pronunciation box - if you click it, it expands into a list of clickable phonemes - just the ones that aren’t ordinary letters, and diphthongs etc.\nThe search button lets you look up the lexicon for similar words - this probably won’t help for place names, but for words like “tarseal”, you can click the lookup button, enter “tar seal” in the box as two separate words, and you’ll get back the DISC pronunciation of each word, with clickable buttons to copy the given pronunciation into the box. This is useful for digits and numbers too, which may not be in the lexicon - so for “1”, search for “one” and copy the pronunciation.\nIf you click on the word itself, the transcript for the first instance of that word is opened, in case you want to listen to it, or in case it’s actually just a typo and you want to correct the transcript.\nIf you’re using CELEX, when you specify the pronunciations, it’s recommended to put syllable separators (-) and primary stress markers (’) too - e.g. for “tarseal” you can put t#sil but it would actually be better to put t#-’sil. These markers are entered into the dictionary even though they’re stripped out for HTK, and they may come in handy later (e.g. the syllable separators are used by the CELEX layer manager to count syllables).\n\n\n\nWhen you add pronunciations this way, they’re added to the dictionary and all the instances of those words in LaBB-CAT are updated with the pronunciations - not just the participant you’re looking at, but all participants in the database. So you only have to come up with a pronunciation for each word once.\n\nOnce you’ve filled in all the missing pronunciations, forced alignment will start automatically. If you want to start forced alignment before you’ve entered all pronunciations, click the Start button at the bottom of the page.\n\nYou should see a progress bar while the forced alignment is running. It will take a few minutes to complete.\n\nOnce HTK has produced the word and segment alignments, it:\n\nsets the start/end times of the words on the transcript layer accordingly,\nadds new phone annotations to the segments layer with the alignments of the phones, and\nsaves a timestamp in the htk layer.\n\nWhen the layer manager has finished, you’ll see a message saying “Complete - words and phones from selected utterances are now aligned.”"
  },
  {
    "objectID": "howto/forced-alignment/correction-emu.html",
    "href": "howto/forced-alignment/correction-emu.html",
    "title": "Checking/Correcting Alignments: EMU-webApp",
    "section": "",
    "text": "Checking/Correcting Alignments: EMU-webApp\nDepending on how LaBB-CAT is configured, when the forced aligner finishes it may automatically open a new browser tab with a list of utterances on the left, and a representation of the first utterance in the centre.\nThis is the EMU-webApp annotation tool, which you can use to manually inspect, and correct, the phone alignments that HTK just produced.\n\n\n\nEMU-webApp\n\n\nThe left panel contains a list of all the utterances that were just processed by HTK. If you click on an utterance in the list, the selected utterance will be loaded into the central panel.\nThe central panel is split into two parts. The upper part represents the utterance’s recording with a wave-form and a spectrogram. The lower part contains the annotation tool that represents each phone, including its label and start/end time.\nBelow this are a series of buttons you can use to control which portion of the utterance is displayed, and control audio playback.\nTo correct the phone alignments:\n\nSelect a phone in the annotation panel by clicking on it.\nTap the C key on your keyboard to play the audio for the selected phone.\nIf you want to change the start time, hover the mouse over the start time and hold down the Shift key on your keyboard. Moving the mouse will move the phone boundary until you let go of the Shift key. You can similarly correct end times. When you make changes, the utterance will turn red in the list on the left.\n\nTo save your corrections back to LaBB-CAT, press the round Save button in the utterance list panel.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023-2024 NZILBB"
  },
  {
    "objectID": "howto/forced-alignment/identifying-unaligned-utterances.html",
    "href": "howto/forced-alignment/identifying-unaligned-utterances.html",
    "title": "Identifying Unaligned Utterances",
    "section": "",
    "text": "Under some circumstances, forced alignment can fail to produce alignments for some utterances; i.e. the utterance has no phone annotations created, the words are not aligned, and no htk annotation is created. This can happen because of the following factors:\n\nNot enough data (if you’re using the ‘train-and-align’ approach)\nPoor quality recording, background noises, etc.\nSimultaneous speech (ignored by default)\nInaccurate transcripts\nInaccurate utterance alignment\nLack of pause marking in the transcripts\nMismatched phonology between dictionary and speech\ne.g. using a rhotic dictionary to align non-rhotic speech\n\nYou can identify the utterances for which alignment has failed using LaBB-CAT’s search and export functionality:\n\nClick search and select the speaker(s) you aligned.\nThe search should be “the first word of each utterance that doesn’t have an htk annotation” - i.e.:\n\northography layer: matches .+\nutterance layer: tick the left-hand checkbox that anchors the word to the beginning of the utterance\nhtk1 layer: doesn’t match .+. \n\nWhen the results are listed, click CSV Export\n\nThe resulting file has the start and end time of each utterance in the Line and LineEnd columns. If you want to know the total duration of the unaligned utterances, use Excel or R to calculate the difference between LineEnd and Line to get the line duration, and then sum these durations to get the total, which is in seconds."
  },
  {
    "objectID": "howto/forced-alignment/identifying-unaligned-utterances.html#footnotes",
    "href": "howto/forced-alignment/identifying-unaligned-utterances.html#footnotes",
    "title": "Identifying Unaligned Utterances",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhtk or whatever the phrase tag layer is in the forced alignment configuration↩︎"
  },
  {
    "objectID": "howto/forced-alignment/htk-train-align.html",
    "href": "howto/forced-alignment/htk-train-align.html",
    "title": "HTK: Train-and-Align",
    "section": "",
    "text": "You can use ‘HTK’ to train new speaker-specific acoustic models on your speech data, and then to force align the data on those models. You may decide to do this if:\n\nYou can’t share your data with third parties and so can’t use WebMAUS.\nYour data isn’t US English (or similar) and so you can’t use HTK with the P2FA pre-trained models.\nYou have at least 5 minutes’ speech for each speaker.\n\nThe general process is illustrated below:\n\n\n\nPronunciations are generated from transcripts, and then combined with the recordings to train acoustic models, which are then used to compute phone-level alignements, which are saved to LaBB-CAT’s database\n\n\n\n\nIn order to be able to force-align transcripts to the word and/or segment level, you first need the following:\n\nTranscripts that are aligned at the utterance level (i.e. there’s a known time-point every 20 or so words), whch have been uploaded into LaBB-CAT\nA WAV file for each transcript, on the LaBB-CAT server\nA phonemic transcription word layer, that has at least one pronunciation for every word. If there are some lines/utterances that contain words with missing pronunciations, those lines will be ignored by the HTK Layer Manager.\n\nDepending on your speech data, there are several ways to obtain phonemic transcriptions for words:\n\nLexical tagging\n\nCELEX - for British English, German, Dutch, using one of the CELEX layer managers.\nCMU Pronouncing Dictionary - for US English, using th CMU Pronouncing Dictionary layer manager.\nUnisyn - for various English varieties, using the Unisyn layer manager.\nDefine your own lexicon, and use the Flat File Dictionary layer manager to integrate it into LaBB-CAT.\n\nInferring pronunciation from orthography\n\nSpanish, using the Spanish Phonological Transcriber layer manager\nBas Web Service: G2P - for various languages.\nDefine your own simple mapping rules from orthography to phonology, using the Character Mapper layer manager.\n\n\nIf the speech corpus includes data in more than one language, it is possible to ensure that the utterances are phonemically tagged in a way that’s sensitive to the language of the specific utterance, using the language layers and attributes, and auxiliary layer managers.\nWhichever method you choose, you need a phonemes ‘word layer’ on which each word token is tagged with its pronunciation, before you can proceed with the forced-alignment steps below.\n\n\n\nThe broad steps for getting forced-alignments from HTK are:\n\nInstall HTK on the same computer that LaBB-CAT is installed on\nInstall the HTK Layer Manager, which integrates LaBB-CAT with HTK\nCreate and configure a new HTK layer in LaBB-CAT\nPick a speaker/participant in your database\nFill in the missing pronunciations for that participant\nRun forced alignment\nRepeat steps 4-6 for all the participants in your database\n\n\n\nAs mentioned above, HTK is a 3rd-party tool that you must download and install from the Cambridge University website.\n\nRegister at http://htk.eng.cam.ac.uk/register.shtml\nDownload the version of HTK that is appropriate for the computer that LaBB-CAT is install on:\nFor Windows systems, there are pre-compiled .exe files that you can download. For Unix-like systems, you need to download the source code, which you will then install following the provided instructions (you may also need to install the xorg-dev package before it will successfully compile).\nUnzip (for Windows) or compile and install (for Unix-like systems) the downloaded files on the computer that LaBB-CAT is installed on.\n\n\n\n\nThe HTK Layer Manager is a LaBB-CAT module that integrates LaBB-CAT with HTK.\n\nIn LaBB-CAT, select the layer managers option on the menu, which gives you a list of the layer managers already installed.\nAt the bottom of the page, follow the List of layer managers that are not yet installed link.\nLook for HTK in the list, and press it’s Install button.\nYou will see a form with boxes for filling in information.\n\nHTK Path must be set to the location where the HTK files are installed on your system. If this is already filled in, it’s probably correct. If it’s blank, you have to enter the full path for the HTK programs:\n\nOn Windows systems, this is where you unzipped the HTK .exe files - e.g. something like C:\\Downloads\\HTK\nOn Unix-like systems, this is probably /usr/local/bin, but you can verify this by entering which HCopy at a command shell prompt.\n\nHTK Working Folder will already have a default value, which is probably best left as-is\n\nPress Install Layer Manager\n\n\n\n\nOnce you’ve installed HTK and the HTK Layer Manager, you need to create a new layer for triggering and controlling forced alignment. This layer will itself contain a timestamp for each line/utterance it has force-aligned (and so it’s a ‘phrase’ layer), but during that process, the word and phone alignments will also be set on other layers.\n\nIn LaBB-CAT, select the phrase layers option\nAt the top of the page, there’s a blank form for creating a new layer; fill in the following details:\n\nLayer ID: HTK\nType: Text\nManager: HTK Manager\nAlignment: Time Intervals\nGenerate: Always\n\nPress New.\nYou will see the layer configuration page.\nCheck the online help if you want information about all the options, however, most likely the default options are approriate, except:\n\nPronunciaton Layer: this is the layer that provides the phonemic transcriptions for all the words; ensure you select the phonemes layer you created above.\nNB If you have created this layer but it doesn’t appear here as an option, it’s probably because the ‘layer type’ of your pronunciation layer is not set to ‘Phonological’, which will need to be changed in order for it to appear as an option here.\nNB In the list of options there’s also a layer called “pronounce”; this is a system layer for manually-added pronunciations, and you would only select that layer here if you have manually annotated pronunciations against every single word in your transcripts. You probably haven’t done that, so you don’t want to select the “pronounce” layer here.\nUse P2FA models: ensure this option is un-ticked.\n\nPress Save.\nIf you are confident all your transcripts include all pronunciations for all words, you can press Regenerate to force-align your whole corpus now. However, most likely you’ll need to proceed per-speaker, described below, in order to fill in missing pronunciations.\n\n\n\n\nTo start a forced-alignment process per-speaker, you need to first select a speaker who will be aligned. Then you will fill in any missing pronunciations. After that, HTK will automatically force-align their utterances.\n\nIn LaBB-CAT, select the participants optoin on the menu\nTick a speaker, and press the All Utterances button\nClick List\nOnce the paginated list of utterances appears, press the HTK button below.\nBasically you need to fill in the boxes with the pronunciations and click Save Pronunciations.\n\n\n\n\n\n\n\nNote\n\n\n\n\nYou don’t have to fill them all in at once, you can do a few, and click Save, which will save your work and list what’s left.\nYou don’t have to fill them all in, you can leave some empty and continue with the HTK forced-alignment by clicking Start (HTK will ignore any lines where the remaining unknown words appear, but the ones you filled in will be included).\nSome of the boxes will be initially filled in with a suggestion from the lexicon layer manager - these may or may not be correct, and aren’t saved until you save them.\nThe pronunciations have to be in the ‘DISC’ format - i.e. one character per phoneme, with no spaces. There’s a ‘helper’ link on the right of each pronunciation box - if you click it, it expands into a list of clickable phonemes - just the ones that aren’t ordinary letters, and diphthongs etc.\nThe search button lets you look up the lexicon for similar words - this probably won’t help for place names, but for words like “tarseal”, you can click the lookup button, enter “tar seal” in the box as two separate words, and you’ll get back the DISC pronunciation of each word, with clickable buttons to copy the given pronunciation into the box. This is useful for digits and numbers too, which may not be in the lexicon - so for “1”, search for “one” and copy the pronunciation.\nIf you click on the word itself, the transcript for the first instance of that word is opened, in case you want to listen to it, or in case it’s actually just a typo and you want to correct the transcript.\nIf you’re using CELEX, when you specify the pronunciations, it’s recommended to put syllable separators (-) and primary stress markers (’) too - e.g. for “tarseal” you can put t#sil but it would actually be better to put t#-’sil. These markers are entered into the dictionary even though they’re stripped out for HTK, and they may come in handy later (e.g. the syllable separators are used by the CELEX layer manager to count syllables).\n\n\n\nWhen you add pronunciations this way, they’re added to the dictionary and all the instances of those words in LaBB-CAT are updated with the pronunciations - not just the participant you’re looking at, but all participants in the database. So you only have to come up with a pronunciation for each word once.\n\nOnce you’ve filled in all the missing pronunciations, forced alignment will start automatically. If you want to start forced alignment before you’ve entered all pronunciations, click the Start button at the bottom of the page.\n\nYou should see a progress bar while the forced alignment is running. It will take a few minutes to complete.\n\nOnce HTK has produced the word and segment alignments, it:\n\nsets the start/end times of the words on the transcript layer accordingly,\nadds new phone annotations to the segments layer with the alignments of the phones, and\nsaves a timestamp in the htk layer.\n\nWhen the layer manager has finished, you’ll see a message saying “Complete - words and phones from selected utterances are now aligned.”"
  },
  {
    "objectID": "howto/forced-alignment/htk-train-align.html#prerequisites",
    "href": "howto/forced-alignment/htk-train-align.html#prerequisites",
    "title": "HTK: Train-and-Align",
    "section": "",
    "text": "In order to be able to force-align transcripts to the word and/or segment level, you first need the following:\n\nTranscripts that are aligned at the utterance level (i.e. there’s a known time-point every 20 or so words), whch have been uploaded into LaBB-CAT\nA WAV file for each transcript, on the LaBB-CAT server\nA phonemic transcription word layer, that has at least one pronunciation for every word. If there are some lines/utterances that contain words with missing pronunciations, those lines will be ignored by the HTK Layer Manager.\n\nDepending on your speech data, there are several ways to obtain phonemic transcriptions for words:\n\nLexical tagging\n\nCELEX - for British English, German, Dutch, using one of the CELEX layer managers.\nCMU Pronouncing Dictionary - for US English, using th CMU Pronouncing Dictionary layer manager.\nUnisyn - for various English varieties, using the Unisyn layer manager.\nDefine your own lexicon, and use the Flat File Dictionary layer manager to integrate it into LaBB-CAT.\n\nInferring pronunciation from orthography\n\nSpanish, using the Spanish Phonological Transcriber layer manager\nBas Web Service: G2P - for various languages.\nDefine your own simple mapping rules from orthography to phonology, using the Character Mapper layer manager.\n\n\nIf the speech corpus includes data in more than one language, it is possible to ensure that the utterances are phonemically tagged in a way that’s sensitive to the language of the specific utterance, using the language layers and attributes, and auxiliary layer managers.\nWhichever method you choose, you need a phonemes ‘word layer’ on which each word token is tagged with its pronunciation, before you can proceed with the forced-alignment steps below."
  },
  {
    "objectID": "howto/forced-alignment/htk-train-align.html#procedure-for-htk-forced-alignment",
    "href": "howto/forced-alignment/htk-train-align.html#procedure-for-htk-forced-alignment",
    "title": "HTK: Train-and-Align",
    "section": "",
    "text": "The broad steps for getting forced-alignments from HTK are:\n\nInstall HTK on the same computer that LaBB-CAT is installed on\nInstall the HTK Layer Manager, which integrates LaBB-CAT with HTK\nCreate and configure a new HTK layer in LaBB-CAT\nPick a speaker/participant in your database\nFill in the missing pronunciations for that participant\nRun forced alignment\nRepeat steps 4-6 for all the participants in your database\n\n\n\nAs mentioned above, HTK is a 3rd-party tool that you must download and install from the Cambridge University website.\n\nRegister at http://htk.eng.cam.ac.uk/register.shtml\nDownload the version of HTK that is appropriate for the computer that LaBB-CAT is install on:\nFor Windows systems, there are pre-compiled .exe files that you can download. For Unix-like systems, you need to download the source code, which you will then install following the provided instructions (you may also need to install the xorg-dev package before it will successfully compile).\nUnzip (for Windows) or compile and install (for Unix-like systems) the downloaded files on the computer that LaBB-CAT is installed on.\n\n\n\n\nThe HTK Layer Manager is a LaBB-CAT module that integrates LaBB-CAT with HTK.\n\nIn LaBB-CAT, select the layer managers option on the menu, which gives you a list of the layer managers already installed.\nAt the bottom of the page, follow the List of layer managers that are not yet installed link.\nLook for HTK in the list, and press it’s Install button.\nYou will see a form with boxes for filling in information.\n\nHTK Path must be set to the location where the HTK files are installed on your system. If this is already filled in, it’s probably correct. If it’s blank, you have to enter the full path for the HTK programs:\n\nOn Windows systems, this is where you unzipped the HTK .exe files - e.g. something like C:\\Downloads\\HTK\nOn Unix-like systems, this is probably /usr/local/bin, but you can verify this by entering which HCopy at a command shell prompt.\n\nHTK Working Folder will already have a default value, which is probably best left as-is\n\nPress Install Layer Manager\n\n\n\n\nOnce you’ve installed HTK and the HTK Layer Manager, you need to create a new layer for triggering and controlling forced alignment. This layer will itself contain a timestamp for each line/utterance it has force-aligned (and so it’s a ‘phrase’ layer), but during that process, the word and phone alignments will also be set on other layers.\n\nIn LaBB-CAT, select the phrase layers option\nAt the top of the page, there’s a blank form for creating a new layer; fill in the following details:\n\nLayer ID: HTK\nType: Text\nManager: HTK Manager\nAlignment: Time Intervals\nGenerate: Always\n\nPress New.\nYou will see the layer configuration page.\nCheck the online help if you want information about all the options, however, most likely the default options are approriate, except:\n\nPronunciaton Layer: this is the layer that provides the phonemic transcriptions for all the words; ensure you select the phonemes layer you created above.\nNB If you have created this layer but it doesn’t appear here as an option, it’s probably because the ‘layer type’ of your pronunciation layer is not set to ‘Phonological’, which will need to be changed in order for it to appear as an option here.\nNB In the list of options there’s also a layer called “pronounce”; this is a system layer for manually-added pronunciations, and you would only select that layer here if you have manually annotated pronunciations against every single word in your transcripts. You probably haven’t done that, so you don’t want to select the “pronounce” layer here.\nUse P2FA models: ensure this option is un-ticked.\n\nPress Save.\nIf you are confident all your transcripts include all pronunciations for all words, you can press Regenerate to force-align your whole corpus now. However, most likely you’ll need to proceed per-speaker, described below, in order to fill in missing pronunciations.\n\n\n\n\nTo start a forced-alignment process per-speaker, you need to first select a speaker who will be aligned. Then you will fill in any missing pronunciations. After that, HTK will automatically force-align their utterances.\n\nIn LaBB-CAT, select the participants optoin on the menu\nTick a speaker, and press the All Utterances button\nClick List\nOnce the paginated list of utterances appears, press the HTK button below.\nBasically you need to fill in the boxes with the pronunciations and click Save Pronunciations.\n\n\n\n\n\n\n\nNote\n\n\n\n\nYou don’t have to fill them all in at once, you can do a few, and click Save, which will save your work and list what’s left.\nYou don’t have to fill them all in, you can leave some empty and continue with the HTK forced-alignment by clicking Start (HTK will ignore any lines where the remaining unknown words appear, but the ones you filled in will be included).\nSome of the boxes will be initially filled in with a suggestion from the lexicon layer manager - these may or may not be correct, and aren’t saved until you save them.\nThe pronunciations have to be in the ‘DISC’ format - i.e. one character per phoneme, with no spaces. There’s a ‘helper’ link on the right of each pronunciation box - if you click it, it expands into a list of clickable phonemes - just the ones that aren’t ordinary letters, and diphthongs etc.\nThe search button lets you look up the lexicon for similar words - this probably won’t help for place names, but for words like “tarseal”, you can click the lookup button, enter “tar seal” in the box as two separate words, and you’ll get back the DISC pronunciation of each word, with clickable buttons to copy the given pronunciation into the box. This is useful for digits and numbers too, which may not be in the lexicon - so for “1”, search for “one” and copy the pronunciation.\nIf you click on the word itself, the transcript for the first instance of that word is opened, in case you want to listen to it, or in case it’s actually just a typo and you want to correct the transcript.\nIf you’re using CELEX, when you specify the pronunciations, it’s recommended to put syllable separators (-) and primary stress markers (’) too - e.g. for “tarseal” you can put t#sil but it would actually be better to put t#-’sil. These markers are entered into the dictionary even though they’re stripped out for HTK, and they may come in handy later (e.g. the syllable separators are used by the CELEX layer manager to count syllables).\n\n\n\nWhen you add pronunciations this way, they’re added to the dictionary and all the instances of those words in LaBB-CAT are updated with the pronunciations - not just the participant you’re looking at, but all participants in the database. So you only have to come up with a pronunciation for each word once.\n\nOnce you’ve filled in all the missing pronunciations, forced alignment will start automatically. If you want to start forced alignment before you’ve entered all pronunciations, click the Start button at the bottom of the page.\n\nYou should see a progress bar while the forced alignment is running. It will take a few minutes to complete.\n\nOnce HTK has produced the word and segment alignments, it:\n\nsets the start/end times of the words on the transcript layer accordingly,\nadds new phone annotations to the segments layer with the alignments of the phones, and\nsaves a timestamp in the htk layer.\n\nWhen the layer manager has finished, you’ll see a message saying “Complete - words and phones from selected utterances are now aligned.”"
  },
  {
    "objectID": "howto/forced-alignment/index.html",
    "href": "howto/forced-alignment/index.html",
    "title": "Forced Alignment",
    "section": "",
    "text": "Forced alignment is the automatic processing of recordings of utterance and their orthographic transcripts in order order to determing the start and end times of the individual words, and the phones within the words.\n\n\n\nForced Alignment\n\n\nThere are several ways that forced alignment can be achieved in LaBB-CAT:\n\nWebMAUS with BAS Web Services\nHTK using the Penn Aligner (P2FA) pre-trained acoustic models\nHTK by training your own acoustic models for alignment (‘train-and-align’)\nMFA using pre-trained acoustic models\nMFA by trining your own acoustic models for alignment (‘train-and-align’)\n\n\n\nThere are several tools and methods listed above for force-aligning your recordings, and each works well or badly depending on different factors. It can be difficult to know which method to use.\nYou can compare different forced alignment methods with your own data, in order to decide which method to use.\n\n\n\nBeing an unsupervised automatic process, the alignments are not always optimal. Various factors can degrade the quality of alignments:\n\nNot enough data (if you’re using the ‘train-and-align’ approach)\nPoor quality recording, background noises, etc.\nSimultaneous speech (ignored by default)\nInaccurate transcripts\nInaccurate utterance alignment\nLack of pause marking in the transcripts\nMismatched phonology between dictionary and speech. e.g. using a rhotic dictionary to align non-rhotic speech\n\nBecause of this, you should manually inspect and possibly correct at least some of your data.\nSometimes the above factors can cause alignment failure for some utterances; i.e. the utterance has no phone annotations created, the words are not aligned.\nYou can use LaBB-CAT’s search/export functionality to identify utterances that were not aligned.\n\n\n\nThere are two ways you can check/correct alignments:\n\nLaBB-CAT integrates with Praat\nLaBB-CAT integrates with the EMU-webApp\n\n\n\n\nOnce your data has been force-aligned, you will have start/end times for phones within words, which opens many possibilities for analysis and further annotation, for example.\n\nBatch processing of targeted tokens with Praat\nReconstruction of syllables"
  },
  {
    "objectID": "howto/forced-alignment/index.html#comparing-forced-alignment-methods",
    "href": "howto/forced-alignment/index.html#comparing-forced-alignment-methods",
    "title": "Forced Alignment",
    "section": "",
    "text": "There are several tools and methods listed above for force-aligning your recordings, and each works well or badly depending on different factors. It can be difficult to know which method to use.\nYou can compare different forced alignment methods with your own data, in order to decide which method to use."
  },
  {
    "objectID": "howto/forced-alignment/index.html#alignment-accuracy",
    "href": "howto/forced-alignment/index.html#alignment-accuracy",
    "title": "Forced Alignment",
    "section": "",
    "text": "Being an unsupervised automatic process, the alignments are not always optimal. Various factors can degrade the quality of alignments:\n\nNot enough data (if you’re using the ‘train-and-align’ approach)\nPoor quality recording, background noises, etc.\nSimultaneous speech (ignored by default)\nInaccurate transcripts\nInaccurate utterance alignment\nLack of pause marking in the transcripts\nMismatched phonology between dictionary and speech. e.g. using a rhotic dictionary to align non-rhotic speech\n\nBecause of this, you should manually inspect and possibly correct at least some of your data.\nSometimes the above factors can cause alignment failure for some utterances; i.e. the utterance has no phone annotations created, the words are not aligned.\nYou can use LaBB-CAT’s search/export functionality to identify utterances that were not aligned."
  },
  {
    "objectID": "howto/forced-alignment/index.html#checkingcorrecting-alignments",
    "href": "howto/forced-alignment/index.html#checkingcorrecting-alignments",
    "title": "Forced Alignment",
    "section": "",
    "text": "There are two ways you can check/correct alignments:\n\nLaBB-CAT integrates with Praat\nLaBB-CAT integrates with the EMU-webApp"
  },
  {
    "objectID": "howto/forced-alignment/index.html#after-alignment",
    "href": "howto/forced-alignment/index.html#after-alignment",
    "title": "Forced Alignment",
    "section": "",
    "text": "Once your data has been force-aligned, you will have start/end times for phones within words, which opens many possibilities for analysis and further annotation, for example.\n\nBatch processing of targeted tokens with Praat\nReconstruction of syllables"
  },
  {
    "objectID": "howto/forced-alignment/compare.html",
    "href": "howto/forced-alignment/compare.html",
    "title": "Comparing Forced Alignment Methods",
    "section": "",
    "text": "There are several tools and methods for force-aligning speech recordings, and each works well or badly depending on different factors. It can be difficult to know which method to use.\nYou can compare different forced alignment methods with your own data by configuring LaBB-CAT to allow multiple simultaneous alignments which can then be compared, in order to decide which method to use.\nTo do this, the broad process is:\n\nset up multiple forced alignment configurations, configured to output their word and phone alignments to separate layers (i.e. not the normal word and segment layers), and then 1.run each forced alignment configuration on some subset of the data, so that the separate word and phone alignment layers are populated with aligned intervals.\nFinally, you can compare the configurations, either by:\n\nselecting individual utterances, extracting all the alignments to a TextGrid with the audio, and doing an auditory/visual evaluation of each set of alignments, or\nmanually aligning a selection of utterances, and automatically comparing each set of automatic alignments with the corresponding manual alignments.\n\n\nOnce you have decided on a winner, you then delete the losing configurations, and reconfigure the winning one to output its alignments to LaBB-CAT’s word and segment layers, and run forced alignment again, on all the data.\n\n\nNormally, LaBB-CAT only allows one set of word/phone alignments at a time, and these are stored in the word and segment layers. In order to compare alignments, you need to be able to have multiple word/phone alignments in LaBB-CAT at the same time, so you can compare them side-by-side in order to choose the best one.\nIn order to set this up, you need to set up the forced aligment configurations to output their alignments to different layers, instead of the word and segment layers which are the default. The steps to achieve this are as follows:\n\nDecide which forced alignment configurations you want to compare. They may all involve the same layer manager module (e.g. if you want to compare HTK’s train/align procedure with its built-in P2FA pretrained acoustic models), or they may involve different layer manager modules (e.g. if you want to compare MFA’s pretrained models with the WebMAUS ones).\n(In these instructions, we show a comparison of HTK’s P2FA models with MFA’s pretrained US English models.)\nEnsure that the layer manager modules you need are installed in LaBB-CAT - i.e. install the HTK and the HTK Manager, or MFA and the MFA Manage, etc.\nIn LaBB-CAT, select the phrase layers menu option.\nYou will see a list of phrase layers, which may include language and entity.\nThe column headings at the top are also a form to fill in in order to add a new phrase layer. Fill it in with the following details:\n\nLayer ID: a descriptive name of the configuration, e.g. WebMAUS or HTK-P2FA\nType: Text (this layer will end up containing the word tokens)\nAlignment: Intervals (i.e. annotations with a start time and an end time)\nManager: select the layer manager for this forced alignment configuration, e.g. HTK Manager\nGenerate: Never (we will be running forced alignment manually on a selection of recordings, and we don’t want these forced alignments being automatically triggered when new transcripts are uploaded)\nProject: this can be left as-is, unless you have already created a ‘project’ to categorise these layers under.\nDescription: A descriptive note something like “Word alignments produced by HTK using the Penn Forced Aligner pretrained acoustic models (P2FA)”\nThe new layer form filled in with Layer ID = HTK-P2FA, Type = Text, Alignment = Intervals, Manager = HTK Manager and Generate = Never. There is a ‘New’ button on the far right.\n\n\nPress the New button.\nThe layer’s configuration form will appear.\nFill in the configuration form as required for this configuration, e.g. if it’s HTK with P2FA models, tick the Use P2FA models checkbox, etc.\n\nThe configuration form will also allow you to configure which layer the word alignments and phone alignments are saved to, with a setting like Word Alignment Layer set to word by default, and Phone Alignment Layer set to segment by default.\nChange the Word Alignment Layer to be the layer itself (i.e. if the layer you just added is called “HTK-P2FA”, select the HTK-P2FA option in the dropdown list next to Word Alignment Layer).\nChange the Phone Alignment Layer by selecting the [add new layer] option.\nYou will be asked what you want the new layer to be called.\n\nYou can leave this as the default value, which will be something like “HTK-P2FAPhone”, and press OK\nThere may also be an Utterance Tag Layer setting, which by default has the layer itself selected. We’re already using the layer itself for the word alignments, and don’t want it to also contain a time-stamp for when the alignment was done, so we are going to un-set this setting.\nSet Utterance Tag Layer to [none].\n\nPress the Set Parameters button to save the configuration.\nRepeat steps 3-11 for each configuration you want to compare.\nFor this example, we have added an MFA Manager layer… . …using the english_us_arpa dictionary/models.\n\nSelect the phrase layers option from the menu.\nYou will see that, in addition to the layers you added for the word alignments, the corresponding phone alignment layers have also been automatically created.\n\n\nNow you have multiple forced alignment methods set up, and none of them is configured to output their alignments on the word and segment layers. Instead, each method outputs its alignments to different ‘phrase’ layers.\nThe configurations are set, but there are no alignments to compare yet.\n\n\n\nOnce you have set up all the configurations you need, you can run each forced alignment configuration on a subset of utterances. In order to compare side by side, you should force-align the same utterance for all configurations.\n\nDecide what subset of utterances to align.\n\nIf you are comparing configurations that all use pre-trained acoustic models, the subset can be as small as a handful of utterances from different speakers, or even single utterance, as the amount of data included in the forced alignment run won’t effect the quality of the alignments.\nIf you are comparing configurations that first train acoustic models and then force-align utterances based on those models (i.e. the ‘train/align’ method), you will need to have a larger subset, which might be all the utterances of one or more speakers, or even the whole corpus. How big the subset needs to be depends on the configuration (e.g. for train/align with HTK, you’ll need at least 5 minutes of speech, and for traing/align with MFA, you may need as much as 3-5 hours of speech)\n\nIn LaBB-CAT, collect together your subset of utterances on a ‘matches’ page.\ne.g. if it is to be all the utterances of a selected speaker or speakers:\n\ngo to participants page,\nfind the desired participants,\ntick them\n\npress the All Utterances button, and\npress List.\n\nAlternatively, if you run a search using specific participant and transcript criteria, you will also be shown a ‘matches’ page that lists matching utterances, and allows you to export data and perform a number of other operations on the utterances.\nHowever you determine your subset of utterances, you need to end up at a ‘matches’ page like this:\n\nAll utterances will be ticked by default, but you can manually select utterances from the list by un-ticking Select all results and then ticking individual utterances to align.\nAt the bottom, there will be a button for each of the forced alignment configuration layers you added earlier.\n\nPress the first button.\nThis will start the forced alignment process for that configuration, including only the selected utterances. This may involve filling in missing pronunciations, and then you will see a progress bar while the utterances are being aligned.\n\nOnce the forced alignment for this configuration is complete, repeat steps 2 and 3 above for each of the other forced alignment configurations.\n\nOnce you’ve completed the last forced alignment, you will have multiple sets of alignments simultaneously saved in different layers in LaBB-CAT, ready for comparison.\n\n\n\nThe easiest way to directly compare the different sets of alignments is by extracting utterances to TextGrids, and opening them with corresponding audio in Praat. This can be done directly from the transcript page, using LaBB-CAT’s Praat Integration.\n\nIn LaBB-CAT, select the transcripts option on the menu.\nFind one of the transcripts that includes utterances in the subset you force-aligned above, and click its name to open the transcript page.\nAt the top of the transcript there is a list of layers, which should now include options for the forced alignment layers you added earlier, and their corresponding phone alignment layers.\n\nTick all of the forced-alignment layers.\n\nThis will cause the aligned annotations for these layers to be displayed on the transcript page, for the utterances that have been forced-aligned.\n\nThe transcript page is not intended to accurately represent time alignment at this granularity, so the display will look cramped and confusing. The word and phone annotations on each layer will probably not line up with the corresponding transcript words below.\nClick any word in the utterance (i.e. from the olive-coloured words), and select Open TextGrid in Praat from the resulting menu.\n\nThis will open Praat and show the utterance TextGrid with the corresponding audio. The TextGrid will include:\n\na tier named after each forced-alignment configuration layer, containing the word alignments from that configuration\na tier named after each configuration’s Phone Alignment Layer, containing the phone alignments from that configuration\na word tier at the bottom, containing the main word alignments - most likely these will be evenly spaced out during the duration of the turn, and will not line up with the words in the audio; this is because the main word tokens haven’t been aligned yet, and won’t be until you select the winning configuration and change it to output it’s word alignments to the word layer (and phone alignments to the segment layer)\n\n\n\n\nMultiple sets of alignments shown in Praat\n\n\nIn Praat, you can compare the phone/word alignments of each configuration, by selecting intervals, examining how they line up with the spectrogram above, and playing the corresponding audio to get an impression of how well aligned the segment is with the audio.\nThis type of evaluation is impressionistic, but after comparing a number of utterances across a number of speakers this way, it may become clear which configuration has more accurate alignments."
  },
  {
    "objectID": "howto/forced-alignment/compare.html#setting-up-multiple-forced-alignment-configurations",
    "href": "howto/forced-alignment/compare.html#setting-up-multiple-forced-alignment-configurations",
    "title": "Comparing Forced Alignment Methods",
    "section": "",
    "text": "Normally, LaBB-CAT only allows one set of word/phone alignments at a time, and these are stored in the word and segment layers. In order to compare alignments, you need to be able to have multiple word/phone alignments in LaBB-CAT at the same time, so you can compare them side-by-side in order to choose the best one.\nIn order to set this up, you need to set up the forced aligment configurations to output their alignments to different layers, instead of the word and segment layers which are the default. The steps to achieve this are as follows:\n\nDecide which forced alignment configurations you want to compare. They may all involve the same layer manager module (e.g. if you want to compare HTK’s train/align procedure with its built-in P2FA pretrained acoustic models), or they may involve different layer manager modules (e.g. if you want to compare MFA’s pretrained models with the WebMAUS ones).\n(In these instructions, we show a comparison of HTK’s P2FA models with MFA’s pretrained US English models.)\nEnsure that the layer manager modules you need are installed in LaBB-CAT - i.e. install the HTK and the HTK Manager, or MFA and the MFA Manage, etc.\nIn LaBB-CAT, select the phrase layers menu option.\nYou will see a list of phrase layers, which may include language and entity.\nThe column headings at the top are also a form to fill in in order to add a new phrase layer. Fill it in with the following details:\n\nLayer ID: a descriptive name of the configuration, e.g. WebMAUS or HTK-P2FA\nType: Text (this layer will end up containing the word tokens)\nAlignment: Intervals (i.e. annotations with a start time and an end time)\nManager: select the layer manager for this forced alignment configuration, e.g. HTK Manager\nGenerate: Never (we will be running forced alignment manually on a selection of recordings, and we don’t want these forced alignments being automatically triggered when new transcripts are uploaded)\nProject: this can be left as-is, unless you have already created a ‘project’ to categorise these layers under.\nDescription: A descriptive note something like “Word alignments produced by HTK using the Penn Forced Aligner pretrained acoustic models (P2FA)”\nThe new layer form filled in with Layer ID = HTK-P2FA, Type = Text, Alignment = Intervals, Manager = HTK Manager and Generate = Never. There is a ‘New’ button on the far right.\n\n\nPress the New button.\nThe layer’s configuration form will appear.\nFill in the configuration form as required for this configuration, e.g. if it’s HTK with P2FA models, tick the Use P2FA models checkbox, etc.\n\nThe configuration form will also allow you to configure which layer the word alignments and phone alignments are saved to, with a setting like Word Alignment Layer set to word by default, and Phone Alignment Layer set to segment by default.\nChange the Word Alignment Layer to be the layer itself (i.e. if the layer you just added is called “HTK-P2FA”, select the HTK-P2FA option in the dropdown list next to Word Alignment Layer).\nChange the Phone Alignment Layer by selecting the [add new layer] option.\nYou will be asked what you want the new layer to be called.\n\nYou can leave this as the default value, which will be something like “HTK-P2FAPhone”, and press OK\nThere may also be an Utterance Tag Layer setting, which by default has the layer itself selected. We’re already using the layer itself for the word alignments, and don’t want it to also contain a time-stamp for when the alignment was done, so we are going to un-set this setting.\nSet Utterance Tag Layer to [none].\n\nPress the Set Parameters button to save the configuration.\nRepeat steps 3-11 for each configuration you want to compare.\nFor this example, we have added an MFA Manager layer… . …using the english_us_arpa dictionary/models.\n\nSelect the phrase layers option from the menu.\nYou will see that, in addition to the layers you added for the word alignments, the corresponding phone alignment layers have also been automatically created.\n\n\nNow you have multiple forced alignment methods set up, and none of them is configured to output their alignments on the word and segment layers. Instead, each method outputs its alignments to different ‘phrase’ layers.\nThe configurations are set, but there are no alignments to compare yet."
  },
  {
    "objectID": "howto/forced-alignment/compare.html#running-multiple-independent-forced-alignments",
    "href": "howto/forced-alignment/compare.html#running-multiple-independent-forced-alignments",
    "title": "Comparing Forced Alignment Methods",
    "section": "",
    "text": "Once you have set up all the configurations you need, you can run each forced alignment configuration on a subset of utterances. In order to compare side by side, you should force-align the same utterance for all configurations.\n\nDecide what subset of utterances to align.\n\nIf you are comparing configurations that all use pre-trained acoustic models, the subset can be as small as a handful of utterances from different speakers, or even single utterance, as the amount of data included in the forced alignment run won’t effect the quality of the alignments.\nIf you are comparing configurations that first train acoustic models and then force-align utterances based on those models (i.e. the ‘train/align’ method), you will need to have a larger subset, which might be all the utterances of one or more speakers, or even the whole corpus. How big the subset needs to be depends on the configuration (e.g. for train/align with HTK, you’ll need at least 5 minutes of speech, and for traing/align with MFA, you may need as much as 3-5 hours of speech)\n\nIn LaBB-CAT, collect together your subset of utterances on a ‘matches’ page.\ne.g. if it is to be all the utterances of a selected speaker or speakers:\n\ngo to participants page,\nfind the desired participants,\ntick them\n\npress the All Utterances button, and\npress List.\n\nAlternatively, if you run a search using specific participant and transcript criteria, you will also be shown a ‘matches’ page that lists matching utterances, and allows you to export data and perform a number of other operations on the utterances.\nHowever you determine your subset of utterances, you need to end up at a ‘matches’ page like this:\n\nAll utterances will be ticked by default, but you can manually select utterances from the list by un-ticking Select all results and then ticking individual utterances to align.\nAt the bottom, there will be a button for each of the forced alignment configuration layers you added earlier.\n\nPress the first button.\nThis will start the forced alignment process for that configuration, including only the selected utterances. This may involve filling in missing pronunciations, and then you will see a progress bar while the utterances are being aligned.\n\nOnce the forced alignment for this configuration is complete, repeat steps 2 and 3 above for each of the other forced alignment configurations.\n\nOnce you’ve completed the last forced alignment, you will have multiple sets of alignments simultaneously saved in different layers in LaBB-CAT, ready for comparison."
  },
  {
    "objectID": "howto/forced-alignment/compare.html#auditoryvisual-comparison-of-the-alignments",
    "href": "howto/forced-alignment/compare.html#auditoryvisual-comparison-of-the-alignments",
    "title": "Comparing Forced Alignment Methods",
    "section": "",
    "text": "The easiest way to directly compare the different sets of alignments is by extracting utterances to TextGrids, and opening them with corresponding audio in Praat. This can be done directly from the transcript page, using LaBB-CAT’s Praat Integration.\n\nIn LaBB-CAT, select the transcripts option on the menu.\nFind one of the transcripts that includes utterances in the subset you force-aligned above, and click its name to open the transcript page.\nAt the top of the transcript there is a list of layers, which should now include options for the forced alignment layers you added earlier, and their corresponding phone alignment layers.\n\nTick all of the forced-alignment layers.\n\nThis will cause the aligned annotations for these layers to be displayed on the transcript page, for the utterances that have been forced-aligned.\n\nThe transcript page is not intended to accurately represent time alignment at this granularity, so the display will look cramped and confusing. The word and phone annotations on each layer will probably not line up with the corresponding transcript words below.\nClick any word in the utterance (i.e. from the olive-coloured words), and select Open TextGrid in Praat from the resulting menu.\n\nThis will open Praat and show the utterance TextGrid with the corresponding audio. The TextGrid will include:\n\na tier named after each forced-alignment configuration layer, containing the word alignments from that configuration\na tier named after each configuration’s Phone Alignment Layer, containing the phone alignments from that configuration\na word tier at the bottom, containing the main word alignments - most likely these will be evenly spaced out during the duration of the turn, and will not line up with the words in the audio; this is because the main word tokens haven’t been aligned yet, and won’t be until you select the winning configuration and change it to output it’s word alignments to the word layer (and phone alignments to the segment layer)\n\n\n\n\nMultiple sets of alignments shown in Praat\n\n\nIn Praat, you can compare the phone/word alignments of each configuration, by selecting intervals, examining how they line up with the spectrogram above, and playing the corresponding audio to get an impression of how well aligned the segment is with the audio.\nThis type of evaluation is impressionistic, but after comparing a number of utterances across a number of speakers this way, it may become clear which configuration has more accurate alignments."
  },
  {
    "objectID": "howto/forced-alignment/correction-praat.html",
    "href": "howto/forced-alignment/correction-praat.html",
    "title": "Checking/Correcting Alignments: Praat",
    "section": "",
    "text": "Checking/Correcting Alignments: Praat\nAfter force-aligning transcripts, the alignments can be checked and corrected directly from LaBB-CAT, using the Praat integration with the transcript page.\n\n\n\nRound-trip of alignnments, coming from LaBB-CAT to Praat, with manual corrections imported back into LaBB-CAT\n\n\nTo check the alignments:\n\nIn LaBB-CAT, open a transcript page.\nTick both the forced-aligner’s layer (e.g. HTK) and the segments layer.\n\nYou will see which lines have been force-aligned, as they have an HTK timestamp, and have the segment layer filled in. If it has missed some lines, this is most likely because there is an unknown word, another speaker speaking at the same time, or possible HTK simply failed to align the line (there are various reasons this happens, including not enough data for training, noisy recordings, inaccurate transcription, etc.).\nThe interactive transcript page doesn’t show you the alignments of the words or phones, but you can see those using Praat. You can open individual utterances in Praat directly from the transcript page, but first, the LaBB-CAT/Praat integration has to be set up; this only has to be done once:\n\nOn the top-right of the page, above the playback controls, there’s a Praat icon - click it.\nFollow the instructions that appear (these vary depending on what web browser you use).\nYou may need to grant a browser extension permission to install, and it’s possible you will need a connection to the internet in order to download this extension.\nYou also may be asked where Praat is installed; Navigate to the location where Praat is installed on your computer, and double-click the Praat.exe file (on some systems the file may simply be called Praat).\nThe Praat program may open, and then immediately close, as LaBB-CAT tests it can communicate with Praat.\n\nNow Praat integration has been set up, and you should be able to access Praat options in the transcript page from now on…\n\nClick on a line that has been aligned, and select the Open Text Grid in Praat option on the menu.\nPraat should open, and show you a spectrogram of the line’s audio, with a TextGrid below that includes the words and the segments.\nIf you click on a word, and hit the key, the word’s interval is played. Try out various words, and see what you think about how accurate the forced aligner has been with its alignment.\nTry this out with different lines in the transcript.\n\nYou will see that in some cases the alignment is pretty good, and in other cases, it’s not so good.\n\nAdjust the alignments of the words and phones so that they’re more accurate, and then click the Import Changes button in the transcript page (in LaBB-CAT).\n\nThese changes are flagged as manual edits, so if forced-alignment is run again, they will not be over-written with new bad alignments. Therefore it’s important that the changes you make are actually improvements, because the forced aligner will never change them again.\nThere are some rules about what you can change:\n\nYou’re not allowed to add or delete words (if this is necessary, it should be done by correcting the transcript instead).\nAll the phones must be within the bounds of their own word.\nThe start of the first phone should line up with the start of the word, and the end of the last phone should line up with the end of the word.\nYou should not change the alignment of the utterance itself (which would only be possible if you select the Open Text Grid incl. ± 1 utterance in Praat option).\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023-2024 NZILBB"
  },
  {
    "objectID": "howto/forced-alignment/bas-services-manager.html",
    "href": "howto/forced-alignment/bas-services-manager.html",
    "title": "BAS Web Service Manager and WebMAUS",
    "section": "",
    "text": "The Bavarian Archive for Speech Signals (BAS), has kindly published a set of speech processing web services including one for for forced alignment called WebMAUS. You can use this service yourself directly, using your web browser, but LaBB-CAT also has a module for using it automatically, called the BAS Services Manager.\nThe general process is illustrated below:\n\n\n\nTranscripts/recordings from LaBB-CAT are sent to WebMAUS, and the resulting alignments are saved in LaBB-CAT\n\n\nNB: Using WebMAUS for forced alignment requires LaBB-CAT to send your recordings and transcripts over the internet to a third party. Although point 3 of the BAS Web Services Terms of Service makes clear that uploaded data is deleted after 24 hours, using the service is only suitable in situations in which you have consent from participants to do so.\n\nAlbanian\nAustralian Aboriginal Languages\nAfrikaans\nAlbanian\nBasque\nCatalan\nDutch\nEnglish\nEstonian\nFinnish\nFrench\nGeorgian\nGerman\nHungarian\nItalian\nJapanese\nKunwinjku\nLuxembourgish\nMaltese\nNorwegian\nPolish\nRomanian\nRussian\nSpanish\nSwedish\nYolŋu Matha\n\nLaBB-CAT must be able to identify which language each transcript is in, so you must ensure the language is set either\n\nin the transcript’s Language transcript attribute, or\non the corpora page (where you can define the language for all transcripts each corpus).\n\nThe available language options can be set in LaBB-CAT by going to the transcript attributes page and clicking the Options button of the “language” attribute. The value must be a two-letter ISO639-1 code optionally appended with a two-letter country code - e.g. “en” or “en-NZ”.\n\n\n\nIn LaBB-CAT, select the layer managers option on the menu, which gives you a list of the layer managers already installed.\nAt the bottom of the page, follow the List of layer managers that are not yet installed link.\nLook for BAS Web Services Manager in the list, and press it’s Install button.\nFollow the “terms of usage” link and read the terms.\nClose the terms page, returning to LaBB-CAT.\nSelect true for the “Accept Terms of Usage” option\nPress Install.\nYou will see a page of information about the Layer Manager, including instructions on how to set up forced alignment.\n\n\n\n\n\nSelect the phrase layers option on the menu\nAt the top of the page, there’s a blank form for creating a new layer; fill in the following details:\n\nLayer ID: MAUS\nType: Text\nManager: BAS Web Services Manager\nAlignment: Time Intervals\nGenerate: Always\n\nPress New.\nYou will see a form that allows you to configure the layer; check the online help for that page to guide you. The main choice is the “Phoneme encoding”: the default option, DISC, is probably the best because using this phoneme encoding ensures the layer will work well with other modules, and will be easily searchable. However, it is possible to choose sampa instead, in which case the layer type of the segments layer should be set to Text.\nPress Save\nIf you want to immediately force-align all the recordings in your corpus, press Regenerate."
  },
  {
    "objectID": "howto/forced-alignment/bas-services-manager.html#install-the-layer-manager",
    "href": "howto/forced-alignment/bas-services-manager.html#install-the-layer-manager",
    "title": "BAS Web Service Manager and WebMAUS",
    "section": "",
    "text": "In LaBB-CAT, select the layer managers option on the menu, which gives you a list of the layer managers already installed.\nAt the bottom of the page, follow the List of layer managers that are not yet installed link.\nLook for BAS Web Services Manager in the list, and press it’s Install button.\nFollow the “terms of usage” link and read the terms.\nClose the terms page, returning to LaBB-CAT.\nSelect true for the “Accept Terms of Usage” option\nPress Install.\nYou will see a page of information about the Layer Manager, including instructions on how to set up forced alignment."
  },
  {
    "objectID": "howto/forced-alignment/bas-services-manager.html#set-up-a-layer-for-triggering-forced-alignment",
    "href": "howto/forced-alignment/bas-services-manager.html#set-up-a-layer-for-triggering-forced-alignment",
    "title": "BAS Web Service Manager and WebMAUS",
    "section": "",
    "text": "Select the phrase layers option on the menu\nAt the top of the page, there’s a blank form for creating a new layer; fill in the following details:\n\nLayer ID: MAUS\nType: Text\nManager: BAS Web Services Manager\nAlignment: Time Intervals\nGenerate: Always\n\nPress New.\nYou will see a form that allows you to configure the layer; check the online help for that page to guide you. The main choice is the “Phoneme encoding”: the default option, DISC, is probably the best because using this phoneme encoding ensures the layer will work well with other modules, and will be easily searchable. However, it is possible to choose sampa instead, in which case the layer type of the segments layer should be set to Text.\nPress Save\nIf you want to immediately force-align all the recordings in your corpus, press Regenerate."
  },
  {
    "objectID": "howto/forced-alignment/mfa-pretrained-models.html",
    "href": "howto/forced-alignment/mfa-pretrained-models.html",
    "title": "MFA and Pretrained Acoustic Models",
    "section": "",
    "text": "The Montreal Forced Aligner (MFA) is a third-party tool developed by Michael McAuliffe and others for time aligning orthographic and phonological forms from a pronunciation dictionary to orthographically transcribed audio files. It is open source software based on the Kaldi ASR toolkit.\nLaBB-CAT includes a layer manager module called “MFA Manager” which integrates with MFA in order to facilitate forced alignment of LaBB-CAT corpus data.\nThe layer manager can work in two modes:\n\nTrain and Align - acoustic models are trained on the data you want to align, which can be in any language as long as you have a pronunciation dictionary for it.\nPre-trained Models/Dictionaries - pre-trained models and pronunciation dictionaries are supplied by the Montreal Forced Aligner and used for forced alignment. Languages for which dictionaries are available listed on the MFA website and include:\n\nEnglish\nFrench\nGerman\nBrazilian Portuguese\nSpanish\nCatalan\n\n\nThese instructions assume that your corpus is in one of these languages, and uses the Pre-trained Models/Dictionaries approach…\n\n\nMFA is a 3rd-party tool that LaBB-CAT integrates with via a Layer Manager module. MFA is not included as part of LaBB-CAT, and so it must be installed on the server you have installed LaBB-CAT on before you can integrate LaBB-CAT with it.\nIf MFA has not been installed already, please follow the following steps, depending on the operatings system of your LaBB-CAT server. This is a one-time process.\n\n\nTo install the Montreal Forced Aligner on Linux systems for all users, so that your web server can access it if required:\n\nDownload Miniconda:\nwget https://repo.anaconda.com/miniconda/Miniconda3-py38\\_4.10.3-Linux-x86\\_64.sh\nStart the installer:\nsudo bash Miniconda3-py38_4.10.3-Linux-x86_64.sh\nWhen asked the location to install Miniconda, use:\n/opt/conda\nWhen asked whether the installer should initialize Miniconda, this is unnecessary so you can respond no\nChange ownership of the conda files:\nsudo chown -R $USERNAME:$USERNAME /opt/conda\nMake conda accessible to all users (so you web server can access MFA):\nchmod -R go-w /opt/conda\nchmod -R go+rX /opt/conda\n\nInstall the Montreal Forced Aligner. sudo /opt/conda/bin/conda create -n aligner -c conda-forge montreal-forced-aligner\n\n\n\n\nTo install the Montreal Forced Aligner on Windows systems for all users, so that your web server can access it if required:\n\nDownload the Miniconda installer:\nhttps://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe\nStart the installer by double-clicking it.\nWhen asked, select the “Install for all users” option. This will install conda somewhere like. C:\\ProgramData\\Miniconda3\nWhen asked, tick the “add to PATH” option.\nInstall the Montreal Forced Aligner by specifying a path to the environment. conda create -c conda-forge -p C:\\ProgramData\\Miniconda3\\envs\\aligner montreal-forced-aligner\n\n\n\n\n\nOnce MFA has been installed, you have to install the MFA Manager, which is the LaBB-CAT module that provides MFA with all the data it needs, and then saves to alignments MFA produces back to your database.\n\nSelect the layer managers menu option.\nFollow the List of layer managers that are not yet installed link.\nFind MFA Manager in the list, and press its Install button and then press Install again.\nAs long as MFA has been installed for all users, you should see a box that’s already filled in with the location that MFA was installed to.\nClick Configure to continue the layer manager installation.\nYou will see a window open with some information about integrating with MFA, including the information you’ve already read above.\nNow you need to add a phrase layer for the HTK configuration:\n\nLayer ID: mfa\nType: Text\nAlignment: Intervals\nManager: MFA Manager\nGenerate: always\nDescription: MFA alignment time\n\nWhen you configure the layer, set the following options:\n\nDictionary Name: the dictionary language, e.g. english_uk_mfa\nPretrained Acoustic Models: the models language, e.g. english_mfa\nThe rest of the options can be left as their default values.\nIf you’re curious about what the configuration options do, hover your mouse over each option to see a `tool tip’ that describes what the option is for.\n\nPress Set Parameters\nPress Regenerate\nYou will see a progress bar while LaBB-CAT force-aligns all the transcripts in the corpus, which may take a few minutes.\nWhen the layer manager has finished, you’ll see a message saying:\nComplete - words and phones from selected utterances are now aligned."
  },
  {
    "objectID": "howto/forced-alignment/mfa-pretrained-models.html#mfa-installation",
    "href": "howto/forced-alignment/mfa-pretrained-models.html#mfa-installation",
    "title": "MFA and Pretrained Acoustic Models",
    "section": "",
    "text": "MFA is a 3rd-party tool that LaBB-CAT integrates with via a Layer Manager module. MFA is not included as part of LaBB-CAT, and so it must be installed on the server you have installed LaBB-CAT on before you can integrate LaBB-CAT with it.\nIf MFA has not been installed already, please follow the following steps, depending on the operatings system of your LaBB-CAT server. This is a one-time process.\n\n\nTo install the Montreal Forced Aligner on Linux systems for all users, so that your web server can access it if required:\n\nDownload Miniconda:\nwget https://repo.anaconda.com/miniconda/Miniconda3-py38\\_4.10.3-Linux-x86\\_64.sh\nStart the installer:\nsudo bash Miniconda3-py38_4.10.3-Linux-x86_64.sh\nWhen asked the location to install Miniconda, use:\n/opt/conda\nWhen asked whether the installer should initialize Miniconda, this is unnecessary so you can respond no\nChange ownership of the conda files:\nsudo chown -R $USERNAME:$USERNAME /opt/conda\nMake conda accessible to all users (so you web server can access MFA):\nchmod -R go-w /opt/conda\nchmod -R go+rX /opt/conda\n\nInstall the Montreal Forced Aligner. sudo /opt/conda/bin/conda create -n aligner -c conda-forge montreal-forced-aligner\n\n\n\n\nTo install the Montreal Forced Aligner on Windows systems for all users, so that your web server can access it if required:\n\nDownload the Miniconda installer:\nhttps://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe\nStart the installer by double-clicking it.\nWhen asked, select the “Install for all users” option. This will install conda somewhere like. C:\\ProgramData\\Miniconda3\nWhen asked, tick the “add to PATH” option.\nInstall the Montreal Forced Aligner by specifying a path to the environment. conda create -c conda-forge -p C:\\ProgramData\\Miniconda3\\envs\\aligner montreal-forced-aligner"
  },
  {
    "objectID": "howto/forced-alignment/mfa-pretrained-models.html#forced-alignment",
    "href": "howto/forced-alignment/mfa-pretrained-models.html#forced-alignment",
    "title": "MFA and Pretrained Acoustic Models",
    "section": "",
    "text": "Once MFA has been installed, you have to install the MFA Manager, which is the LaBB-CAT module that provides MFA with all the data it needs, and then saves to alignments MFA produces back to your database.\n\nSelect the layer managers menu option.\nFollow the List of layer managers that are not yet installed link.\nFind MFA Manager in the list, and press its Install button and then press Install again.\nAs long as MFA has been installed for all users, you should see a box that’s already filled in with the location that MFA was installed to.\nClick Configure to continue the layer manager installation.\nYou will see a window open with some information about integrating with MFA, including the information you’ve already read above.\nNow you need to add a phrase layer for the HTK configuration:\n\nLayer ID: mfa\nType: Text\nAlignment: Intervals\nManager: MFA Manager\nGenerate: always\nDescription: MFA alignment time\n\nWhen you configure the layer, set the following options:\n\nDictionary Name: the dictionary language, e.g. english_uk_mfa\nPretrained Acoustic Models: the models language, e.g. english_mfa\nThe rest of the options can be left as their default values.\nIf you’re curious about what the configuration options do, hover your mouse over each option to see a `tool tip’ that describes what the option is for.\n\nPress Set Parameters\nPress Regenerate\nYou will see a progress bar while LaBB-CAT force-aligns all the transcripts in the corpus, which may take a few minutes.\nWhen the layer manager has finished, you’ll see a message saying:\nComplete - words and phones from selected utterances are now aligned."
  },
  {
    "objectID": "howto/manual-annotation/doccano.html",
    "href": "howto/manual-annotation/doccano.html",
    "title": "Phrase Tagging with Doccano",
    "section": "",
    "text": "Doccano is an open-source data labeling tool intended for machine learning practitioners. It allows you to tag words and phrases in texts with a very easy-to-use drag and select user interface.\n\nYou can use Doccano to tag phrases and import your tags into phrase layers in LaBB-CAT. The broad steps of the process are:\n\nExport a selected set of transcripts from LaBB-CAT to the Doccano JSONL format\nImport the resulting file into Doccano\nTag the texts as desired\nExport the tagged texts from Doccano to a JSONL file\nImport the tagged JSONL file into LaBB-CAT\n\n\n\n\nIn LaBB-CAT, open the transcripts page.\nUse the filters at the top to narrow the list down to the transcripts you want to export, and/or tick the target transcripts.\nClick Export Format.\nA list of layers will appear, with a list of formats below.\nIf you wish to include any existing phrase/span layers in Doccano, tick the corresponding layers in the list.\n\nNB: Annotations on the layers you select here will be displayed in Doccano but cannot be edited; any changes to these annotations will be ignored when re-importing the dataset into LaBB-CAT.\nBelow the list of layers, there’s a dropdown list of export formats.\nSelect Doccano JSONL Dataset.\n\nClick Export Format\nSave the resulting …jsonl file.\n\n\n\n\n\nIn Doccano, you will need to create a project to import your texts into. Click Projects on the top right.\nPress Create on the top left.\nSelect the Sequence Labelling option.\n Sequence Labelling option in Doccano is ticked\nEnter a name and description for your project.\nTick the Allow overlapping entity option.\n\nPress Create at the bottom.\nThis will create the project and take you to its Home page.\nPress Dataset on the left.\n\nMove the mouse over the Actions button at the top, and select the Import Dataset option.\n\nSelect JSONL as the File format.\nYou can leave the other options that appear with their default values.\nFind the …jsonl file you exported from LaBB-CAT earlier, and drap/drop it on to the grey area labelled Drop files here…\n(Alternatively you can click on the Drop files here… area, and find/select the …jsonl file.)\n\nPress Import.\n\nOnce the import is complete, you will see a list of texts on the Dataset page. The Metadata column will be full of text and numbers - this is normal; LaBB-CAT includes information in the Metadata that it needs to import the text back into LaBB-CAT correctly.\n\n\n\nThe imported dataset includes metadata to aid re-import into LaBB-CAT\n\n\n\n\n\nBefore adding annotations to the texts, you need to create Labels in Doccano. These are the annotations you’ll be able to add to words/phrases in the texts.\n\nIn Doccano, click Labels on the left-hand menu.\nIf you exported any additional phrase/span layers from LaBB-CAT, you will see labels for the resulting annotations already listed here.\n\nEach imported label is prefixed with the ID of the LaBB-CAT layer it came from, followed by a colon. This is the pattern you must follow with the labels you create.\nMove the mouse over the Actions button at the top and click the Create Label option.\n\nEnter the label. This should be using the format:\n{LaBB-CAT-Layer-ID}:{LaBB-CAT-Label}\ne.g if you intend for your new annotations to be added to a LaBB-CAT layer called “narrative-action” and one of the possible labels in LaBB-CAT will be “complicating action”, then the Label you create in Doccano should be: narrative-action:complicating action\n\nPick a colour for the label if you wish.\nAssuming you want to add more than one label, click Save and add another.\nRepeat the above steps for each label you would like to annotate with.\nOnce you’ve finished, click the Labels option on the left-hand menu.\nYou should see the label’s you’ve added, listed after the imported ones.\n\n\n\n\nNow that you’ve configured the labels you’re going to use, you can annotate the texts you imported:\nIn Doccano click Start Annotation at the top left.\n(Alternatively, you can click Dataset and then press the Annotate button on a text of your choice)\nYou will see one of the texts you imported.\n\n\n\nTranscript including annotations exported from LaBB-CAT\n\n\nThe participant ID of the speaker appears at the beginning of each speaker turn, and if you exported phrase/span annotations from LaBB-CAT, they will appear tagging the corresponding regions of the text.\nTo tag a phrase in the text, simply click and drag over the phrase to select it. A menu of tags will appear.\n\n\n\nClick/drag for label menu\n\n\nWhen you click the desired tag, it will be added to the text.\n\n\n\nNew tags appear in the text\n\n\n\n\n\nDoccano includes a mode for tagging in which you can pre-select the Label you want to use, and then the selected Label is automatically used whenever you click/drag a phrase. This mode may be quicker as it involves fewer clicks overall.\nTo use this method of tagging, scroll to the top of the text, and click the desired Label in the list on the top right.\n\n\n\nPre-select a label in the label Types list at the top right of the text\n\n\nNow, whenever you click/drag a phrase in the tex, it will immediately be tagged with the selected Label.\nChanges are automatically saved. Once you’ve added all the tags you want in this text, you can move to the next by using the navigation buttons at the top right of the text.\n\n\n\nButtons for navigating to the first, previous, next, and last text\n\n\n\n\n\n\nOne you’ve finished annotating all texts, you need to export them with the new tags so they can be imported into LaBB-CAT.\n\nIn Doccano, click the Dataset option on the left-hand menu.\nMove the mouse over Actions at the top and select Export Dataset.\n\nSelect JSONL as the File format\nPress Export.\nSave the resulting …zip file.\nExtract the …jsonl file that is contained in the …zip file you just saved.\n\n\n\n\n\n\nWhen you import the …jsonl file into LaBB-CAT, it will extract the new Labels you’ve added, and assume that each Label is in the format:\n{LaBB-CAT-Layer-ID}:{LaBB-CAT-Label}\nEach label will be split on the colon, and the left part will be assumed to be a layer ID, and the right part will be assumed to be the label for annotations on that layer.\nIf you have added layer ID prefixes for layers that don’t exist yet in LaBB-CAT, you have to create the LaBB-CAT layers before importing the …jsonl file, so that the new annotations have somewhere to go.\nIf the new annotations always tag phrases within the same speaker turn (i.e. never cross turn boundaries), then you can add a phrase layer. Otherwise, you must add a span layer.\n\nIn LaBB-CAT, select phrase layers or span layers from the menu as appropriate.\nAt the top of the list of layers, fill in the details of the blank row for the layer to add:\n\nLayer ID: the Doccanno Label’s prefix (i.e. the part before the colon)\nType: Text\nAlignment: Intervals\nManager: no manager should be selected\nGenerate: Never\nProject: select a project if desired, or none if not\nDescription: An informative description of the layer, perhaps including a lilst of all labels included.\n\nClick New to add the layer\n\nIf you have included Labels corresponding to multiple LaBB-CAT layers, ensure all the layers have been created in LaBB-CAT before continuing with the import.\n\n\n\n\nIn LaBB-CAT, select the upload menu option at the top and then the upload transcripts option.\nPress the first Choose File button on the left.\nSelect the …json file you extracted from the …zip file above.\nTick the Update Existing checkbox.\n\nPress Upload.\nYou will see a list of all the new Label prefixes, with a dropdown box for each for selecting the LaBB-CAT layer that the annotations should be imported into.\n\nEnsure all Label prefixes are matched to the correct LaBB-CAT layer\nPres Next\n\nYour new annotations will be merged into the existing transcript in LaBB-CAT.\nYou can double check this by opening on of the transcripts you tagged in LaBB-CAT and ticking the layer(s) of the new annotations. You annotations will appear, lined up with the phrases as you specified in Doccano."
  },
  {
    "objectID": "howto/manual-annotation/doccano.html#export-from-labb-cat",
    "href": "howto/manual-annotation/doccano.html#export-from-labb-cat",
    "title": "Phrase Tagging with Doccano",
    "section": "",
    "text": "In LaBB-CAT, open the transcripts page.\nUse the filters at the top to narrow the list down to the transcripts you want to export, and/or tick the target transcripts.\nClick Export Format.\nA list of layers will appear, with a list of formats below.\nIf you wish to include any existing phrase/span layers in Doccano, tick the corresponding layers in the list.\n\nNB: Annotations on the layers you select here will be displayed in Doccano but cannot be edited; any changes to these annotations will be ignored when re-importing the dataset into LaBB-CAT.\nBelow the list of layers, there’s a dropdown list of export formats.\nSelect Doccano JSONL Dataset.\n\nClick Export Format\nSave the resulting …jsonl file."
  },
  {
    "objectID": "howto/manual-annotation/doccano.html#import-into-doccano",
    "href": "howto/manual-annotation/doccano.html#import-into-doccano",
    "title": "Phrase Tagging with Doccano",
    "section": "",
    "text": "In Doccano, you will need to create a project to import your texts into. Click Projects on the top right.\nPress Create on the top left.\nSelect the Sequence Labelling option.\n Sequence Labelling option in Doccano is ticked\nEnter a name and description for your project.\nTick the Allow overlapping entity option.\n\nPress Create at the bottom.\nThis will create the project and take you to its Home page.\nPress Dataset on the left.\n\nMove the mouse over the Actions button at the top, and select the Import Dataset option.\n\nSelect JSONL as the File format.\nYou can leave the other options that appear with their default values.\nFind the …jsonl file you exported from LaBB-CAT earlier, and drap/drop it on to the grey area labelled Drop files here…\n(Alternatively you can click on the Drop files here… area, and find/select the …jsonl file.)\n\nPress Import.\n\nOnce the import is complete, you will see a list of texts on the Dataset page. The Metadata column will be full of text and numbers - this is normal; LaBB-CAT includes information in the Metadata that it needs to import the text back into LaBB-CAT correctly.\n\n\n\nThe imported dataset includes metadata to aid re-import into LaBB-CAT"
  },
  {
    "objectID": "howto/manual-annotation/doccano.html#tag-the-texts",
    "href": "howto/manual-annotation/doccano.html#tag-the-texts",
    "title": "Phrase Tagging with Doccano",
    "section": "",
    "text": "Before adding annotations to the texts, you need to create Labels in Doccano. These are the annotations you’ll be able to add to words/phrases in the texts.\n\nIn Doccano, click Labels on the left-hand menu.\nIf you exported any additional phrase/span layers from LaBB-CAT, you will see labels for the resulting annotations already listed here.\n\nEach imported label is prefixed with the ID of the LaBB-CAT layer it came from, followed by a colon. This is the pattern you must follow with the labels you create.\nMove the mouse over the Actions button at the top and click the Create Label option.\n\nEnter the label. This should be using the format:\n{LaBB-CAT-Layer-ID}:{LaBB-CAT-Label}\ne.g if you intend for your new annotations to be added to a LaBB-CAT layer called “narrative-action” and one of the possible labels in LaBB-CAT will be “complicating action”, then the Label you create in Doccano should be: narrative-action:complicating action\n\nPick a colour for the label if you wish.\nAssuming you want to add more than one label, click Save and add another.\nRepeat the above steps for each label you would like to annotate with.\nOnce you’ve finished, click the Labels option on the left-hand menu.\nYou should see the label’s you’ve added, listed after the imported ones.\n\n\n\n\nNow that you’ve configured the labels you’re going to use, you can annotate the texts you imported:\nIn Doccano click Start Annotation at the top left.\n(Alternatively, you can click Dataset and then press the Annotate button on a text of your choice)\nYou will see one of the texts you imported.\n\n\n\nTranscript including annotations exported from LaBB-CAT\n\n\nThe participant ID of the speaker appears at the beginning of each speaker turn, and if you exported phrase/span annotations from LaBB-CAT, they will appear tagging the corresponding regions of the text.\nTo tag a phrase in the text, simply click and drag over the phrase to select it. A menu of tags will appear.\n\n\n\nClick/drag for label menu\n\n\nWhen you click the desired tag, it will be added to the text.\n\n\n\nNew tags appear in the text\n\n\n\n\n\nDoccano includes a mode for tagging in which you can pre-select the Label you want to use, and then the selected Label is automatically used whenever you click/drag a phrase. This mode may be quicker as it involves fewer clicks overall.\nTo use this method of tagging, scroll to the top of the text, and click the desired Label in the list on the top right.\n\n\n\nPre-select a label in the label Types list at the top right of the text\n\n\nNow, whenever you click/drag a phrase in the tex, it will immediately be tagged with the selected Label.\nChanges are automatically saved. Once you’ve added all the tags you want in this text, you can move to the next by using the navigation buttons at the top right of the text.\n\n\n\nButtons for navigating to the first, previous, next, and last text"
  },
  {
    "objectID": "howto/manual-annotation/doccano.html#export-from-doccano",
    "href": "howto/manual-annotation/doccano.html#export-from-doccano",
    "title": "Phrase Tagging with Doccano",
    "section": "",
    "text": "One you’ve finished annotating all texts, you need to export them with the new tags so they can be imported into LaBB-CAT.\n\nIn Doccano, click the Dataset option on the left-hand menu.\nMove the mouse over Actions at the top and select Export Dataset.\n\nSelect JSONL as the File format\nPress Export.\nSave the resulting …zip file.\nExtract the …jsonl file that is contained in the …zip file you just saved."
  },
  {
    "objectID": "howto/manual-annotation/doccano.html#import-into-labb-cat",
    "href": "howto/manual-annotation/doccano.html#import-into-labb-cat",
    "title": "Phrase Tagging with Doccano",
    "section": "",
    "text": "When you import the …jsonl file into LaBB-CAT, it will extract the new Labels you’ve added, and assume that each Label is in the format:\n{LaBB-CAT-Layer-ID}:{LaBB-CAT-Label}\nEach label will be split on the colon, and the left part will be assumed to be a layer ID, and the right part will be assumed to be the label for annotations on that layer.\nIf you have added layer ID prefixes for layers that don’t exist yet in LaBB-CAT, you have to create the LaBB-CAT layers before importing the …jsonl file, so that the new annotations have somewhere to go.\nIf the new annotations always tag phrases within the same speaker turn (i.e. never cross turn boundaries), then you can add a phrase layer. Otherwise, you must add a span layer.\n\nIn LaBB-CAT, select phrase layers or span layers from the menu as appropriate.\nAt the top of the list of layers, fill in the details of the blank row for the layer to add:\n\nLayer ID: the Doccanno Label’s prefix (i.e. the part before the colon)\nType: Text\nAlignment: Intervals\nManager: no manager should be selected\nGenerate: Never\nProject: select a project if desired, or none if not\nDescription: An informative description of the layer, perhaps including a lilst of all labels included.\n\nClick New to add the layer\n\nIf you have included Labels corresponding to multiple LaBB-CAT layers, ensure all the layers have been created in LaBB-CAT before continuing with the import.\n\n\n\n\nIn LaBB-CAT, select the upload menu option at the top and then the upload transcripts option.\nPress the first Choose File button on the left.\nSelect the …json file you extracted from the …zip file above.\nTick the Update Existing checkbox.\n\nPress Upload.\nYou will see a list of all the new Label prefixes, with a dropdown box for each for selecting the LaBB-CAT layer that the annotations should be imported into.\n\nEnsure all Label prefixes are matched to the correct LaBB-CAT layer\nPres Next\n\nYour new annotations will be merged into the existing transcript in LaBB-CAT.\nYou can double check this by opening on of the transcripts you tagged in LaBB-CAT and ticking the layer(s) of the new annotations. You annotations will appear, lined up with the phrases as you specified in Doccano."
  },
  {
    "objectID": "howto/aligned-data/syllables.html",
    "href": "howto/aligned-data/syllables.html",
    "title": "Syllables and Stress",
    "section": "",
    "text": "This page describes how to generate stress-marked syllable annotations after forced alignment, which can be achieved if your original poronunciation dictionary includes syllable/stress marking (e.g. CELEX and Unisyn do, but the CMU Pronouncing Dictionary doesn’t).\nWhat LaBB-CAT does is:\n\nTake the phonemic transcription of each word from the segment layer (e.g. the one chosen by HTK during forced alignment), which has no syllable or stress marks.\nLook it up in CELEX.\nIf it’s found, it takes the syllable/stress-marked version of the phonemic transcription\nIt splits the transcription into syllables, and creates an annotation for each syllable that spans the corresponding phones on the segment layer.\n\nOnce you have such annotations, it’s much easier to identify, for example, specific vowels but in stressed syllables only.\n\n\n\nA layer that tags each word token with its phonemic transcription according to CELEX or a Unisyn lexicon.\nForce-alignment using the above phonemic transcription layer, so you have a segments layer filled in with aligned phones.\n\nNB: If phone alignments are produced by a different dictionary, then the resulting phonemic transcriptions will not match the dictionary and so syllables cannot be reconstructed. e.g. if you use the MFA built-in English dictionary, with MFA’s pretrained acoustic models, reconstructing syllables from CELEX transcriptions will fail. \n\n\n\n\nIn LaBB-CAT, click the word layers menu option\nAt the top of the list of word layers, fill in the blank header form in order to add a new layer. Important points are:\n\nType = Phonological\nAlignment = Intervals\nManager = CELEX English\n\n\nPress New. You will bee taken to the layer configuration page.\nOn the left, select the Syllables from Phonology option.\nThis will automatically set Source Layer to segments and Delimiters to - (hyphen)\n\nPress Save\nPres Regenerate\nYou will see a progress bar as LaBB-CAT processes all the transcripts in the database. This processing is done in the background, you don’t need to wait until it’s finished before visiting other pages.\n\nOnce the layer annotations are generated, each word that has been previously force-aligned will now have one or more syllable annotations, marking out the phones that belong to each syllable.\nThe label of each syllable annotation is the stress-marked phonemic transcription of the syllable, so for syllables with primary stress, the first character in the label is ’ (an apostrophe), and those with secondary stress have ” (double-quote) as the first character.\n\n\n\nStress-marked syllables as seen in Praat\n\n\nNow, if you want to, for example, identify all kit vowels in syllables with primary stress only, you can do a search:\n\nfor segments labelled as the kit vowel\nwhose syllable starts with the primary stress mark '\n\n\n\n\nSearch matrix for KIT segments whose syllable starts with the primary stress marker"
  },
  {
    "objectID": "howto/aligned-data/syllables.html#prerequisites",
    "href": "howto/aligned-data/syllables.html#prerequisites",
    "title": "Syllables and Stress",
    "section": "",
    "text": "A layer that tags each word token with its phonemic transcription according to CELEX or a Unisyn lexicon.\nForce-alignment using the above phonemic transcription layer, so you have a segments layer filled in with aligned phones.\n\nNB: If phone alignments are produced by a different dictionary, then the resulting phonemic transcriptions will not match the dictionary and so syllables cannot be reconstructed. e.g. if you use the MFA built-in English dictionary, with MFA’s pretrained acoustic models, reconstructing syllables from CELEX transcriptions will fail."
  },
  {
    "objectID": "howto/aligned-data/syllables.html#steps",
    "href": "howto/aligned-data/syllables.html#steps",
    "title": "Syllables and Stress",
    "section": "",
    "text": "In LaBB-CAT, click the word layers menu option\nAt the top of the list of word layers, fill in the blank header form in order to add a new layer. Important points are:\n\nType = Phonological\nAlignment = Intervals\nManager = CELEX English\n\n\nPress New. You will bee taken to the layer configuration page.\nOn the left, select the Syllables from Phonology option.\nThis will automatically set Source Layer to segments and Delimiters to - (hyphen)\n\nPress Save\nPres Regenerate\nYou will see a progress bar as LaBB-CAT processes all the transcripts in the database. This processing is done in the background, you don’t need to wait until it’s finished before visiting other pages.\n\nOnce the layer annotations are generated, each word that has been previously force-aligned will now have one or more syllable annotations, marking out the phones that belong to each syllable.\nThe label of each syllable annotation is the stress-marked phonemic transcription of the syllable, so for syllables with primary stress, the first character in the label is ’ (an apostrophe), and those with secondary stress have ” (double-quote) as the first character.\n\n\n\nStress-marked syllables as seen in Praat\n\n\nNow, if you want to, for example, identify all kit vowels in syllables with primary stress only, you can do a search:\n\nfor segments labelled as the kit vowel\nwhose syllable starts with the primary stress mark '\n\n\n\n\nSearch matrix for KIT segments whose syllable starts with the primary stress marker"
  },
  {
    "objectID": "howto/aligned-data/index.html",
    "href": "howto/aligned-data/index.html",
    "title": "Aligned Data",
    "section": "",
    "text": "Aligned Data\nCommon tasks that can be done once phones are aligned:\n\nReconstruct syllables and stress\nSpeech rate and articulation rate\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023-2024 NZILBB"
  },
  {
    "objectID": "howto/aligned-data/articulation-rate.html",
    "href": "howto/aligned-data/articulation-rate.html",
    "title": "Speech Rate and Articulation Rate",
    "section": "",
    "text": "Speech rate is usually measured in syllables per minute or syllables per second. LaBB-CAT can calculate this if it has the following information:\n\nstart and end times of utterances, from which the each utterance duration can be calculated (usually, transcripts you upload to LaBB-CAT include this information), and\nthe number of syllables in each word token, which can be obtained either\n\nby using the CELEX layer manager to tag each with with its syllable count, or\nif the data has been force-aligned, using a lexicon with syllabification information, the syllables themselves can be reconstructed.\n\n\nIf the speech has not been force-aligned, LaBB-CAT only knows, for each utterance, how many syllables were uttered between the start and end times of the utterance; any inter-word pauses during the utterance are counted as speech. Usually, this level of granularity is referred to as the Speech Rate.\nIf the speech has been force-aligned, LaBB-CAT can use the start and end times of the individual word tokens, and so can exclude inter-word pauses from its rate calculation. This higher level of granularity is usually referred to as the Articulation Rate.\nThe rate annotations can cover a number of different scopes with different granularities, e.g.\n\nthe local rate for each utterance,\nthe local rate for each speaker turn (which may include many utterances),\nthe global rate for the entire recording, or\nthe global rate for the speaker, across all the recordings they appear in.\n\nOnce the rate annotations are generated, they can be searched or extracted, in the same way other annotations can.\n\n\n\nCreate a word layer that tags each token with the number of syllables in the word. For example, if you’ve got English data, and have the CELEX Layer Manager installed, you can achieve this by creating a new word layer with the following characteristics:\n\nLayer ID: syllableCount\nType: Number\nAlignment: None\nManager: CELEX English\n…configured with the Syllable count option ticked\nIMPORTANT: Ensure tick the First match only option for this layer, as some words have more than one possible syllable count, and we don’t want any words counted more than once.\n. If you do not use the CELEX Layer Manager, you may find the layer manager you use for phonemic transcriptions also has a similar syllable count option.\n\nOnce the layer has finished generating, have a look at a transcript or two to check the results. Each word should be tagged with a number corresponding to the number of syllables:\n\nClick the phrase layers menu option.\nAdd a new phrase layer for speech rate. Key points are:\n\nThe layer manager to use is the Statistics Layer Manager\nThe layer to summarize should be the syllableCount layer\nThe statistic to compute is Label-Sum Rate (per minute), because we want LaBB-CAT to take the sum of all the labels, and then compute the rate from the start/end time.\n\nYou can select difference scopes for the computation. The illustration above computes a local rate for each speaker turn, and a global rate for each participant.\n\nHave a look in a transcript or two, and a participant or two, to see what the annotations you just generated look like.\n\n\n\n\nIf forced alignment has already be done, the steps to compute speech rate are:\n\nCreate a word layer that reconstructs the syllables, using these steps.\nOnce the layer has finished generating, have a look at a transcript or two to check the results. Each word should be tagged with a phonemic transcription, separated into syllables:\n\nSelect the phrase layers menu option.\nAdd a new phrase layer for speech rate. Key points are:\n\nThe layer manager to use is the Statistics Layer Manager\nThe layer to summarize should be the syllable layer\nThe statistic to compute is Token Rate (per minute), because we want LaBB-CAT to take the count the number of syllables, and then compute the rate from the start/end time of each word.\n. You can select difference scopes for the computation. The illustration above computes a local rate for each utterance, and a global rate for each participant.\n\nHave a look in a transcript or two, and a participant or two, to see what the annotations you just generated look like."
  },
  {
    "objectID": "howto/aligned-data/articulation-rate.html#computing-speecharticulation-rate",
    "href": "howto/aligned-data/articulation-rate.html#computing-speecharticulation-rate",
    "title": "Speech Rate and Articulation Rate",
    "section": "",
    "text": "Create a word layer that tags each token with the number of syllables in the word. For example, if you’ve got English data, and have the CELEX Layer Manager installed, you can achieve this by creating a new word layer with the following characteristics:\n\nLayer ID: syllableCount\nType: Number\nAlignment: None\nManager: CELEX English\n…configured with the Syllable count option ticked\nIMPORTANT: Ensure tick the First match only option for this layer, as some words have more than one possible syllable count, and we don’t want any words counted more than once.\n. If you do not use the CELEX Layer Manager, you may find the layer manager you use for phonemic transcriptions also has a similar syllable count option.\n\nOnce the layer has finished generating, have a look at a transcript or two to check the results. Each word should be tagged with a number corresponding to the number of syllables:\n\nClick the phrase layers menu option.\nAdd a new phrase layer for speech rate. Key points are:\n\nThe layer manager to use is the Statistics Layer Manager\nThe layer to summarize should be the syllableCount layer\nThe statistic to compute is Label-Sum Rate (per minute), because we want LaBB-CAT to take the sum of all the labels, and then compute the rate from the start/end time.\n\nYou can select difference scopes for the computation. The illustration above computes a local rate for each speaker turn, and a global rate for each participant.\n\nHave a look in a transcript or two, and a participant or two, to see what the annotations you just generated look like."
  },
  {
    "objectID": "howto/aligned-data/articulation-rate.html#aligned-words",
    "href": "howto/aligned-data/articulation-rate.html#aligned-words",
    "title": "Speech Rate and Articulation Rate",
    "section": "",
    "text": "If forced alignment has already be done, the steps to compute speech rate are:\n\nCreate a word layer that reconstructs the syllables, using these steps.\nOnce the layer has finished generating, have a look at a transcript or two to check the results. Each word should be tagged with a phonemic transcription, separated into syllables:\n\nSelect the phrase layers menu option.\nAdd a new phrase layer for speech rate. Key points are:\n\nThe layer manager to use is the Statistics Layer Manager\nThe layer to summarize should be the syllable layer\nThe statistic to compute is Token Rate (per minute), because we want LaBB-CAT to take the count the number of syllables, and then compute the rate from the start/end time of each word.\n. You can select difference scopes for the computation. The illustration above computes a local rate for each utterance, and a global rate for each participant.\n\nHave a look in a transcript or two, and a participant or two, to see what the annotations you just generated look like."
  }
]